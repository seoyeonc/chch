<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Wasserstein GAN | Seoyeon Choi</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Wasserstein GAN" />
<meta name="author" content="최서연" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Martin Arjovsky(Courant Institute of Mathematical Sciences), Soumith Chintala(Facebook AI Research), and L´eon Bottou1(Courant Institute of Mathematical Sciences, Facebook AI Research)" />
<meta property="og:description" content="Martin Arjovsky(Courant Institute of Mathematical Sciences), Soumith Chintala(Facebook AI Research), and L´eon Bottou1(Courant Institute of Mathematical Sciences, Facebook AI Research)" />
<link rel="canonical" href="https://seoyeonc.github.io/chch/wasserstein%20gan/gan/2022/02/08/WGAN.html" />
<meta property="og:url" content="https://seoyeonc.github.io/chch/wasserstein%20gan/gan/2022/02/08/WGAN.html" />
<meta property="og:site_name" content="Seoyeon Choi" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-08T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Martin Arjovsky(Courant Institute of Mathematical Sciences), Soumith Chintala(Facebook AI Research), and L´eon Bottou1(Courant Institute of Mathematical Sciences, Facebook AI Research)","url":"https://seoyeonc.github.io/chch/wasserstein%20gan/gan/2022/02/08/WGAN.html","@type":"BlogPosting","headline":"Wasserstein GAN","dateModified":"2022-02-08T00:00:00-06:00","datePublished":"2022-02-08T00:00:00-06:00","author":{"@type":"Person","name":"최서연"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://seoyeonc.github.io/chch/wasserstein%20gan/gan/2022/02/08/WGAN.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/chch/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://seoyeonc.github.io/chch/feed.xml" title="Seoyeon Choi" /><link rel="shortcut icon" type="image/x-icon" href="/chch/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/chch/">Seoyeon Choi</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/chch/about/">About Me</a><a class="page-link" href="/chch/search/">Search</a><a class="page-link" href="/chch/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Wasserstein GAN</h1><p class="page-description">Martin Arjovsky(Courant Institute of Mathematical Sciences), Soumith Chintala(Facebook AI Research), and L´eon Bottou1(Courant Institute of Mathematical Sciences, Facebook AI Research)</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-08T00:00:00-06:00" itemprop="datePublished">
        Feb 8, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">최서연</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      13 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/chch/categories/#Wasserstein GAN">Wasserstein GAN</a>
        &nbsp;
      
        <a class="category-tags-link" href="/chch/categories/#GAN">GAN</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Different-Distances">Different Distances </a></li>
<li class="toc-entry toc-h2"><a href="#Wasserstein-GAN">Wasserstein GAN </a></li>
<li class="toc-entry toc-h2"><a href="#Empirical-Results">Empirical Results </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Experimental-Procedure">Experimental Procedure </a></li>
<li class="toc-entry toc-h3"><a href="#Meaningful-loss-metric">Meaningful loss metric </a></li>
<li class="toc-entry toc-h3"><a href="#Improved-stability">Improved stability </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-02-08-WGAN.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ref: <a href="https://arxiv.org/pdf/1701.07875.pdf">https://arxiv.org/pdf/1701.07875.pdf</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://ahjeong.tistory.com/7">https://ahjeong.tistory.com/7</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>wgan math: <a href="https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i">https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>jonathan hui: <a href="https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490">https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Different-Distances">
<a class="anchor" href="#Different-Distances" aria-hidden="true"><span class="octicon octicon-link"></span></a>Different Distances<a class="anchor-link" href="#Different-Distances"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $X$ be a compact metric set (such as the space of images $[0, 1]^d$) <ul>
<li>compact하다는 것은 close하고 bounded 하다는 것!</li>
</ul>
</li>
<li>and let $Σ$ denote the set of all the Borel subsets of $X$ . </li>
<li>Let $Prob(X )$ denote the space of probability measures defined on $X$ .</li>
<li>We can now define elementary distances and divergences between two distributions $P_r, P_g ∈ Prob(X )$<ul>
<li>The Total Variation (TV) distance<ul>
<li>Total Variation(TV)은 두 확률측도의 측정값이 벌어질 수 있는 값 중 가장 큰 값(혹은  supremum)</li>
<li>같은 집합 A라 하더라도 두 확률분포가 측정하는 값은 다를 수 있고, 이 때 TV는 모든 $A \in \Sigma$에 대해 가장 큰 값으로 정의한 것</li>
<li>만약 두 확률분포의 확률밀도함수가 서로 겹치지 않는다면(확률분포의 suppot의 교집합이 공집합이라면) TV는 무조건 1</li>
</ul>
</li>
<li>The Kullback-Leibler (KL) divergence<ul>
<li>KL은 대칭성과 삼각부등식이 깨지기 때문에 metric이 아니지만, Premetric!</li>
<li>TV 보다 Strong함(즉, KL으로 TV 설명 가능), 그래서 TV에서 수렴하지 않으면 KL에서 수렴하지 않음</li>
</ul>
</li>
<li>The Jensen-Shannon (JS) divergence<ul>
<li>JS는 TV랑 equivalent 함.(즉, 서로를 표현 가능), 그래서 TV에서 수렴하지 않으면 JS에서 수렴하지 않음</li>
</ul>
</li>
<li>The Earth-Mover (EM) distance or Wasserstein-1</li>
<li>p값들은 절대적으로 연속!</li>
<li>그러므로 카이제곱에서 정의되는 같은 측정치 뮤가 나옴.</li>
</ul>
</li>
<li>The following example illustrates how apparently simple sequences of probability distributions converge under the EM distance but do not converge under the other distances and divergences defined above<ul>
<li>다음 예시에서 보기에 단순한 확률 분포 시퀀스가 EM 거리에서는 수렴하지만 제시된 4개의 거리 및 발산에서는 수렴하지 않음을 보임.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$X$: compact metric set</p>
<ul>
<li>Metric은 Distance라고도 불리며, 5가지 성분 만족하는 거리함수 d가 존재하는 걸 말함<ul>
<li>$d(x,y) \geq 0$</li>
<li>$d(x,y) = 0$ 이면 $x=y$ 다.</li>
<li>$x=y$이면 $d(x,y) = 0$이다.</li>
<li>$d(x,y)=d(y,x)$: 대칭이다.</li>
<li>$d(x,y) \leq d(x,z) + d(z,y)$ 삼각부등식이 성립한다.</li>
</ul>
</li>
<li>metric의 개념의 중요성<ul>
<li>수렴convergence의 개념을 정의내릴 수 있기 때문.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Example 1 (Learning parallel lines)</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $Z ∼ U[0, 1]$ the uniform distribution on the unit interval.</li>
<li>Let $P_0$ be the distribution of $(0, Z) ∈ R^2$ (a 0 on the x-axis and the random variable Z on the y-axis), uniform on a straight vertical line passing through the origin.</li>
<li>Now let $g_θ(z) = (θ, z)$ with $θ$ a single real parameter. </li>
<li>It is easy to see that in this case,</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>$\theta \neq 0$이라면 $P_0$와 $P_{\theta}$는 서로 겹치지 않는 확률분포이므로 TV가 1이 된다.</li>
<li>$\theta \neq 0$이라면<ul>
<li>$P_0 (x) \neq 0 \rightarrow P_{\theta}(x) = 0$</li>
<li>$P_{\theta} (x) \neq 0 \rightarrow P_{0}(x) = 0$<ul>
<li>따라서 $P_{\theta}&gt;0$인 곳에서 $log\big( \frac{P_{\theta}(x)}{P_0 (x)} \big)$값은 무한이 된다.</li>
<li>그래서 KL도 무한, 단, $\theta = 0$이 되지 않을 때</li>
</ul>
</li>
</ul>
</li>
<li>$\theta \neq 0$이라면<ul>
<li>$P_0 (x) \neq 0 \rightarrow P_{\theta}(x) = 0 \rightarrow P_m = \frac{P_0}{2} \rightarrow KL(P_0\|P_m) = log2$</li>
<li>$P_{\theta} (x) \neq 0 \rightarrow P_{0}(x) = 0 \rightarrow P_m = frac{P_{\theta}}{2} \rightarrow KL()P_{\theta}\|P_m) = log2$<ul>
<li>JS는 KL처럼 무한대는 되지 않지만 TV처럼 $\theta \neq 0$일땐 log2로 일정한 값만 가지게 된다.</li>
<li>$\theta = 0$이면 0
<strong>TV나 KL이나 JS는 두 확률분포 $\mathbb{P}_R, \mathbb{P}_G$가 서로 다른 영역에서 측정된 경우 완전히 다르다고 판단을 내리게끔 metric이 계산되기 때문, 즉, 두 확률 분포의 차이를 harsh하게 봄</strong>
</li>
</ul>
</li>
<li>GAN에서는 discriminator의 학습이 잘 죽는 이유..</li>
<li>따라서 GAN 학습의 유연성을 위해 수렴에 focus를 맞춘 metric 필요</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>When $θ_t → 0$, the sequence ($P_{θ_t})_{t∈N}$ converges to $P_0$ under the EM distance, but does not converge at all under either the JS, KL, reverse KL, or TV divergences.</li>
<li>Figure 1 illustrates this for the case of the EM and JS distances</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Example 1 gives us a case where we can learn a probability distribution over a low dimensional manifold by doing gradient descent on the EM distance. </li>
<li>This cannot be done with the other distances and divergences because the resulting loss function is not even continuous.<ul>
<li>결과 손실 함수가 연속적이지 않기 때문에 다른 거리와 발산으로 할 수 없음.</li>
</ul>
</li>
<li>Although this simple example features distributions with disjoint supports, the same conclusion holds when the supports have a non empty intersection contained in a set of measure zero.<ul>
<li>disjoint suppoert를 가진 분포를 특장으로 하는 이 단순한 예시는 supports가 0 집합에 포함된 비어 있지 않은 interaction을 가진 때에도 동일한 결과가 유지된다.</li>
</ul>
</li>
<li>This happens to be the case when two low dimensional manifolds intersect in general position.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>두 확률변수 $(X,Y)$가 각각 $X~\mathbb{P_0}, Y~\mathbb{P_{\theta}}$라고 할때, 각 $w$에 대해서 $X,Y$는 2치원 공간으로 매핑됨<ul>
<li>$X(w) = (0,Z_1(w)), Y(w) = (\theta, Z_2(w))$</li>
</ul>
</li>
<li>두 점 사이의 거리는 $d(X,Y) = (|\theta - 0|^2 + |Z_1(w) - Z_2(w)|)^{1/2} \geq |\theta|$로 계산<ul>
<li>$d(X,Y)$의 기대값은 어떤 결함확률분포 $\gamma$ 를 사용하든 항상 $|\theta|$보다 크거나 같음.</li>
<li>$\mathbb{e}^{\gamma}[d(X,Y)] \geq \mathbb{E}^{\gamma} [|\theta|] = |\theta|$<ul>
<li>$Z_1 = Z_2$인 분포를 따를떄 $d(X,Y) = (|\theta - 0|^2 + |Z_1(w) - Z_2(w)|)^{1/2}  = |\theta|$ 가능!</li>
<li>따라서 $W(\mathbb{P_0}, \mathbb{P_{\theta}}) = |\theta|$</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://xiucheng.org/assets/images/wgan-example.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since the Wasserstein distance is much weaker than the JS distance , we can now ask whether $W(P_r, P_θ)$ is a continuous loss function on $θ$ under mild assumptions. This, and more, is true, as we now state and prove.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Theorem 1.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $P_r$ be a fixed distribution over $X$ .</li>
<li>Let $Z$ be a random variable(e.g Gaussian) over another space $Z$. </li>
<li>Let $g : Z × R^d → X$ be a function, that will be denoted $g_θ(z)$ with $z$ the first coordinate and $θ$ the second.</li>
<li>Let $P_θ$ denote the distribution of $g_θ(Z)$. Then,</li>
</ul>
<ol>
<li>If $g$ is continuous in $θ$, so is $W(P_r, P_θ)$.<ul>
<li>g가 세타에 대해 연속이면 EM 거리도 연속!</li>
</ul>
</li>
<li>If $g$ is locally Lipschitz and satisfies regularity assumption 1, then $W(P_r, P_θ)$ is continuous everywhere, and differentiable almost everywhere.<ul>
<li>g가 립시츠 조건 만족한다면  EM 거리도 연속!</li>
</ul>
</li>
<li>Statements 1-2 are false for the Jensen-Shannon divergence $JS(P_r, P_θ)$ and all the KLs.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following corollary tells us that learning by minimizing the EM distance makes sense (at least in theory) with neural networks.</p>
<ul>
<li>다음 결과는 EM 거리를 최소화하여 학습하는 것이 신경망에 타당하다는 것을 말해줌</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Corollary 1</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $g_θ$ be any feedforward neural network parameterized by $θ$, and $p(z)$ a prior over $z$ such that $E_{z∼p(z)}(||z||) &lt; ∞$ (e.g. Gaussian, uniform, etc.)</li>
<li>Then assumption 1 is satisfied and therefore $W(P_r, P_θ)$ is continuous everywhere and differentiable almost everywhere.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>All this shows that EM is a much more sensible cost function for our problem than at least the Jensen-Shannon divergence. The following theorem describes the relative strength of the topologies induced by these distances and divergences, with KL the strongest, followed by JS and TV, and EM the weakest.</p>
<ul>
<li>EM이 최소 JS 발산보다 제시한 문제에 보다 더 합리적인 cost function이라는 것을 보임.</li>
<li>상대적인 강도는 KL &gt; JS &gt; TV &gt; EM 의 순을 이룸</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Theorem 2.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $P$ be a distribution on a compact space $X$ and $(P_n)_{n∈N}$ be a sequence of distributions on $X$ .</li>
<li>Then, considering all limits as $n → ∞$, </li>
</ul>
<ol>
<li>The following statements are equivalent <ul>
<li>$δ(P_n, P) → 0$ with δ the total variation distance </li>
<li>$JS(P_n, P) → 0$ with JS the Jensen-Shannon divergence.</li>
</ul>
</li>
<li>The following statements are equivalent<ul>
<li>$W(P_n, P) → 0.$</li>
<li>$P_n D→ P$ where $D→$ represents convergence in distribution for random variables.</li>
</ul>
</li>
<li>KL($P_n|P) → 0$ or KL($P|P_n) → 0$ imply the statements in (1).</li>
<li>The statements in (1) imply the statements in (2).</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This highlights the fact that the KL, JS, and TV distances are not sensible cost functions when learning distributions supported by low dimensional manifolds. However the EM distance is sensible in that setup. This obviously leads us to the next section where we introduce a practical approximation of optimizing the EM distance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>TV, KL, JS 는 $(\mathbb{P}_{r}, \mathbb{P}_g)$가 서로 겹치지 않는 상황에서는 불연속이 된다.</li>
<li>EM(Wasserstein distance)은 TV, KL, JS 보다 약한weak metric이므로 수렴하는데 무른soft 성질을 가진다.</li>
<li>EM은 분포수렴과 동등하다.<ul>
<li>분포수렴<ul>
<li>확률분포 수렴 종류 중 하나로서, 가장 약한weak 수렴.</li>
<li>확률분포의 개별적인 특징보다 전체적인 모양을 중시하는 수렴</li>
<li>중심극한정리에서 표본평균이 정규분포로 수렴하는 종류가 분포수렴</li>
<li>$X_n$의 모든 모멘트가 $X$의 모멘트로 수렴하면 분포수렴</li>
<li>$X_n$의 누적확률밀도함수가 $X$의 누적확률밀도함수 중 연속인 모든 점에서 수렴하면 분포수렴</li>
<li>$X_n$의 푸리에 변환Fourier transform이 수렴하면 분포수렴</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Wasserstein-GAN">
<a class="anchor" href="#Wasserstein-GAN" aria-hidden="true"><span class="octicon octicon-link"></span></a>Wasserstein GAN<a class="anchor-link" href="#Wasserstein-GAN"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Again, Theorem 2 points to the fact that $W(P_r, P_θ)$ might have nicer properties when optimized than $JS(P_r, P_θ)$. </li>
<li>However, the infimum in (1) is highly intractable. <ul>
<li>inf A = max{lower bound of A}, 즉 infmnum 은 the greatest lower bound로서, 하한값lower bound 중 가장 큰 값maximum임!</li>
</ul>
</li>
<li>On the other hand, the Kantorovich-Rubinstein duality [22] tells us that (2)<ul>
<li>supA = min{upper bound of A}, 즉 Supermum은 the least upper bound 로서, 상한값upper bound 에서 가장 작은 값minimum임!</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>WGAN, $W{\mathbb{P}, \mathbb{Q}} = \inf_{\substack{ \gamma \in \prod (\mathbb{P},\mathbb{Q}) }} \mathbb{E}^{\gamma} [d(X,Y)]$<ul>
<li>$\prod (\mathbb{P}, \mathbb{Q})$는 두 확률분포 $\mathbb{P}, \mathbb{Q}$의 결합확률분포들을 모은 집합, $\gamma$는 그 중 하나.</li>
<li>즉 모든 결합확률분포 $\prod (\mathbb{P}, \mathbb{Q})$ 중에서 $d(X,Y)$의 기대값을 가장 작게 추정한 값</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Empirical-Results">
<a class="anchor" href="#Empirical-Results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Empirical Results<a class="anchor-link" href="#Empirical-Results"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We run experiments on image generation using our Wasserstein-GAN algorithm and show that there are significant practical benefits to using it over the formulation used in standard GANs.</p>
<p>We claim two main benefits:</p>
<ul>
<li>a meaningful loss metric that correlates with the generator’s convergence and sample quality</li>
<li>improved stability of the optimization process</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Experimental-Procedure">
<a class="anchor" href="#Experimental-Procedure" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experimental Procedure<a class="anchor-link" href="#Experimental-Procedure"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The target distribution to learn is the LSUN-Bedrooms dataset [24] – a collection of natural images of indoor bedrooms.</p>
<p>Our baseline comparison is DCGAN [18], a GAN with a convolutional architecture trained with the standard GAN procedure using the − log D trick.</p>
<p>The generated samples are 3-channel images of 64x64 pixels in size.</p>
<p>We use the hyper-parameters specified in Algorithm 1 for all of our experiments.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcdzH6j%2Fbtqu1s1iIpM%2FDWoDjAxuENK3jhXgEWQ9g0%2Fimg.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figure 3: Training curves and samples at different stages of training.</p>
<ul>
<li>We can see a clear correlation between lower error and better sample quality.</li>
<li>upper left: the generator is an MLP with 4 hidden layers and 512 units at each layer.<ul>
<li>The loss decreases constistently as training progresses and sample quality increases. </li>
</ul>
</li>
<li>upper right: the generator is a standard DCGAN.<ul>
<li>In both upper plots the critic is a DCGAN without the sigmoid so losses can be subjected to comparison.</li>
</ul>
</li>
<li>Lower half: both the generator and the discriminator are MLPs with substantially high learning rates (so training failed).<ul>
<li>Loss is constant and samples are constant as well. </li>
<li>The training curves were passed through a median filter for visualization purposes</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Meaningful-loss-metric">
<a class="anchor" href="#Meaningful-loss-metric" aria-hidden="true"><span class="octicon octicon-link"></span></a>Meaningful loss metric<a class="anchor-link" href="#Meaningful-loss-metric"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Because the WGAN algorithm attempts to train the critic f (lines 2–8 in Algorithm 1) relatively well before each generator update (line 10 in Algorithm 1), the loss function at this point is an estimate of the EM distance, up to constant factors related to the way we constrain the Lipschitz constant of f.</p>
<ul>
<li>WGAN 알고리즘은 각 generator 업데이트가 상대적으로 잘 되기 전에 critic f를 학습하려고 시도하기 때문에, 그 지점에서 손실함수는 f의 립시츠 constant를 제약하는 방법과관련된 constant 요인에 따른 EM 거리의 추정치가 된다.</li>
</ul>
<p>Our first experiment illustrates how this estimate correlates well with the quality of the generated samples.</p>
<p>Besides the convolutional DCGAN architecture, we also ran experiments where we replace the generator or both the generator and the critic by 4-layer ReLU-MLP with 512 hidden units.</p>
<p>Figure 3 plots the evolution of the WGAN estimate (3) of the EM distance during WGAN training for all three architectures. The plots clearly show that these curves correlate well with the visual quality of the generated samples.</p>
<ul>
<li>plot에서는 생성된 샘플의 시각적 quality와 curve가 좋은 상관관계에 있다는 것을 명확히 보여준다.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcfa3kB%2Fbtqu13NvCe7%2FyMOv6wUAJ7KLx2b8jN2HM0%2Fimg.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figure 4:</p>
<ul>
<li>JS estimates for an MLP generator (upper left) and a DCGAN generator (upper right) trained with the standard GAN procedure. </li>
<li>Both had a DCGAN discriminator.</li>
<li>Both curves have increasing error.</li>
<li>Samples get better for the DCGAN but the JS estimate increases or stays constant, pointing towards no significant correlation between sample quality and loss.</li>
<li>Bottom: MLP with both generator and discriminator. The curve goes up and down regardless of sample quality.</li>
<li>All training curves were passed through the same median filter as in Figure 3.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To our knowledge, this is the first time in GAN literature that such a property is shown, where the loss of the GAN shows properties of convergence.</p>
<p>This property is extremely useful when doing research in adversarial networks as one does not need to stare at the generated samples to figure out failure modes and to gain information on which models are doing better over others.</p>
<ul>
<li>이 속성은 실패한 mode를 알아내고, 어떤 모델이 다른 것보다 좋은지의 정보를 얻기 위해 생성된 샘플들을 볼 필요가 없기 때문에 적대적 네트워크를 연구할 때 정말 유용하다.</li>
</ul>
<p>However, we do not claim that this is a new method to quantitatively evaluate generative models yet. The constant scaling factor that depends on the critic’s architecture means it’s hard to compare models with different critics. Even more, in practice the fact that the critic doesn’t have infinite capacity makes it hard to know just how close to the EM distance our estimate really is. This being said, we have succesfully used the loss metric to validate our experiments repeatedly and without failure, and we see this as a huge improvement in training GANs which previously had no such facility.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>This quantity clearly correlates poorly the sample quality. Note also that the JS estimate usually stays constant or goes up instead of going down.</strong></p>
<p>In fact it often remains very close to log 2 ≈ 0.69 which is the highest value taken by the JS distance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, as a negative result, we report that WGAN training becomes unstable at times when one uses a momentum based optimizer such as Adam [8] (with β1 &gt; 0) on the critic, or when one uses high learning rates.</p>
<p>Since the loss for the critic is nonstationary, momentum based methods seemed to perform worse.</p>
<p><strong>We identified momentum as a potential cause because, as the loss blew up and samples got worse, the cosine between the Adam step and the gradient usually turned negative. The only places where this cosine was negative was in these situations of instability.</strong></p>
<ul>
<li>Adam 안 쓴 이유</li>
</ul>
<p>We therefore switched to RMSProp [21] which is known to perform well even on very nonstationary problems [13].</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Improved-stability">
<a class="anchor" href="#Improved-stability" aria-hidden="true"><span class="octicon octicon-link"></span></a>Improved stability<a class="anchor-link" href="#Improved-stability"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One of the benefits of WGAN is that it allows us to train the critic till optimality.</p>
<p>When the critic is trained to completion, it simply provides a loss to the generator that we can train as any other neural network.</p>
<p><strong>This tells us that we no longer need to balance generator and discriminator’s capacity properly.</strong></p>
<p>The better the critic, the higher quality the gradients we use to train the generator.</p>
<p>We observe that WGANs are much more robust than GANs when one varies the architectural choices for the generator. We illustrate this by running experiments on three generator architectures:</p>
<ul>
<li>(1) a convolutional DCGAN generator, </li>
<li>(2) a convolutional DCGAN generator without batch normalization and with a constant number of filters, and </li>
<li>(3) a 4-layer ReLU-MLP with 512 hidden units. </li>
</ul>
<p>The last two are known to perform very poorly with GANs. We keep the convolutional DCGAN architecture for the WGAN critic or the GAN discriminator.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figures 5, 6, and 7 show samples generated for these three architectures using both the WGAN and GAN algorithms.</p>
<p>We refer the reader to Appendix F for full sheets of generated samples. Samples were not cherry-picked.</p>
<p><strong>In no experiment did we see evidence of mode collapse for the WGAN algorithm.</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fu0gFw%2Fbtqu1DaqDg2%2FdnKiu9aqEDHrL6KWkzVYY0%2Fimg.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdieO6m%2Fbtqu2HXvq4A%2FSKHAQU1vWRSgaC4m2LtFKk%2Fimg.png" alt=""></p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/chch/wasserstein%20gan/gan/2022/02/08/WGAN.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/chch/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/chch/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/chch/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/seoyeonc" target="_blank" title="seoyeonc"><svg class="svg-icon grey"><use xlink:href="/chch/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
