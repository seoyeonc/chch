<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Wasserstein GAN | Seoyeon Choi</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Wasserstein GAN" />
<meta name="author" content="최서연" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Martin Arjovsky(Courant Institute of Mathematical Sciences), Soumith Chintala(Facebook AI Research), and L´eon Bottou(Courant Institute of Mathematical Sciences, Facebook AI Research)" />
<meta property="og:description" content="Martin Arjovsky(Courant Institute of Mathematical Sciences), Soumith Chintala(Facebook AI Research), and L´eon Bottou(Courant Institute of Mathematical Sciences, Facebook AI Research)" />
<link rel="canonical" href="https://seoyeonc.github.io/chch/wasserstein%20gan/gan/2022/02/13/WGAN.html" />
<meta property="og:url" content="https://seoyeonc.github.io/chch/wasserstein%20gan/gan/2022/02/13/WGAN.html" />
<meta property="og:site_name" content="Seoyeon Choi" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-13T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Martin Arjovsky(Courant Institute of Mathematical Sciences), Soumith Chintala(Facebook AI Research), and L´eon Bottou(Courant Institute of Mathematical Sciences, Facebook AI Research)","url":"https://seoyeonc.github.io/chch/wasserstein%20gan/gan/2022/02/13/WGAN.html","@type":"BlogPosting","headline":"Wasserstein GAN","dateModified":"2022-02-13T00:00:00-06:00","datePublished":"2022-02-13T00:00:00-06:00","author":{"@type":"Person","name":"최서연"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://seoyeonc.github.io/chch/wasserstein%20gan/gan/2022/02/13/WGAN.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/chch/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://seoyeonc.github.io/chch/feed.xml" title="Seoyeon Choi" /><link rel="shortcut icon" type="image/x-icon" href="/chch/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/chch/">Seoyeon Choi</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/chch/about/">About Me</a><a class="page-link" href="/chch/search/">Search</a><a class="page-link" href="/chch/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Wasserstein GAN</h1><p class="page-description">Martin Arjovsky(Courant Institute of Mathematical Sciences), Soumith Chintala(Facebook AI Research), and L´eon Bottou(Courant Institute of Mathematical Sciences, Facebook AI Research)</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-13T00:00:00-06:00" itemprop="datePublished">
        Feb 13, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">최서연</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      18 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/chch/categories/#Wasserstein GAN">Wasserstein GAN</a>
        &nbsp;
      
        <a class="category-tags-link" href="/chch/categories/#GAN">GAN</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Different-Distances">Different Distances </a></li>
<li class="toc-entry toc-h2"><a href="#Wasserstein-GAN">Wasserstein GAN </a></li>
<li class="toc-entry toc-h2"><a href="#Empirical-Results">Empirical Results </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Experimental-Procedure">Experimental Procedure </a></li>
<li class="toc-entry toc-h3"><a href="#Meaningful-loss-metric">Meaningful loss metric </a></li>
<li class="toc-entry toc-h3"><a href="#Improved-stability">Improved stability </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#GAN-—-Wasserstein-GAN-&-WGAN-GP">GAN — Wasserstein GAN &amp; WGAN-GP </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Earth-Mover-(EM)-distance/-Wasserstein-Metric">Earth-Mover (EM) distance/ Wasserstein Metric </a></li>
<li class="toc-entry toc-h3"><a href="#Wasserstein-Distance">Wasserstein Distance </a></li>
<li class="toc-entry toc-h3"><a href="#Wasserstein-GAN">Wasserstein GAN </a></li>
<li class="toc-entry toc-h3"><a href="#Algorithm">Algorithm </a></li>
<li class="toc-entry toc-h3"><a href="#Experiment">Experiment </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Correlation-between-loss-metric-and-image-quality">Correlation between loss metric and image quality </a></li>
<li class="toc-entry toc-h4"><a href="#Improve-training-stability">Improve training stability </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#WGAN-—-Issues">WGAN — Issues </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Lipschitz-constraint">Lipschitz constraint </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Model-capacity">Model capacity </a></li>
<li class="toc-entry toc-h3"><a href="#Wasserstein-GAN-with-gradient-penalty-(WGAN-GP)">Wasserstein GAN with gradient penalty (WGAN-GP) </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Gradient-penalty">Gradient penalty </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Algorithm">Algorithm </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-02-13-WGAN.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ref: <a href="https://arxiv.org/pdf/1701.07875.pdf">https://arxiv.org/pdf/1701.07875.pdf</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>wgan math: <a href="https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i">https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>jonathan hui: <a href="https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490">https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>정리</p>
<ul>
<li>KL 발산과 JS 발산은 기울기 손실 때문에 제대로 학습하지 못하는 경우가 있다!</li>
<li>그래서 GAN 이 제안되었는데, 모델을 불안정하게 만드는 큰 기울기 변수를 가진 새로운 함수에서는 GAN 자체도 불안했다</li>
<li>1-립시츠함수()를 사용, 가중치 클리핑을 제한한 와서스테인 거리로 WGAN 제시</li>
<li>하지만 가중치 클리핑(c)을 제한하는 것은 discriminator의 가중치를 하이퍼 파라미터c가 건드는(제약하는) 일이 되어버림</li>
<li>WGAN-GP를 제안하게 EHOA!<ul>
<li>GAN을 WGAN-GP로 바꾸는 조건!!<ul>
<li>critic loss 함수에 기울기 패널티 항을 포함한다.</li>
<li>critic의 가중치를 클리핑하지 않는다.</li>
<li>critic에 배치 정규화 층을 사용하지 않는다.(배치정규화는 상관관계를 만든다.)</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Different-Distances">
<a class="anchor" href="#Different-Distances" aria-hidden="true"><span class="octicon octicon-link"></span></a>Different Distances<a class="anchor-link" href="#Different-Distances"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $X$ be a compact metric set (such as the space of images $[0, 1]^d$) <ul>
<li>compact하다는 것은 close하고 bounded 하다는 것!</li>
</ul>
</li>
<li>and let $Σ$ denote the set of all the Borel subsets of $X$ . </li>
<li>Let $Prob(X )$ denote the space of probability measures defined on $X$ .</li>
<li>We can now define elementary distances and divergences between two distributions $P_r, P_g ∈ Prob(X )$<ul>
<li>The Total Variation (TV) distance<ul>
<li>Total Variation(TV)은 두 확률측도의 측정값이 벌어질 수 있는 값 중 가장 큰 값(혹은  supremum)</li>
<li>같은 집합 A라 하더라도 두 확률분포가 측정하는 값은 다를 수 있고, 이 때 TV는 모든 $A \in \Sigma$에 대해 가장 큰 값으로 정의한 것</li>
<li>만약 두 확률분포의 확률밀도함수가 서로 겹치지 않는다면(확률분포의 suppot의 교집합이 공집합이라면) TV는 무조건 1</li>
</ul>
</li>
<li>The Kullback-Leibler (KL) divergence<ul>
<li>KL은 대칭성과 삼각부등식이 깨지기 때문에 metric이 아니지만, Premetric!</li>
<li>TV 보다 Strong함(즉, KL으로 TV 설명 가능), 그래서 TV에서 수렴하지 않으면 KL에서 수렴하지 않음</li>
</ul>
</li>
<li>The Jensen-Shannon (JS) divergence<ul>
<li>JS는 TV랑 equivalent 함.(즉, 서로를 표현 가능), 그래서 TV에서 수렴하지 않으면 JS에서 수렴하지 않음</li>
</ul>
</li>
<li>The Earth-Mover (EM) distance or Wasserstein-1</li>
<li>p값들은 절대적으로 연속!</li>
<li>그러므로 카이제곱에서 정의되는 같은 측정치 뮤가 나옴.</li>
</ul>
</li>
<li>The following example illustrates how apparently simple sequences of probability distributions converge under the EM distance but do not converge under the other distances and divergences defined above<ul>
<li>다음 예시에서 보기에 단순한 확률 분포 시퀀스가 EM 거리에서는 수렴하지만 제시된 4개의 거리 및 발산에서는 수렴하지 않음을 보임.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$X$: compact metric set</p>
<ul>
<li>Metric은 Distance라고도 불리며, 5가지 성분 만족하는 거리함수 d가 존재하는 걸 말함<ul>
<li>$d(x,y) \geq 0$</li>
<li>$d(x,y) = 0$ 이면 $x=y$ 다.</li>
<li>$x=y$이면 $d(x,y) = 0$이다.</li>
<li>$d(x,y)=d(y,x)$: 대칭이다.</li>
<li>$d(x,y) \leq d(x,z) + d(z,y)$ 삼각부등식이 성립한다.</li>
</ul>
</li>
<li>metric의 개념의 중요성<ul>
<li>수렴convergence의 개념을 정의내릴 수 있기 때문.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Example 1 (Learning parallel lines)</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $Z ∼ U[0, 1]$ the uniform distribution on the unit interval.</li>
<li>Let $P_0$ be the distribution of $(0, Z) ∈ R^2$ (a 0 on the x-axis and the random variable Z on the y-axis), uniform on a straight vertical line passing through the origin.</li>
<li>Now let $g_θ(z) = (θ, z)$ with $θ$ a single real parameter. </li>
<li>It is easy to see that in this case,</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>$\theta \neq 0$이라면 $P_0$와 $P_{\theta}$는 서로 겹치지 않는 확률분포이므로 TV가 1이 된다.</li>
<li>$\theta \neq 0$이라면<ul>
<li>$P_0 (x) \neq 0 \rightarrow P_{\theta}(x) = 0$</li>
<li>$P_{\theta} (x) \neq 0 \rightarrow P_{0}(x) = 0$<ul>
<li>따라서 $P_{\theta}&gt;0$인 곳에서 $log\big( \frac{P_{\theta}(x)}{P_0 (x)} \big)$값은 무한이 된다.</li>
<li>그래서 KL도 무한, 단, $\theta = 0$이 되지 않을 때</li>
</ul>
</li>
</ul>
</li>
<li>$\theta \neq 0$이라면<ul>
<li>$P_0 (x) \neq 0 \rightarrow P_{\theta}(x) = 0 \rightarrow P_m = \frac{P_0}{2} \rightarrow KL(P_0\|P_m) = log2$</li>
<li>$P_{\theta} (x) \neq 0 \rightarrow P_{0}(x) = 0 \rightarrow P_m = frac{P_{\theta}}{2} \rightarrow KL()P_{\theta}\|P_m) = log2$<ul>
<li>JS는 KL처럼 무한대는 되지 않지만 TV처럼 $\theta \neq 0$일땐 log2로 일정한 값만 가지게 된다.</li>
<li>$\theta = 0$이면 0
<strong>TV나 KL이나 JS는 두 확률분포 $\mathbb{P}_R, \mathbb{P}_G$가 서로 다른 영역에서 측정된 경우 완전히 다르다고 판단을 내리게끔 metric이 계산되기 때문, 즉, 두 확률 분포의 차이를 harsh하게 봄</strong>
</li>
</ul>
</li>
<li>GAN에서는 discriminator의 학습이 잘 죽는 이유..</li>
<li>따라서 GAN 학습의 유연성을 위해 수렴에 focus를 맞춘 metric 필요</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>When $θ_t → 0$, the sequence ($P_{θ_t})_{t∈N}$ converges to $P_0$ under the EM distance, but does not converge at all under either the JS, KL, reverse KL, or TV divergences.</li>
<li>Figure 1 illustrates this for the case of the EM and JS distances</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Example 1 gives us a case where we can learn a probability distribution over a low dimensional manifold by doing gradient descent on the EM distance. </li>
<li>This cannot be done with the other distances and divergences because the resulting loss function is not even continuous.<ul>
<li>결과 손실 함수가 연속적이지 않기 때문에 다른 거리와 발산으로 할 수 없음.</li>
</ul>
</li>
<li>Although this simple example features distributions with disjoint supports, the same conclusion holds when the supports have a non empty intersection contained in a set of measure zero.<ul>
<li>disjoint suppoert를 가진 분포를 특장으로 하는 이 단순한 예시는 supports가 0 집합에 포함된 비어 있지 않은 interaction을 가진 때에도 동일한 결과가 유지된다.</li>
</ul>
</li>
<li>This happens to be the case when two low dimensional manifolds intersect in general position.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>두 확률변수 $(X,Y)$가 각각 $X~\mathbb{P_0}, Y~\mathbb{P_{\theta}}$라고 할때, 각 $w$에 대해서 $X,Y$는 2치원 공간으로 매핑됨<ul>
<li>$X(w) = (0,Z_1(w)), Y(w) = (\theta, Z_2(w))$</li>
</ul>
</li>
<li>두 점 사이의 거리는 $d(X,Y) = (|\theta - 0|^2 + |Z_1(w) - Z_2(w)|)^{1/2} \geq |\theta|$로 계산<ul>
<li>$d(X,Y)$의 기대값은 어떤 결함확률분포 $\gamma$ 를 사용하든 항상 $|\theta|$보다 크거나 같음.</li>
<li>$\mathbb{e}^{\gamma}[d(X,Y)] \geq \mathbb{E}^{\gamma} [|\theta|] = |\theta|$<ul>
<li>$Z_1 = Z_2$인 분포를 따를떄 $d(X,Y) = (|\theta - 0|^2 + |Z_1(w) - Z_2(w)|)^{1/2}  = |\theta|$ 가능!</li>
<li>따라서 $W(\mathbb{P_0}, \mathbb{P_{\theta}}) = |\theta|$</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://xiucheng.org/assets/images/wgan-example.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since the Wasserstein distance is much weaker than the JS distance , we can now ask whether $W(P_r, P_θ)$ is a continuous loss function on $θ$ under mild assumptions. This, and more, is true, as we now state and prove.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Theorem 1.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $P_r$ be a fixed distribution over $X$ .</li>
<li>Let $Z$ be a random variable(e.g Gaussian) over another space $Z$. </li>
<li>Let $g : Z × R^d → X$ be a function, that will be denoted $g_θ(z)$ with $z$ the first coordinate and $θ$ the second.</li>
<li>Let $P_θ$ denote the distribution of $g_θ(Z)$. Then,</li>
</ul>
<ol>
<li>If $g$ is continuous in $θ$, so is $W(P_r, P_θ)$.<ul>
<li>g가 세타에 대해 연속이면 EM 거리도 연속!</li>
</ul>
</li>
<li>If $g$ is locally Lipschitz and satisfies regularity assumption 1, then $W(P_r, P_θ)$ is continuous everywhere, and differentiable almost everywhere.<ul>
<li>g가 립시츠 조건 만족한다면  EM 거리도 연속!</li>
</ul>
</li>
<li>Statements 1-2 are false for the Jensen-Shannon divergence $JS(P_r, P_θ)$ and all the KLs.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following corollary tells us that learning by minimizing the EM distance makes sense (at least in theory) with neural networks.</p>
<ul>
<li>다음 결과는 EM 거리를 최소화하여 학습하는 것이 신경망에 타당하다는 것을 말해줌</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Corollary 1</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $g_θ$ be any feedforward neural network parameterized by $θ$, and $p(z)$ a prior over $z$ such that $E_{z∼p(z)}(||z||) &lt; ∞$ (e.g. Gaussian, uniform, etc.)</li>
<li>Then assumption 1 is satisfied and therefore $W(P_r, P_θ)$ is continuous everywhere and differentiable almost everywhere.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>All this shows that EM is a much more sensible cost function for our problem than at least the Jensen-Shannon divergence. The following theorem describes the relative strength of the topologies induced by these distances and divergences, with KL the strongest, followed by JS and TV, and EM the weakest.</p>
<ul>
<li>EM이 최소 JS 발산보다 제시한 문제에 보다 더 합리적인 cost function이라는 것을 보임.</li>
<li>상대적인 강도는 KL &gt; JS &gt; TV &gt; EM 의 순을 이룸</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Theorem 2.</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $P$ be a distribution on a compact space $X$ and $(P_n)_{n∈N}$ be a sequence of distributions on $X$ .</li>
<li>Then, considering all limits as $n → ∞$, </li>
</ul>
<ol>
<li>The following statements are equivalent <ul>
<li>$δ(P_n, P) → 0$ with δ the total variation distance </li>
<li>$JS(P_n, P) → 0$ with JS the Jensen-Shannon divergence.</li>
</ul>
</li>
<li>The following statements are equivalent<ul>
<li>$W(P_n, P) → 0.$</li>
<li>$P_n D→ P$ where $D→$ represents convergence in distribution for random variables.</li>
</ul>
</li>
<li>KL($P_n|P) → 0$ or KL($P|P_n) → 0$ imply the statements in (1).</li>
<li>The statements in (1) imply the statements in (2).</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This highlights the fact that the KL, JS, and TV distances are not sensible cost functions when learning distributions supported by low dimensional manifolds. However the EM distance is sensible in that setup. This obviously leads us to the next section where we introduce a practical approximation of optimizing the EM distance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>TV, KL, JS 는 $(\mathbb{P}_{r}, \mathbb{P}_g)$가 서로 겹치지 않는 상황에서는 불연속이 된다.</li>
<li>EM(Wasserstein distance)은 TV, KL, JS 보다 약한weak metric이므로 수렴하는데 무른soft 성질을 가진다.</li>
<li>EM은 분포수렴과 동등하다.<ul>
<li>분포수렴<ul>
<li>확률분포 수렴 종류 중 하나로서, 가장 약한weak 수렴.</li>
<li>확률분포의 개별적인 특징보다 전체적인 모양을 중시하는 수렴</li>
<li>중심극한정리에서 표본평균이 정규분포로 수렴하는 종류가 분포수렴</li>
<li>$X_n$의 모든 모멘트가 $X$의 모멘트로 수렴하면 분포수렴</li>
<li>$X_n$의 누적확률밀도함수가 $X$의 누적확률밀도함수 중 연속인 모든 점에서 수렴하면 분포수렴</li>
<li>$X_n$의 푸리에 변환Fourier transform이 수렴하면 분포수렴</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Wasserstein-GAN">
<a class="anchor" href="#Wasserstein-GAN" aria-hidden="true"><span class="octicon octicon-link"></span></a>Wasserstein GAN<a class="anchor-link" href="#Wasserstein-GAN"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Again, Theorem 2 points to the fact that $W(P_r, P_θ)$ might have nicer properties when optimized than $JS(P_r, P_θ)$. </li>
<li>However, the infimum in (1) is highly intractable. <ul>
<li>inf A = max{lower bound of A}, 즉 infmnum 은 the greatest lower bound로서, 하한값lower bound 중 가장 큰 값maximum임!</li>
</ul>
</li>
<li>On the other hand, the Kantorovich-Rubinstein duality [22] tells us that (2)<ul>
<li>supA = min{upper bound of A}, 즉 Supermum은 the least upper bound 로서, 상한값upper bound 에서 가장 작은 값minimum임!</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>WGAN, $W{\mathbb{P}, \mathbb{Q}} = \inf_{\substack{ \gamma \in \prod (\mathbb{P},\mathbb{Q}) }} \mathbb{E}^{\gamma} [d(X,Y)]$<ul>
<li>$\prod (\mathbb{P}, \mathbb{Q})$는 두 확률분포 $\mathbb{P}, \mathbb{Q}$의 결합확률분포들을 모은 집합, $\gamma$는 그 중 하나.</li>
<li>즉 모든 결합확률분포 $\prod (\mathbb{P}, \mathbb{Q})$ 중에서 $d(X,Y)$의 기대값을 가장 작게 추정한 값</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Empirical-Results">
<a class="anchor" href="#Empirical-Results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Empirical Results<a class="anchor-link" href="#Empirical-Results"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We run experiments on image generation using our Wasserstein-GAN algorithm and show that there are significant practical benefits to using it over the formulation used in standard GANs.</p>
<p>We claim two main benefits:</p>
<ul>
<li>a meaningful loss metric that correlates with the generator’s convergence and sample quality</li>
<li>improved stability of the optimization process</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Experimental-Procedure">
<a class="anchor" href="#Experimental-Procedure" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experimental Procedure<a class="anchor-link" href="#Experimental-Procedure"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The target distribution to learn is the LSUN-Bedrooms dataset [24] – a collection of natural images of indoor bedrooms.</p>
<p>Our baseline comparison is DCGAN [18], a GAN with a convolutional architecture trained with the standard GAN procedure using the − log D trick.</p>
<p>The generated samples are 3-channel images of 64x64 pixels in size.</p>
<p>We use the hyper-parameters specified in Algorithm 1 for all of our experiments.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcdzH6j%2Fbtqu1s1iIpM%2FDWoDjAxuENK3jhXgEWQ9g0%2Fimg.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figure 3: Training curves and samples at different stages of training.</p>
<ul>
<li>We can see a clear correlation between lower error and better sample quality.</li>
<li>upper left: the generator is an MLP with 4 hidden layers and 512 units at each layer.<ul>
<li>The loss decreases constistently as training progresses and sample quality increases. </li>
</ul>
</li>
<li>upper right: the generator is a standard DCGAN.<ul>
<li>In both upper plots the critic is a DCGAN without the sigmoid so losses can be subjected to comparison.</li>
</ul>
</li>
<li>Lower half: both the generator and the discriminator are MLPs with substantially high learning rates (so training failed).<ul>
<li>Loss is constant and samples are constant as well. </li>
<li>The training curves were passed through a median filter for visualization purposes</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Meaningful-loss-metric">
<a class="anchor" href="#Meaningful-loss-metric" aria-hidden="true"><span class="octicon octicon-link"></span></a>Meaningful loss metric<a class="anchor-link" href="#Meaningful-loss-metric"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Because the WGAN algorithm attempts to train the critic f (lines 2–8 in Algorithm 1) relatively well before each generator update (line 10 in Algorithm 1), the loss function at this point is an estimate of the EM distance, up to constant factors related to the way we constrain the Lipschitz constant of f.</p>
<ul>
<li>WGAN 알고리즘은 각 generator 업데이트가 상대적으로 잘 되기 전에 critic f를 학습하려고 시도하기 때문에, 그 지점에서 손실함수는 f의 립시츠 constant를 제약하는 방법과관련된 constant 요인에 따른 EM 거리의 추정치가 된다.</li>
</ul>
<p>Our first experiment illustrates how this estimate correlates well with the quality of the generated samples.</p>
<p>Besides the convolutional DCGAN architecture, we also ran experiments where we replace the generator or both the generator and the critic by 4-layer ReLU-MLP with 512 hidden units.</p>
<p>Figure 3 plots the evolution of the WGAN estimate (3) of the EM distance during WGAN training for all three architectures. The plots clearly show that these curves correlate well with the visual quality of the generated samples.</p>
<ul>
<li>plot에서는 생성된 샘플의 시각적 quality와 curve가 좋은 상관관계에 있다는 것을 명확히 보여준다.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcfa3kB%2Fbtqu13NvCe7%2FyMOv6wUAJ7KLx2b8jN2HM0%2Fimg.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figure 4:</p>
<ul>
<li>JS estimates for an MLP generator (upper left) and a DCGAN generator (upper right) trained with the standard GAN procedure. </li>
<li>Both had a DCGAN discriminator.</li>
<li>Both curves have increasing error.</li>
<li>Samples get better for the DCGAN but the JS estimate increases or stays constant, pointing towards no significant correlation between sample quality and loss.</li>
<li>Bottom: MLP with both generator and discriminator. The curve goes up and down regardless of sample quality.</li>
<li>All training curves were passed through the same median filter as in Figure 3.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To our knowledge, this is the first time in GAN literature that such a property is shown, where the loss of the GAN shows properties of convergence.</p>
<p>This property is extremely useful when doing research in adversarial networks as one does not need to stare at the generated samples to figure out failure modes and to gain information on which models are doing better over others.</p>
<ul>
<li>이 속성은 실패한 mode를 알아내고, 어떤 모델이 다른 것보다 좋은지의 정보를 얻기 위해 생성된 샘플들을 볼 필요가 없기 때문에 적대적 네트워크를 연구할 때 정말 유용하다.</li>
</ul>
<p>However, we do not claim that this is a new method to quantitatively evaluate generative models yet. The constant scaling factor that depends on the critic’s architecture means it’s hard to compare models with different critics. Even more, in practice the fact that the critic doesn’t have infinite capacity makes it hard to know just how close to the EM distance our estimate really is. This being said, we have succesfully used the loss metric to validate our experiments repeatedly and without failure, and we see this as a huge improvement in training GANs which previously had no such facility.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>This quantity clearly correlates poorly the sample quality. Note also that the JS estimate usually stays constant or goes up instead of going down.</strong></p>
<p>In fact it often remains very close to log 2 ≈ 0.69 which is the highest value taken by the JS distance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, as a negative result, we report that WGAN training becomes unstable at times when one uses a momentum based optimizer such as Adam [8] (with β1 &gt; 0) on the critic, or when one uses high learning rates.</p>
<p>Since the loss for the critic is nonstationary, momentum based methods seemed to perform worse.</p>
<p><strong>We identified momentum as a potential cause because, as the loss blew up and samples got worse, the cosine between the Adam step and the gradient usually turned negative. The only places where this cosine was negative was in these situations of instability.</strong></p>
<ul>
<li>Adam 안 쓴 이유</li>
</ul>
<p>We therefore switched to RMSProp [21] which is known to perform well even on very nonstationary problems [13].</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Improved-stability">
<a class="anchor" href="#Improved-stability" aria-hidden="true"><span class="octicon octicon-link"></span></a>Improved stability<a class="anchor-link" href="#Improved-stability"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One of the benefits of WGAN is that it allows us to train the critic till optimality.</p>
<p>When the critic is trained to completion, it simply provides a loss to the generator that we can train as any other neural network.</p>
<p><strong>This tells us that we no longer need to balance generator and discriminator’s capacity properly.</strong></p>
<p>The better the critic, the higher quality the gradients we use to train the generator.</p>
<p>We observe that WGANs are much more robust than GANs when one varies the architectural choices for the generator. We illustrate this by running experiments on three generator architectures:</p>
<ul>
<li>(1) a convolutional DCGAN generator, </li>
<li>(2) a convolutional DCGAN generator without batch normalization and with a constant number of filters, and </li>
<li>(3) a 4-layer ReLU-MLP with 512 hidden units. </li>
</ul>
<p>The last two are known to perform very poorly with GANs. We keep the convolutional DCGAN architecture for the WGAN critic or the GAN discriminator.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figures 5, 6, and 7 show samples generated for these three architectures using both the WGAN and GAN algorithms.</p>
<p>We refer the reader to Appendix F for full sheets of generated samples. Samples were not cherry-picked.</p>
<p><strong>In no experiment did we see evidence of mode collapse for the WGAN algorithm.</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fu0gFw%2Fbtqu1DaqDg2%2FdnKiu9aqEDHrL6KWkzVYY0%2Fimg.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdieO6m%2Fbtqu2HXvq4A%2FSKHAQU1vWRSgaC4m2LtFKk%2Fimg.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ref: <a href="https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490">https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="GAN-—-Wasserstein-GAN-&amp;-WGAN-GP">
<a class="anchor" href="#GAN-%E2%80%94-Wasserstein-GAN-&amp;-WGAN-GP" aria-hidden="true"><span class="octicon octicon-link"></span></a>GAN — Wasserstein GAN &amp; WGAN-GP<a class="anchor-link" href="#GAN-%E2%80%94-Wasserstein-GAN-&amp;-WGAN-GP"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>GAN을 훈련하는 것이 어려운 이유</p>
<ol>
<li>모델들이 수렴하지 않을 수 있다.</li>
<li>모드 축소가 일반적이다.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Earth-Mover-(EM)-distance/-Wasserstein-Metric">
<a class="anchor" href="#Earth-Mover-(EM)-distance/-Wasserstein-Metric" aria-hidden="true"><span class="octicon octicon-link"></span></a>Earth-Mover (EM) distance/ Wasserstein Metric<a class="anchor-link" href="#Earth-Mover-(EM)-distance/-Wasserstein-Metric"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>와서스테인 거리(혹은 EM 거리)는 chpeapest tranport plan의 cost이다!</p>
<p>와서스테인 거리 설명 위한 base</p>
<ul>
<li>데이터 분포 q를 데이터 분포 p로 변환하기 위한 최소 질량 운반 비용minimum xost of tranporting mass.</li>
<li>실제 데이터 분포 와 생성된 데이터 분포에 대한 와서스테인 거리는 수학적으로 모든 tranport plan에 대한 하한(최소값)으로 정의된다.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*Fd1-qVkNAbLbgEetsZfBdA.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>$Π(P_r, P_g)$ denotes the set of all joint distributions $γ(x, y)$ whose marginals are respectively $P_r$ and $P_g$.<ul>
<li>결합확률분포 $Π(P_r, P_g)$ 중에서 maginal이 각각 pr,pg인 모든 결합 분포 $γ(x, y)$ 의 집합<ul>
<li>(질문)</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>KL발산과 JS발산</p>
<ul>
<li>P와 Q가 같을때 발산은 0이 된다. </li>
<li>q의 평균이 증가함으로써, 발산이 증가한다.</li>
<li>즉. 생성자는 기울기 손실로부터 아무것도 배우지 못하게 된다.<ul>
<li>생성된 이미지가 가진 분포 q가 실제 p와 멀리 떨어져 있으면 아무것도 배우지 못한다.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*N0eggxE65amVSI0FE0zzAg.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>원래 GAN은 이 기울기 손실 문제를 해결(?)하기 위한 대안적인 cost 함수로 제시되었는데, 모델을 불안정하게 만드는 큰 기울기 변수를 가진 새로운 함수에서는 GAN 자체도 불안했다</p>
<ul>
<li>이 뜻은 또 기울기 소실 문제가 발생했다는 뜻?!</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Wasserstein-Distance">
<a class="anchor" href="#Wasserstein-Distance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Wasserstein Distance<a class="anchor-link" href="#Wasserstein-Distance"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>이에 WGAN은 노이즈를 추가하지 않고 와서스테인 거리를 이용해서 새로운 cost 함수를 제안한 것!</p>
<p><strong>WGAN learns no matter the generator is performing or not.</strong></p>
<ul>
<li>WGAN은 generator가 perform하든 안 하든 학습한다!</li>
</ul>
<p>아래 이미지는 wgan 논문에 있는 거 (위에서도 사용된)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*-VajV2qCbPWDCdNGbQfCng.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>이 그림은 GAN, WGAN에 대해 D(X)의 값에서 유사한 plot을 반복한 것인데,</p>
<ul>
<li>빨간 GAN 선의 경우 감소하거나 폭발하는 기울기의 영역으로 채워져 있다.</li>
<li>파란 WGAN 선의 경우 generator가 나은 이미지르를 만들지 않아도 기울기가 어디서든지 부드럽고 잘 학습되는 것을 볼 수 있다!</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Wasserstein-GAN">
<a class="anchor" href="#Wasserstein-GAN" aria-hidden="true"><span class="octicon octicon-link"></span></a>Wasserstein GAN<a class="anchor-link" href="#Wasserstein-GAN"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*6y-tz57odJpHh4pwRfXACw.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>여기서 sup은 최소값들중에 상한값, f는 1-립시츠함수!(조건:$|f(x_1)-f(x_2)| \le |x_1 - x_2|$)</p>
<ul>
<li>그리니까 와서스테인 거리를 계산하기 위해서 1-립시츠함수를 찾으면 되겠지?</li>
<li>이걸 학습하기 위한 deep network를 구축할 수 있는데, 실제로 시그모이드 함수가 없는 discriminator D와 네트워크가 매우 유사하고 확률보다는 스칼라 점수를(EM 거라의 결과) 출력함.<ul>
<li>이를 state(input)이 얼마나 좋은지 측정하는 가치 함수라 부를 예정~</li>
</ul>
</li>
<li>discriminator의 새로운 역할을 반영하기 위해 critic으로 이름도 변경~<ul>
<li>discriminator는 가짜/진짜를 판별하기 위해 sigmoid를 사용하고 output은 가짜/진짜에 대한 예측 확률 값이다.</li>
<li>반면 critic은 EM(Earth Mover) distance로부터 얻은 scalar값을 이용한다.<ul>
<li>EM distance는 확률 붙포 사이의 거리를 측정하는 척도 중 하나인데, 그동안 일반적으로 사용된 척도는 KL 발산이다.</li>
<li>KL 발산은 매우 엄격하게 거리를 측정하는 방법이라서 연속되지 않는 경우가 있으며 학습시키기도 어렵다.</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>GAN</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*M_YipQF_oC6owsU1VVrfhg.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>WGAN</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*Yfa9bZL0d4NHaU1mHbGzjw.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>설계한 것을 보면 critic이 시그모이드 함수를 출력하지 않았다는 것만 빼고 거의 동일하다!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*5jF5gbIDwU6k9m1ILl0Utg.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>이렇게 cost함수만 다르지! 여기서 주의할 점은 f가 1-립시츠 함수라는 점~</p>
<p>1-립시츠 함수의 제약 조건 걸려고 WGAN은 f의 최대 가중치를 제한하는 간단한 클립핑을 제한한다~</p>
<ul>
<li>즉, discriminator의 가중치는 초매개변수 c에 의해 제어되는 특정 범위 안에 있어야 함!!!</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*6vhidunAHMVDZ3nIpjWzjg.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Algorithm">
<a class="anchor" href="#Algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Algorithm<a class="anchor-link" href="#Algorithm"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*JOg9lC2JLl2Crmx5uk6S2g.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Experiment">
<a class="anchor" href="#Experiment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experiment<a class="anchor-link" href="#Experiment"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Correlation-between-loss-metric-and-image-quality">
<a class="anchor" href="#Correlation-between-loss-metric-and-image-quality" aria-hidden="true"><span class="octicon octicon-link"></span></a>Correlation between loss metric and image quality<a class="anchor-link" href="#Correlation-between-loss-metric-and-image-quality"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>GAN의 loss는 이미지 품질의 측정보다는 얼마나 discriminator 를 잘 속이는지를 측정함~</p>
<ul>
<li>첫번째 그림을 보면 GAN은 generator loss가 화질이 좋아져도 떨어지지 않음!<ul>
<li>이 말은 우리가 어떻게 진행되고 있는지 설명하기 어렵다는 뜻이겠지?</li>
<li>하지만 우리는 테스트한 이미지를 저장하고 시각적으로 평가해야 하잖아?</li>
</ul>
</li>
<li>세번째 그림을 보면 WGAN loss 함수는 이미지 품질을 더 바람직하게 반영하는 것을 볼 수 있어!</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*BKai93EIfFfVpwFHlDDhug.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Improve-training-stability">
<a class="anchor" href="#Improve-training-stability" aria-hidden="true"><span class="octicon octicon-link"></span></a>Improve training stability<a class="anchor-link" href="#Improve-training-stability"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>WGAN의 중요한 contribution 두 가지!</p>
<ol>
<li>experiment에서 모드의 불과 sign은 없었다!</li>
<li>generator는 critic이 잘 perform할 때 여전히 학습할 수 있다!</li>
</ol>
<p>DCGAN에서 배치정규화 제거해도 WGAN은 여전히 perform가능~~</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*F_M3KzHDud7CuBb2DCtzSA.jpeg" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="WGAN-—-Issues">
<a class="anchor" href="#WGAN-%E2%80%94-Issues" aria-hidden="true"><span class="octicon octicon-link"></span></a>WGAN — Issues<a class="anchor-link" href="#WGAN-%E2%80%94-Issues"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Lipschitz-constraint">
<a class="anchor" href="#Lipschitz-constraint" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lipschitz constraint<a class="anchor-link" href="#Lipschitz-constraint"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>클리핑하면 critic 모델에 립시츠 제약조건을 적용해서 와서스테인 거리를 계산할 수 있음~</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\star$ 립시츠 제약 간단히 정리

$$||f(x_1)-f(x_2)| \le K|x_1 - x_2$$

이 부등식을 만족하면 1-립시츠라고 부르는데, 여기서 $x_1-x_2$는 두 이미지 픽셀의 평균적인 절대값 차이이고 $||f(x_1)-f(x_2)|$는 비평자 예측 간의 절대값 차이이다. 기본적으로 두 이미지 사이에서 비평자의 예측이 변화할 수 있는 비율을 제한할 필요가 있다. 즉, 기울기의 절대값이 어디에서나 최대 1이어야 한다. 그러면 이 직선은 어느 지점에서나 상승하거나 하강하는 비율이 한정되어 있다!!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>WGAN의 difficulty는 립시츠 제약을 강제로 적용하는 건데 clipping은 단순한데 약간 문제있어~</p>
<ul>
<li>그 모델은 여전히 품질 낮은 이미지를 만들 수 있고 hyperparameter c가 올바르게 조정되지 않을때 수렴되지 않을 수 있지!</li>
</ul>
<p>model perfomenmce는 htperparameter에 매우 민감해!!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*RlnW0f-Gg8fC17GiUaYwNQ.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Model-capacity">
<a class="anchor" href="#Model-capacity" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model capacity<a class="anchor-link" href="#Model-capacity"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>첫번째 줄은 WGAN</p>
<ul>
<li>capacity 가 감소되었을때 복접한 경계를 만들지 못함</li>
</ul>
<p>두번쨰 줄은 WGAN-GP</p>
<ul>
<li>capacity가 감소되었을때 복잡한 경계를 만듦!</li>
<li>가중 클리핑weightclipping 은 모델 f의 capacity를 줄이고 복잡한 함수들을 모델화하기 위한 가능성을 제약함!</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*eP-QrSB2gfnB42p0ytNy2w.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Wasserstein-GAN-with-gradient-penalty-(WGAN-GP)">
<a class="anchor" href="#Wasserstein-GAN-with-gradient-penalty-(WGAN-GP)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Wasserstein GAN with gradient penalty (WGAN-GP)<a class="anchor-link" href="#Wasserstein-GAN-with-gradient-penalty-(WGAN-GP)"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>WGAN-GP는 weight clipping 대신 기울기 패널티를 사용해서 립시츠 제약을 적용함

$$||f(x_1)-f(x_2)| \le K|x_1 - x_2|$$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>WGAN을 WGAN-GP로 바꾸는 조건!!</p>
<ol>
<li>critic loss 함수에 기울기 패널티 항을 포함한다.</li>
<li>critic의 가중치를 클리핑하지 않는다.</li>
<li>critic에 배치 정규화 층을 사용하지 않는다.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>$\star$ 배치 정규화</p>
<ul>
<li>배치 정규화는 같은 <strong>배치 안의 이미지 사이에 상관관계를 만들기 때문에</strong> 기울기 패널티 손실의 효과가 떨어진다. 실험을 해보면 critic에서 배치 정규화를 사용하지 않더라도 WGAN-GP이 여전히 훌률한 결과를 만든다는 것을 알 수 있다</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Gradient-penalty">
<a class="anchor" href="#Gradient-penalty" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient penalty<a class="anchor-link" href="#Gradient-penalty"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>입력 이미지에 대한 예측의 기울기 norm과 1 사이의 차이를 제곱한 것</li>
<li>이 모델은 자연스럽게 기울기 패널티 항을 최소화하는 가중치를 찾으려고 하고 이는 립시츠 제약을 따르도록 만듦</li>
<li>학습 과정의 모든 곳에서 기울기를 계산하는 것은 힘들다. 개신 WGAN-GP는 일부 지점만 기울기를 계산한다.</li>
<li>한쪽으로 치우치지 않기 위해 진짜 이미지와 가짜 이미지 쌍을 연결한 직선을 따라 무작위로 포인트를 선택해 보간interpolation 이미지들을 사용한다.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Algorithm">
<a class="anchor" href="#Algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Algorithm<a class="anchor-link" href="#Algorithm"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://miro.medium.com/max/1400/1*yYvwVzRnlVmRFCh7-JOASw.png" alt=""></p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/chch/wasserstein%20gan/gan/2022/02/13/WGAN.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/chch/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/chch/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/chch/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/seoyeonc" target="_blank" title="seoyeonc"><svg class="svg-icon grey"><use xlink:href="/chch/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
