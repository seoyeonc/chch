<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Instance Selection for GANs | Seoyeon Choi</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Instance Selection for GANs" />
<meta name="author" content="최서연" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Terrance DeVries, Michal Drozdzal, Graham W. Taylor" />
<meta property="og:description" content="Terrance DeVries, Michal Drozdzal, Graham W. Taylor" />
<link rel="canonical" href="https://seoyeonc.github.io/chch/2022/01/31/Instance-Selection-for-GANs.html" />
<meta property="og:url" content="https://seoyeonc.github.io/chch/2022/01/31/Instance-Selection-for-GANs.html" />
<meta property="og:site_name" content="Seoyeon Choi" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-31T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Terrance DeVries, Michal Drozdzal, Graham W. Taylor","url":"https://seoyeonc.github.io/chch/2022/01/31/Instance-Selection-for-GANs.html","@type":"BlogPosting","headline":"Instance Selection for GANs","dateModified":"2022-01-31T00:00:00-06:00","datePublished":"2022-01-31T00:00:00-06:00","author":{"@type":"Person","name":"최서연"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://seoyeonc.github.io/chch/2022/01/31/Instance-Selection-for-GANs.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/chch/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://seoyeonc.github.io/chch/feed.xml" title="Seoyeon Choi" /><link rel="shortcut icon" type="image/x-icon" href="/chch/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/chch/">Seoyeon Choi</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/chch/about/">About Me</a><a class="page-link" href="/chch/search/">Search</a><a class="page-link" href="/chch/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Instance Selection for GANs</h1><p class="page-description">Terrance DeVries, Michal Drozdzal, Graham W. Taylor</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-01-31T00:00:00-06:00" itemprop="datePublished">
        Jan 31, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">최서연</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      12 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/seoyeonc/chch/tree/master/_notebooks/2022-01-31-Instance Selection for GANs.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/chch/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/seoyeonc/chch/master?filepath=_notebooks%2F2022-01-31-Instance+Selection+for+GANs.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/chch/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/seoyeonc/chch/blob/master/_notebooks/2022-01-31-Instance Selection for GANs.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/chch/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Abstract">Abstract </a></li>
<li class="toc-entry toc-h2"><a href="#Instance-Selection-for-GAN">Instance Selection for GAN </a></li>
<li class="toc-entry toc-h2"><a href="#Experiments">Experiments </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Evaluation-Metrics">Evaluation Metrics </a></li>
<li class="toc-entry toc-h3"><a href="#Relationship-Between-Dataset-Manifold-Density-and-GAN-Performance">Relationship Between Dataset Manifold Density and GAN Performance </a></li>
<li class="toc-entry toc-h3"><a href="#Embedding-and-Scoring-Function">Embedding and Scoring Function </a></li>
<li class="toc-entry toc-h3"><a href="#Retention-Ratio">Retention Ratio </a></li>
<li class="toc-entry toc-h3"><a href="#128-×-128-ImageNet">128 × 128 ImageNet </a></li>
<li class="toc-entry toc-h3"><a href="#256-×256-ImageNet">256 ×256 ImageNet </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Instance-Selection-in-Practice">Instance Selection in Practice </a></li>
<li class="toc-entry toc-h2"><a href="#Conclusion">Conclusion </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-31-Instance Selection for GANs.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ref: <a href="https://arxiv.org/pdf/2007.15255.pdf">https://arxiv.org/pdf/2007.15255.pdf</a></p>
<p>git: <a href="https://github.com/uoguelph-mlrg/instance_selection_for_gans">https://github.com/uoguelph-mlrg/instance_selection_for_gans</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Abstract">
<a class="anchor" href="#Abstract" aria-hidden="true"><span class="octicon octicon-link"></span></a>Abstract<a class="anchor-link" href="#Abstract"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Several recently proposed techniques attempt to avoid spurious samples, either by rejecting them after generation, or by truncating the model’s latent space.</p>
<ul>
<li>최근 제안된 기술들은 가짜 샘플들을 피하려는 시도는 생성 후 거절하거니 모델의 잠재 공간을 잘라내는 것임.</li>
<li>효과적이긴 한데 모델의 대부분이 사용되지 않는 샘플에 할당</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>altering the training dataset via instance selection before model training has taken place.</p>
<ul>
<li>그래서 모델 학습이 일어나기 전에 인스턴스 선택을 통해 학습셋을 변경하는 것을 제안할 것</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Instance-Selection-for-GAN">
<a class="anchor" href="#Instance-Selection-for-GAN" aria-hidden="true"><span class="octicon octicon-link"></span></a>Instance Selection for GAN<a class="anchor-link" href="#Instance-Selection-for-GAN"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>to automatically remove the sparsest regions of the data manifold, specifically those parts that GANs struggle to capture. </li>
<li>define an image embedding function F and a scoring function H.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Embedding function</strong></p>
<ul>
<li>F projects images into an embedding space<ul>
<li>image z data set이 주어지면, $z = F(x)$를 data point $x ∈ X $에 적용하여 embeded image Z 가 주어진다.</li>
<li>image  generation을 위해 사전 학습된 image classifier의 feature space와 같은 aligned embedding function을 제안하고 있다.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Scoring function</strong></p>
<ul>
<li>H is used to to assess the manifold density in a neighbourhood around each embedded data point z.<ul>
<li>논문에서 비교할 세 가지 scoring function selection<ul>
<li>*log likelihood under a standard Gaussian model,</li>
<li>log likelihood under a Probabilistic Principal Component Analysis (PPCA) model,</li>
<li>distance to the Kth nearest neighbour (KNN Distance).</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Gaussian model is fit to the <em>embedded dataset by computing the empirical mean $µ$ and the sample covariance $Σ$ of $Z$.</em></p>
<ul>
<li>d는 z의 demension</li>
</ul>
<p>
$$H_{Gaussian}(z) = −\frac{1}{2}[ln(|Σ|) + (z − µ)^{T} Σ^{−1}(z − µ) + d ln(2π)], (1)$$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>논문 설정: set the number of principal components such that 95% of the variance in the data is preserved.</li>
</ul>
<p>
$$H_{PPCA}(z) = −\frac{1}{2}[ln(|C|) + Tr((z − µ)^{T} C^{−1}(z − µ)) + d ln(2π)], C = WW^T + σ^2 I, (2)$$
</p>
<ul>
<li>$W$ is the fit model weight matrix,</li>
<li>$µ$ is the empirical mean of $Z$, </li>
<li>$σ$ is the residual variance, </li>
<li>$I$ is the identity matrix, </li>
<li>$d$ is the dimension of $z$.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>KNN</strong></p>
<ul>
<li>$z$와 $Z \ {z}$의 유클리드 거리 계산 후 가장 가까운 k번째 원소까지 거리 반환해 data point 얻는데 사용한다.</li>
<li>To convert to a score, we make the resulting distance negative, such that smaller distances return larger values. </li>
</ul>
<p>
$$H_{KNN}(z, K, Z) = − min\underset{K}  \{||z − z_i ||_2 : z_i ∈ Z \ {z} \}, (3)$$
</p>
<ul>
<li>집합에서 k번째 가장 작은 값.$\leftarrow$논문에서는 k=5로 정함</li>
<li>To perform <em>instance selection</em>, we compute scores $H(F(x))$ for each data point and keep all data points with scores above some threshold $ψ$.</li>
<li>For convenience, *we often set $ψ$ to be equal to some percentile of the scores, such that we preserve the top N% of the best scoring data points</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figure 1에서 High likelihood images share a similar visual structure, while low likelihood samples are more varied 였음!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>
$$X' = {x ∈ X s.t. H(F(x)) &gt; ψ}$$
</p>
<ul>
<li>data points $x ∈ X$ 의 초기 학습 set을 구성함으로써 reduced training set $X'$를 구성함</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figure 1에서 ImageNet의 Red Fox class 에서 most and least likely imgaed를 보면 training set으로부터 data points를 제거하는 것이 좋은 이유가 설명된다.</p>
<p>Likelihood는 pretrain된 Inceptionv3 classifier에서 feature embedding에 적합한 가우시안모델에 의해 결정된다.</p>
<ul>
<li>The most likely images (a) are similarly cropped around the fox’s face, while the least likely images (b) have many odd viewpoints and often suffer from occlusion. It is logical to imagine how a generative model trained on these unusual instances may try to generate samples that mimic such conditions, resulting in undesirable outputs.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Experiments">
<a class="anchor" href="#Experiments" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experiments<a class="anchor-link" href="#Experiments"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>review evaluation metrics,</li>
<li>motivate selecting instances based on manifold density, </li>
<li>analyze the impact of applying instance selection to GAN training.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Evaluation-Metrics">
<a class="anchor" href="#Evaluation-Metrics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluation Metrics<a class="anchor-link" href="#Evaluation-Metrics"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>When calculating FID we follow Brock et al. [2] in using all images in the training set <em>to estimate the reference distribution</em>, and <em>sampling 50 k images</em> to make up the generated distribution.</li>
<li>For P&amp;R and D&amp;C we use an Inceptionv3 embedding.</li>
<li>1 N and M are <strong>set to 10 k samples</strong> for both the reference and generated distributions, and K is <strong>set equal to 5</strong> as recommended by Naeem et al. [19]</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Relationship-Between-Dataset-Manifold-Density-and-GAN-Performance">
<a class="anchor" href="#Relationship-Between-Dataset-Manifold-Density-and-GAN-Performance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Relationship Between Dataset Manifold Density and GAN Performance<a class="anchor-link" href="#Relationship-Between-Dataset-Manifold-Density-and-GAN-Performance"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>image manifold는 많은 data point들이 서로 가까이에 있는 영역에서보다 정확히 정의된다.</li>
<li>GAN은 주어진 dataset의 data point를 기반으로 image manifold를 재현하려고 시도하기 떄문에 잘 정의된 manifold(no sparse manifold regions)가 있는 dataset에서 더 나은 성능을 발휘해야 한다고 suspect한다.<ul>
<li>그래서 use the ImageNet2 dataset [7] and treat each of the 1000 classes as a separate dataset 할거다</li>
<li>use a single class-conditional BigGAN from [2] that has been pretrained on ImageNet at 128 × 128 resolution. </li>
<li>For each class, we sample 700 real images from the dataset, and generate 700 class-conditioned samples with the BigGAN.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To measure the density for each class manifold we compare three different methods:</p>
<ul>
<li>Gaussian likelihood, </li>
<li>Probabilistic Principal Component Analysis (PPCA) likelihood,</li>
<li>and distance to the Kth neighbour (KNN Distance) (§3).</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://d3i71xaburhd42.cloudfront.net/d534182c1a64143e74e9f00fd7394b9223fe62a0/5-Figure2-1.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figure 2. image data set의 각 class에 대한 manifold 밀도 추정치와 FID 사이의 상관관계. x측 값이 낮을수록 dataset manifold의 밀도가 높다는 것을 나타냄. y축 값이 낮을수록 sample의 품질이 우수함을 나타냄.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Embedding-and-Scoring-Function">
<a class="anchor" href="#Embedding-and-Scoring-Function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Embedding and Scoring Function<a class="anchor-link" href="#Embedding-and-Scoring-Function"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>dataset msnifold가 GAN 성능과 상관있다는 것을 확인하면 data manifold의 저밀도 영역에 놓여진 data point를 제거하여 training set의 전체 밀도를 인위적으로 증가시킨다.</strong></p>
<ul>
<li>ImageNet에서 64 * 64 해상도로 여러 개의 Self-Attention GANs (SAGAN) train하기</li>
<li>Each model is trained on <em>a different 50% subset of ImageNet</em>, as chosen by instance selection using different embedding and scoring functions.</li>
<li>instance 선택으로 class 별로 이루어진다.</li>
<li>use the default settings for SAGAN</li>
<li>use a batch size of 128</li>
<li>apply the self-attention module at 32 × 32 resolution</li>
<li>All models are trained for 200k iterations. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://d3i71xaburhd42.cloudfront.net/d534182c1a64143e74e9f00fd7394b9223fe62a0/5-Table1-1.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Table 1. Comparison of embedding and scoring functions on 64 × 64 ImageNet image generation task.</p>
<ul>
<li>Models trained with instance selection significantly outperform models trained without instance selection, despite training on a fraction of the available data.<ul>
<li>인스턴스 선택으로 훈련된 모델은 사용가능한 데이터의 일부의 훈련에도 인스턴스 선택없이 훈련된 모델보다 상당히 뛰어남</li>
</ul>
</li>
<li>RR is the retention ratio (percentage of dataset trained on). Best results in bold.</li>
</ul>
<p><strong>All runs utilizing instance selection significantly outperform the baseline model trained on the full dataset, despite only having access to half as much training data</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>We observe a large increase in image fidelity, as indicated by the improvements in Inception Score, Precision, and Density, and a slight drop in overall diversity, as measured by Recall.</li>
<li>Coverage, which measures realism-constrained diversity, benefits greatly from the more realistic samples and thus sees an increase, despite the reduction in overall diversity. </li>
<li>Since the increase in image quality is much greater than the decrease in diversity, FID also improves. </li>
<li>To verify that the gains are not simply caused by the reduction in dataset size we train a model on a 50% subset that was uniform-randomly sampled from the full dataset. </li>
<li>Here, we observe little change in performance compared to the baseline, indicating that performance improvements are <strong>indeed due to careful selection of training data</strong>, rather than the reduction of dataset size.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>table 1의 결과로 미루어보아 all three candidate scoring functions: Gaussian likelihood, PPCA likelihood, and KNN distance, significantly outperform the full dataset baseline를 알 수 있었음</p>
<ul>
<li>그 중 가우시안이 가장 좋아보여서 점수 함수로 사용할 거(Gaussian likelihood slightly outperforms the alternatives, so we use it as the scoring function in the remainder of our experiments. )</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>여러가지 임베딩 비교해볼 예정</p>
<ul>
<li>Inceptionv3 [28] trained on ImageNet, ResNet50 [9] trained on Places365 [40], ImageNet, and with SwAV unsupervised pretraining [4], and ResNeXt-101 32x8d [34] trained with weak supervision on Instagram 1B [15]. </li>
<li>compare a randomly initialized Inceptionv3 with no pretraining as a random embedding. </li>
</ul>
<p>For all architectures features are extracted after the global average pooling layer.</p>
<ul>
<li>모든 아키텍터 특징들은 global average pooling layer 후 extract될 거</li>
</ul>
<p>We find that all feature embeddings improve performance over the full dataset baseline except for the randomly initialized network. These results suggest that an embedding function that is well aligned with the target domain is required in order for instance selection to be effective.</p>
<ul>
<li>instance selection이 효과적이려면 target domain과 잘 정렬된 embedding function이 필요하다는 것을 알았다.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Retention-Ratio">
<a class="anchor" href="#Retention-Ratio" aria-hidden="true"><span class="octicon octicon-link"></span></a>Retention Ratio<a class="anchor-link" href="#Retention-Ratio"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Instance selection을 수행할떄 가장 중요한 고려 사항은 보존 비율이라 불리는 hyperparameter인 원래 dataset의 비율을 결정하는 것!</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://d3i71xaburhd42.cloudfront.net/d534182c1a64143e74e9f00fd7394b9223fe62a0/6-Figure3-1.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figure 3.  SAGAN trained on 64 × 64 ImageNet, with instance selection used to reduced the dataset by varying amounts.</p>
<ul>
<li>retention ratio 보존 비율=100은 전체 dataset에 대해 train된 model을 나타냄, 즉 no instance selection</li>
<li>The application of instance selection boosts overall performance significantly.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://d3i71xaburhd42.cloudfront.net/d534182c1a64143e74e9f00fd7394b9223fe62a0/7-Figure4-1.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figure 4. Samples of bird classes from SAGAN trained on 64×64 ImageNet.</p>
<ul>
<li>Each row is conditioned on a different class.</li>
<li>Red borders indicate misclassification by a row-specific pretrained Inceptionv3 classifier. </li>
<li><strong>Instance selection (b) significantly improves sample fidelity and class consistency compared to the baseline (a).</strong></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our best performing SAGAN model in terms of FID was trained on only 40% of the ImageNet dataset, yet outperforms FQ-BigGAN [39], the current state-of-the-art model for the task of 64 × 64 ImageNet generation.</p>
<p>Despite using 2× less parameters and a 4× smaller batch size, our SAGAN achieves a better FID (9.07 vs. 9.76).</p>
<p>Figure 4에서 볼 수 있듯이 instance selection model의 sample이 full dataset에서 train된 기준 model의 sample보다 더 잘 인식한다</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="128-×-128-ImageNet">
<a class="anchor" href="#128-%C3%97-128-ImageNet" aria-hidden="true"><span class="octicon octicon-link"></span></a>128 × 128 ImageNet<a class="anchor-link" href="#128-%C3%97-128-ImageNet"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>이 장의 목적: To examine the impact of instance selection on the training time of large-scale models, we train two BigGAN models on 128 × 128 ImageNet3 .</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>uses the default hyperparameters from BigGAN with the exception that we reduce the channel multiplier from 96 to 64</li>
<li>use a single discriminator update instead of two for faster training</li>
<li>baseline BigGAN으로 우수한 성능을 이루는데 large batch가 중요하긴 하지만 instance selection과 결합하면 성능이 저하된다는 것이 발견됨.(Although large batch sizes are critical for achieving good performance with the baseline BigGAN [2], we found them to degrade performance when combined with instance selection. )</li>
<li>Therefore, we reduce the batch size from BigGAN’s default of 2048 to 256 for the instance selection model</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Despite using a much smaller batch size, our model trained with instance selection outperforms the baseline in all metrics except for Recall (Table 2), as expected due to the diversity/fidelity trade-off.</p>
<ul>
<li>훨씬 작은 배치 사이즈를 사용했지만 instance model로 훈련된 모델이 다양성/충실성 trade-off 때문에 예상대로 리콜을 제외한 모든 매트릭스에서 기준치를 능가한다.</li>
</ul>
<p>The instance selection model trains significantly faster than the baseline, requiring less than four days while the baseline requires more than two weeks.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Figure 5: Samples from BigGAN trained on 256 × 256 ImageNet, with the truncation trick. Samples are selected to demonstrate the highest quality outputs for each model. The baseline model (a) struggles to produce convincing facial details, which the instance selection model (b) successfully achieves.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="256-×256-ImageNet">
<a class="anchor" href="#256-%C3%97256-ImageNet" aria-hidden="true"><span class="octicon octicon-link"></span></a>256 ×256 ImageNet<a class="anchor-link" href="#256-%C3%97256-ImageNet"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>절대 시도 못할 조건..</p>
<ul>
<li>To further demonstrate instance selection we train a BigGAN on ImageNet at 256 × 256 resolution using 4 V100s with 32GB of RAM each.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Instance-Selection-in-Practice">
<a class="anchor" href="#Instance-Selection-in-Practice" aria-hidden="true"><span class="octicon octicon-link"></span></a>Instance Selection in Practice<a class="anchor-link" href="#Instance-Selection-in-Practice"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As the experiments have shown, instance selection stands as a useful tool for trading away sample diversity in exchange for improvements in image fidelity, faster training, and lower model capacity requirements. We believe that this trade-off is a worthwhile hyperparameter to tune in consideration of the available compute budget, just as it is common practice to adjust model capacity or batch size to fit within the memory constraints of the available hardware.</p>
<ul>
<li>instance selection은 이미지 충실도 개선, faster training, 낮은 capacity로 sample 가양성을 교확하는 유용한 tool.</li>
</ul>
<p>The control over the diversity/fidelity trade-off afforded by instance selection also yields a tool that can be used to better understand the behaviour and limitations of existing evaluation metrics. For instance, in some cases when applying instance selection, we observed that certain diversity-sensitive metrics (such as FID and Coverage) improved, even though the diversity of the training set had been significantly reduced. We leave it for future work to determine whether this is a limitation of these metrics, or a behaviour that should be expected.</p>
<p>Finally, instance selection can be used to automatically curate new datasets for the task of image generation. Existing datasets that are designed for image synthesis often use manual filtering and hand-crafted cropping and alignment tools to increase the dataset manifold density [11]. As an alternative to these time-intensive procedures, instance selection provides a generic solution that can quickly be applied to any uncurated set of images.</p>
<ul>
<li>instance selection은 uncurated set에 빠르게 적용할 수 있는 일반적인 solution을 제공한다.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">
<a class="anchor" href="#Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion<a class="anchor-link" href="#Conclusion"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our motivation is to remove sparse regions of the data manifold before training, acknowledging that they will ultimately be poorly represented by the GAN, and therefore, that attempting to capture them is an inefficient use of model capacity.</p>
<p>There are multiple benefits of taking the instance selection approach: 충실도, 시간 단축</p>
<ol>
<li>We improve sample fidelity across a variety of metrics compared to training on uncurated data; </li>
<li>We demonstrate that reallocating model capacity to denser regions of the data manifold leads to efficiency gains, meaning that we can achieve SOTA quality with smaller-capacity models trained in far less time. </li>
</ol>
<ul>
<li>
<p>To our knowledge, instance selection has not yet been formally analyzed in the generative setting.</p>
</li>
<li>
<p><strong>We have only considered the setting where curation is performed up-front, prior to training.</strong></p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Training models with instance selection results in improvements to image quality, as well as significant reduction in training time and model capacity requirements. On 128x128 ImageNet, instance selection reduces training time by 4x compared to the baseline, and achieves better image quality than a model trained with twice the model capacity. On 256x256 ImageNet, a model trained with instance selection produces higher fidelity images than a model with twice the capacity, while also using approximately one order of magnitude less multiply-accumulate operations (MACS) throughout the duration of training.</p>
<p>인스턴스 선택을하여 모델을 훈련시키면이미지 품질이 향상되고 훈련 시간과 모델 용량 요구사항(GPU )도 감소함</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/chch/2022/01/31/Instance-Selection-for-GANs.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/chch/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/chch/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/chch/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/seoyeonc" target="_blank" title="seoyeonc"><svg class="svg-icon grey"><use xlink:href="/chch/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
