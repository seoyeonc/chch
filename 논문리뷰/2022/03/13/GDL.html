<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Geometric deep learning going beyond Euclidean data | Seoyeon Choi</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Geometric deep learning going beyond Euclidean data" />
<meta name="author" content="최서연" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst" />
<meta property="og:description" content="Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst" />
<link rel="canonical" href="https://seoyeonc.github.io/chch/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/2022/03/13/GDL.html" />
<meta property="og:url" content="https://seoyeonc.github.io/chch/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/2022/03/13/GDL.html" />
<meta property="og:site_name" content="Seoyeon Choi" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-13T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst","url":"https://seoyeonc.github.io/chch/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/2022/03/13/GDL.html","@type":"BlogPosting","headline":"Geometric deep learning going beyond Euclidean data","dateModified":"2022-03-13T00:00:00-06:00","datePublished":"2022-03-13T00:00:00-06:00","author":{"@type":"Person","name":"최서연"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://seoyeonc.github.io/chch/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/2022/03/13/GDL.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/chch/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://seoyeonc.github.io/chch/feed.xml" title="Seoyeon Choi" /><link rel="shortcut icon" type="image/x-icon" href="/chch/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/chch/">Seoyeon Choi</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/chch/about/">About Me</a><a class="page-link" href="/chch/search/">Search</a><a class="page-link" href="/chch/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Geometric deep learning going beyond Euclidean data</h1><p class="page-description">Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-03-13T00:00:00-06:00" itemprop="datePublished">
        Mar 13, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">최서연</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      26 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/chch/categories/#논문리뷰">논문리뷰</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/seoyeonc/chch/tree/master/_notebooks/2022-03-13-GDL.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/chch/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/seoyeonc/chch/master?filepath=_notebooks%2F2022-03-13-GDL.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/chch/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/seoyeonc/chch/blob/master/_notebooks/2022-03-13-GDL.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/chch/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#I.-INTRODUCTION">I. INTRODUCTION </a></li>
<li class="toc-entry toc-h3"><a href="#II.-GEOMETRIC-LEARNING-PROBLEMS">II. GEOMETRIC LEARNING PROBLEMS </a></li>
<li class="toc-entry toc-h3"><a href="#III.-DEEP-LEARNING-ON-EUCLIDEAN-DOMAINS">III. DEEP LEARNING ON EUCLIDEAN DOMAINS </a></li>
<li class="toc-entry toc-h3"><a href="#IV.-THE-GEOMETRY-OF-MANIFOLDS-AND-GRAPHS">IV. THE GEOMETRY OF MANIFOLDS AND GRAPHS </a></li>
<li class="toc-entry toc-h3"><a href="#V.-SPECTRAL-METHODS">V. SPECTRAL METHODS </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-03-13-GDL.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds.<ul>
<li>기하학적 딥러닝: 그래프나 매니폴드같은 비유클리디언 도메인에서 구조화된 깊은 신경망 모델을 일반화하는 등장하는 기술 시도를 포괄하는 용어</li>
</ul>
</li>
<li>The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="I.-INTRODUCTION">
<a class="anchor" href="#I.-INTRODUCTION" aria-hidden="true"><span class="octicon octicon-link"></span></a>I. INTRODUCTION<a class="anchor-link" href="#I.-INTRODUCTION"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The use of convolutions has a two-fold effect. <ul>
<li>First, it allows extracting local features that are shared across the image domain and greatly reduces the number of parameters in the network with respect to generic deep architectures (and thus also the risk of overfitting), without sacrificing the expressive capacity of the network.</li>
<li>적은 수의 파라메터, 로컬 특징 추출</li>
<li>Second, the convolutional architecture itself imposes some priors about the data, which appear very suitable especially for natural images</li>
<li>자연 이미지에 적합한 우선순위 부과</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The non-Euclidean nature of such data implies that there are no such familiar properties as global parameterization, common system of coordinates, vector space structure, or shift-invariance. </li>
<li>Consequently, basic operations like convolution that are taken for granted in the Euclidean case are even not well defined on non-Euclidean domains. <ul>
<li>유클리드레서 당연한 컨벌루션같은 기본 연산은 비유클리드 도메인에서 잘 정의되지 않음!</li>
</ul>
</li>
<li>The purpose of our paper is to show different methods of translating the key ingredients of successful deep learning methods such as convolutional neural networks to non-Euclidean data.<ul>
<li>목적: 비유클리디안 데이터에 컨벌루션 신경망같은 성공적인 딥러닝 방법의 주요 구성을 변환하는 다양한 방법을 보여주기~</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="II.-GEOMETRIC-LEARNING-PROBLEMS">
<a class="anchor" href="#II.-GEOMETRIC-LEARNING-PROBLEMS" aria-hidden="true"><span class="octicon octicon-link"></span></a>II. GEOMETRIC LEARNING PROBLEMS<a class="anchor-link" href="#II.-GEOMETRIC-LEARNING-PROBLEMS"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Broadly speaking, we can distinguish between two classes of geometric learning problems.</li>
<li>In the first class of problems, the goal is to characterize the structure of the data.<ul>
<li>목적은 데이터 구조를 특성화하는 것</li>
</ul>
</li>
<li>The second class of problems deals with analyzing functions defined on a given non-Euclidean domain. </li>
<li>These two classes are related, since understanding the properties of functions defined on a domain conveys certain information about the domain, and vice-versa, the structure of the domain imposes certain properties on the functions on it.<ul>
<li>도메인에 정의된 함수의 특성을 이해하는 것은 도메인에 대한 특정 정보를 전달하고, 그 반대로 도매안의 구조는 도메인에서 함수에 특성 속성을 부과하기 때문에 구 클래스는 연관이 있다.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Structure of the domain:</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Many methods for nonlinear dimensionality reduction consist of two steps: <ul>
<li>first, they start with constructing a representation of local affinity of the data points (typically, a sparsely connected graph). <ul>
<li>데이터 포인트의 로컬 관련성 구성</li>
</ul>
</li>
<li>Second, the data points are embedded into a low-dimensional space trying to preserve some criterion of the original affinity.<ul>
<li>데이터 포인트는 저차원 공간에 임베드됨</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Instead of embedding the vertices, the graph structure can be processed by decomposing it into small sub-graphs called motifs [36] or graphlets [37].</p>
<ul>
<li>정점 임베딩하는 대신 그래프 구조는 모티브나 그래프릿같은 작은 서브 그래프에서 분해함으로써 처리 가능</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>In network analysis applications such as computational sociology, the topological structure of the social graph representing the social relations between people carries important insights allowing, for example, to classify the vertices and detect communities [40].<ul>
<li>네트워크 분석 어플리케이션은 정점을 분류하고 커뮤니티 탐지</li>
</ul>
</li>
<li>In natural language processing, words in a corpus can be represented by the co-occurrence graph, where two words are connected if they often appear near each other [41].<ul>
<li>자연어 처리에서는 두 단어가 서로 근처에 나타나면 연결된 동시 발생 그래프로 표현 가능</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Data on a domain</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Our second class of problems deals with analyzing functions defined on a given non-Euclidean domain.</li>
<li>We can further break down such problems into two subclasses: problems where the domain is fixed and those where multiple domains are given.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>In computer graphics and vision applications, finding similarity and correspondence between shapes are examples of the second sub-class of problems: each shape is modeled as a manifold, and one has to work with multiple such domains.<ul>
<li>각 모양은 매니폴드로 모델와되고, 하나는 여러 도메인에서 작업해야 한다.</li>
</ul>
</li>
<li>In this setting, a generalization of convolution in the spatial domain using local charting [46], [47], [48] appears to be more appropriate.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Brief history</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The main focus of this review is on this second class of problems, namely learning functions on non-Euclidean structured domains, and in particular, attempts to generalize the popular CNNs to such settings.</li>
<li>First attempts to generalize neural networks to graphs we are aware of are due to Scarselli et al. [49], who proposed a scheme combining recurrent neural networks and random walk models.<ul>
<li>그래프에서 신경망을 일반화하려는 첫번째 시도는 Scarselli이 함~ 임의보행 모델과 반복 신경망 모델의 결합을 제안함</li>
</ul>
</li>
<li>The first formulation of CNNs on graphs is due to Bruna et al. [52], who used the definition of convolutions in the spectral domain.<ul>
<li>그레프에서 cnn을 첫번째 공식화한 건 Bruna, 스펙트럼 도메인에서 컨벌루션 정의를 사용!</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Their paper, while being of conceptual importance, came with significant computational drawbacks that fell short of a truly useful method.<ul>
<li>유용한데 계산상의 결점이 존재.</li>
</ul>
</li>
<li>These drawbacks were subsequently addressed in the followup works of Henaff et al.[44] and Defferrard et al. [45].</li>
<li>In the latter paper, graph CNNs allowed achieving some state-of-the-art results.<ul>
<li>후에 해결책 제시하는듯?!</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>In a parallel effort in the computer vision and graphics community, Masci et al. [47] showed the first CNN model on meshed surfaces, resorting to a spatial definition of the convolution operation based on local intrinsic patches.<ul>
<li>평행적 노력: 로컬 고유 패치를 기반으로 한 컨벌루션 연산의 공간적 정의에 의존한 매쉬드 표면에서 첫번째 CNN 모델을 보임</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The interest in deep learning on graphs or manifolds has exploded in the past year, resulting in numerous attempts to apply these methods in a broad spectrum of problems ranging from biochemistry [55] to recommender systems [56].</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Structure of the paper</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Going to the non-Euclidean world in Section IV, we then define basic notions in differential geometry and graph theory.</li>
<li>These topics are insufficiently known in the signal processing community, and to our knowledge, there is no introductorylevel reference treating these so different structures in a common way.</li>
<li>One of our goals is to provide an accessible overview of these models resorting as much as possible to the intuition of traditional signal processing.</li>
<li>In Sections V–VIII, we overview the main geometric deep learning paradigms, emphasizing the similarities and the differences between Euclidean and non-Euclidean learning methods.</li>
<li>In Section IX, we show examples of selected problems from the fields of network analysis, particle physics, recommender systems, computer vision, and graphics. </li>
<li>In Section X, we draw conclusions and outline current main challenges and potential future research directions in geometric deep learning. </li>
<li>To make the paper more readable, we use inserts to illustrate important concepts.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="III.-DEEP-LEARNING-ON-EUCLIDEAN-DOMAINS">
<a class="anchor" href="#III.-DEEP-LEARNING-ON-EUCLIDEAN-DOMAINS" aria-hidden="true"><span class="octicon octicon-link"></span></a>III. DEEP LEARNING ON EUCLIDEAN DOMAINS<a class="anchor-link" href="#III.-DEEP-LEARNING-ON-EUCLIDEAN-DOMAINS"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Geometric priors</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Consider a compact d-dimensional Euclidean domain $\Omega = [0; 1]^d \subset \mathbb{R}^d $ on which squareintegrable functions $f \in L2(\Omega)$ are defined<ul>
<li>제곱할 수 있는 함수가 정의된 컴책트한 d차원의 유클리디안 도메인을 고려해보자.</li>
</ul>
</li>
<li>We consider a generic supervised learning setting, in which an unknown8 function $y : L^2(\Omega) \rightarrow \cal{Y}$ is observed on a training set<ul>
<li>$\{ f_i \in L^2 (\Omega) , y_i = y(f_i )  \}_{i \in \cal{I}}$..(1)</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>In a supervised classification setting, the target space $\cal{Y}$ can be thought discrete with $| \cal{Y} |$ being the number of classes. </li>
<li>In a multiple object recognition setting, we can replace $\cal{Y}$ by the $K$-dimensional simplex, which represents the posterior class probabilities $p(y|x)$. In regression tasks, we may consider $\cal{Y} = \mathbb{R}^m$.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Stationarity</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $\cal{T}vf(x) = f(x - v),$  $x; v \in \Omega$ (2) be a translation operator acting on functions $f \in L^2 (\Omega)$</li>
<li>Our first assumption is that the function y is either invariant or equivariant with respect to translations, depending on the task.</li>
<li>In the former case, we have $y(\cal{T}_v f) = y(f)$ for any $f \in L^2(\Omega)$ and $v \in \Omega$. </li>
<li>This is typically the case in object classification tasks. </li>
<li>In the latter, we have $y(\cal{T}_v f) = \cal{T}_v y(f)$, which is welldefined when the output of the model is a space in which translations can act upon (for example, in problems of object8 localization, semantic segmentation, or motion estimation).</li>
<li><em>Our definition of invariance should not be confused with the traditional notion of translation invariant systems in signal processing, which corresponds to translation equivariance in our language (since the output translates whenever the input translates).</em></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Local deformations and scale separation</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Similarly, a deformation $\cal{L}_{\cal{T}}$, where $\tau : \Omega \rightarrow \Omega$ is a smooth vector field, acts on $L^2 (\Omega)$ as $\cal{L}_{\cal{T}} f(x) = f(x - \tau(x))$.</li>
<li>deformation can model local translations, changes in point of view, rotations and frequency transpositions</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Most tasks studied in computer vision are not only translation invariant/equivariant, but also stable with respect to local deformations [57], [18]. </li>
<li>In tasks that are translation invariant we have<ul>
<li>$| y(\cal{L}_{\cal{T}} f) - y(f)| \approx \| \bigtriangledown \tau \|$, (3)</li>
<li>for all $f$, $\tau$ .</li>
</ul>
</li>
<li>Here, $\| \bigtriangledown \tau \|$ measures the smoothness of a given deformation field.</li>
<li>In other words, the quantity to be predicted does not change much if the input image is slightly deformed.</li>
<li>In tasks that are translation equivariant, we have<ul>
<li>$| y(\cal{L}_{\cal{T}} f) - \cal{L}_{\cal{T}}y(f)| \approx \| \bigtriangledown \tau \|$, (4)</li>
</ul>
</li>
<li>This property is much <strong>stronger</strong> than the previous one, since the space of local deformations has a high dimensionality, as opposed to the d-dimensional translation group.</li>
<li>It follows from (3) that we can extract sufficient statistics at a lower spatial resolution by downsampling demodulated localized filter responses without losing approximation power.</li>
<li>An important consequence of this is that long-range dependencies can be broken into multi-scale local interaction terms, leading to hierarchical models in which spatial resolution is progressively reduced. </li>
<li>To illustrate this principle, denote by<ul>
<li>$Y (x_1; x2; v) = Prob(f(u) = x1 \text{ and } f(u + v) = x2)$ (5)</li>
<li>the joint distribution of two image pixels at an offset $v$ from each other.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>In the presence of long-range dependencies, this joint distribution will not be separable for any $v$.</li>
<li>However, the deformation stability prior states that $Y (x_1, x_2; v) \approx Y (x_1, x_2; v(1 + \epsilon ))$ for small $\epsilon$. </li>
<li>In other words, whereas long-range dependencies indeed exist in natural images and are critical to object recognition, they can be captured and down-sampled at different scales. </li>
<li>This principle of stability to local deformations has been exploited in the computer vision community in models other than CNNs,</li>
<li>for instance, deformable parts models [58].</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Convolutional neural networks</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Stationarity and stability to local translations are both leveraged in convolutional neural networks </li>
</ul>
<ul>
<li>The model is said to be deep if it comprises multiple layers, though this notion is rather vague and one can find  examples of CNNs with as few as a couple and as many as hundreds of layers [11].<ul>
<li>다층으로 구성되면 깊다고는 하는데 이 개념은 오히려 모호하고 구 개 혹은 수백개 층으로 CNN의 예를 찾을 수 있음</li>
</ul>
</li>
<li>The output features enjoy translation invariance/covariance depending on whether spatial resolution is progressively lost by means of pooling or kept fixed.<ul>
<li>출력 특징으로는 공간 해상도가 풀링에 의해 점진적으로 손실되는지 고정을 유지하는지에 따라 변형 불변성(위치 변해도 결과 나옴) 및 공변성을 가진다는 것.</li>
</ul>
</li>
<li>Moreover, if one specifies the convolutional tensors to be complex wavelet decomposition operators and uses complex modulus as pointwise nonlinearities, one can provably obtain stability to local deformations [17].</li>
<li>Although this stability is not rigorously proved for generic compactly supported convolutional tensors, it underpins the empirical success of CNN architectures across a variety of computer vision applications [1].</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>A key advantage of CNNs explaining their success in numerous tasks is that the geometric priors on which CNNs are based result in a learning complexity that avoids the curse of dimensionality.<ul>
<li>수많은 연구에서 성공을 설명하는 CNN의 주요 장점은 CNN이 기반으로 하는 기하학적 prior가 차원의 curse를 피하는 학습 복잡성의 결과를 초래한다는 것.</li>
</ul>
</li>
<li>Thanks to the stationarity and local deformation priors, the linear operators at each layer have a constant number of parameters, independent of the input size n (number of pixels in an image). <ul>
<li>stationarity와 국소적 분해 prior 덕분에 선형 연산자는 출력 크기 n에 독립적으로(상관없이) 각 층에서 매개변수의 일정한 수를 가짐 </li>
</ul>
</li>
<li>Moreover, thanks to the multiscale hierarchical property, the number of layers grows at a rate $\cal{O}(log n)$, resulting in a total learning complexity of $\cal{O}(log n)$ parameters.<ul>
<li>다중 스케일 계층적 특징 덕분에 수많은 층은 $\cal{O}(log n)$ 비율로 증가하고, $\cal{O}(log n)$ 매개변수의 전체 학습 복접성을 초래함</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="IV.-THE-GEOMETRY-OF-MANIFOLDS-AND-GRAPHS">
<a class="anchor" href="#IV.-THE-GEOMETRY-OF-MANIFOLDS-AND-GRAPHS" aria-hidden="true"><span class="octicon octicon-link"></span></a>IV. THE GEOMETRY OF MANIFOLDS AND GRAPHS<a class="anchor-link" href="#IV.-THE-GEOMETRY-OF-MANIFOLDS-AND-GRAPHS"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Manifolds</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>a manifold is a space that is locally Euclidean.</li>
<li>Formally speaking, a (differentiable) d-dimensional manifold $\cal{X}$ is a topological space where each point $x$ has a neighborhood that is topologically equivalent (homeomorphic) to a d-dimensional Euclidean space, called the tangent space and denoted by $T_x \cal{X}$.</li>
<li>The collection of tangent spaces at all points (more formally, their disjoint union) is referred to as the tangent bundle and denoted by $T \cal{X}$. </li>
<li>On each tangent space, we define an inner product $&lt;\bullet,\bullet&gt;_{T_x \cal{X}} : T_x \cal{X} \times T_ \cal{x}X \rightarrow \mathbb{R}$, which is additionally assumed to depend smoothly on the position x.</li>
<li>This inner product is called a Riemannian metric in differential geometry and allows performing local measurements of angles, distances, and volumes. </li>
<li>A manifold equipped with a metric is called a Riemannian manifold.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>리만 다양체: 각 점의 접공간 위에 양의 정부호 쌍선형 형식이 주어져, 두 점 사이의 거리를 측정할 수 있는 매끄러운 다양체이다</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The celebrated Nash Embedding Theorem guarantees that any sufficiently smooth Riemannian manifold can be realized in a Euclidean space of sufficiently high dimension [67]. </li>
<li>An embedding is not necessarily unique; two different realizations of a Riemannian metric are called isometries.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://d3i71xaburhd42.cloudfront.net/0e779fd59353a7f1f5b559b9d65fa4bfe367890c/6-Figure1-1.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Isometries do not affect the metric structure of themanifold and consequently, preserve any quantities that can be expressed in terms of the Riemannian metric (called intrinsic).</li>
<li>Conversely, properties related to the specific realization of the manifold in the Euclidean space are called extrinsic.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Calculus on manifolds</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>우리가 관심있는 두 가지<ul>
<li>A scalar field is a smooth real function $f : \cal{X} \rightarrow \mathbb{R}$ on the manifold.</li>
<li>A tangent vector field $F : \cal{X} \rightarrow T\cal{X}$ is a mapping attaching a tangent vector $F(x) \in T_x \cal{X}$ to each point $x$.</li>
</ul>
</li>
<li>We define the Hilbert spaces of scalar and vector fields on manifolds, denoted by $L^2(\cal{X})$ and $L^2(T \cal{X})$, </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>One of the big differences distinguishing classical calculus from differential geometry is a lack of vector space structure on the manifold, prohibiting us from na¨ıvely using expressions like $f(x+dx)$.</li>
<li>The conceptual leap that is required to generalize such notions to manifolds is the need to work locally in the tangent space.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The operator $\bigtriangledown f : L^2(\cal{X}) \rightarrow L^2(T\cal{X})$ in the definition above is called the intrinsic gradient, and is similar to the classical notion of the gradient defining the direction of the steepest change of the function at a point, with the only difference that the direction is now a tangent vector.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The Laplacian can be interpreted as the difference between the average of a function on an infinitesimal sphere around a point and the value of the function at the point itself.<ul>
<li>라플라시안은 point 주위의 극소수 상의 함수의 평균과 점 자체의 함수 값의 차이로 해석될 수 있다.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Graphs and discrete differential operators</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>For simplicity, we will consider weighted undirected graphs, formally defined as a pair $(\cal{V}, \cal{E})$, where $\cal{V} = \{1; \dots n\}$ is the set of $n$ vertices, and $\cal{E} \subseteq \cal{V} \times \cal{V}$ is the set of edges, where the graph being undirected implies that $(i, j) \in \cal{E}$ iff $(j, i) \in \cal{E}$.</li>
<li>Furthermore, we associate a weight $a_i &gt; 0$ with each vertex $i \in \cal{V}$, and a weight $w_{ij} \ge 0$ with each edge $(i, j) \in \cal{E}$.</li>
<li>Real functions $f : \cal{V} \rightarrow \mathbb{R}$ and $F : \cal{E} \rightarrow \mathbb{R}$ on the vertices and edges of the graph, respectively, are roughly the discrete analogy of continuous scalar and tangent vector fields in differential geometry.</li>
<li>We can define Hilbert spaces $L^2(\cal{V})$ and $L^2(\cal{E})$ of such functions by specifying the respective inner products,

$$&lt;f, g&gt;_{L^2(\cal{V})} = \sum_{i\in \cal{V}} a_i f_i g_i ; \text{ (20)}$$


$$&lt;F, G&gt;_{L^2(\cal{E})} = \sum_{i\in \cal{E}} w_{i,j} F_{i,j} G_{i,j} ; \text{ (21)}$$
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let $f \in L^2(\cal{V})$ and $F \in L^2(\cal{E})$ be functions on the vertices and edges of the graphs, respectively.</li>
<li>We can define differential operators acting on such functions analogously to differential operators on manifolds [72]. </li>
<li>The graph gradient is an operator $\bigtriangledown : L^2(\cal{V}) \rightarrow L^2(\cal{E})$ mapping functions defined on vertices to functions defined on edges,

$$(\bigtriangledown f)_{ij} = f_i - f_j , \text{  (22)}$$
</li>
<li>automatically satisfying $(\bigtriangledown f)_{ij} = (\bigtriangledown f)_{ji}$.</li>
<li>The graph divergence is an operator div : $L^2(\cal{E}) \rightarrow L^2(\cal{V})$ doing the converse,

$$(div F )_i = \frac{1}{a_i} \sum_{j:(i,j)\in \cal{E}} w_{ij} F_{ij} \text{  (23)}$$
</li>
<li>It is easy to verify that the two operators are adjoint w.r.t. the inner products (20)–(21), 

$$ &lt;F, \bigtriangledown f&gt; _{L^2(\cal{E})} = &lt;\bigtriangledown * F, f&gt;_{L^2(\cal{V})} = |&lt;-div F, f&gt;_{L^2(\cal{V})}\text{  (24)}$$
</li>
<li>The graph Laplacian is an operator $\bigtriangleup : L^2(\cal{V}) \rightarrow L^2(\cal{V})$ defined as $\bigtriangleup = -div \bigtriangledown$. </li>
<li>Combining definitions (22)–(23), it can be expressed in the familiar form

$$(\bigtriangleup f)_i = \frac{1}{a_i} \sum_{(i,j)\in \cal{E}} w_{ij} (f_i - f_j) \text{  (25)}$$
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Note that formula (25) captures the intuitive geometric interpretation of the Laplacian as the difference between the local average of a function around a point and the value of the function at the point itself.</li>
<li>Denoting by $W = (w_{ij})$ the $n \times n$ matrix of edge weights (it is assumed that $w_{ij} = 0$ if $(i, j) \notin \cal{E})$, by $A = diag(a_1, \dots , a_n)$ the diagonal matrix of vertex weights, and by $D = diag(\sum_{j:j \neq i} w_{ij} )$ the degree matrix, the graph Laplacian application to a function $f \in L^2(\cal{V})$ represented as a column vector $f = (f_1, \dots ,f_n)^\top$ can be written in matrixvector form as

$$\bigtriangleup f = A^{-1} (D - W) f  \text{  (26)}$$
</li>
<li>The choice of $A = I$ in (26) is referred to as the unnormalized graph Laplacian; another popular choice is $A = D$ producing the random walk Laplacian [73].</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Discrete manifolds</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>As we mentioned, there are many practical situations in which one is given a sampling of points arising from a manifold but not the manifold itself.</li>
<li>In computer graphics applications, reconstructing a correct discretization of a manifold from a point cloud is a difficult problem of its own, referred to a meshing (see insert IN2).</li>
<li>In manifold learning problems, the manifold is typically approximated as a graph capturing the local affinity structure.</li>
<li>We warn the reader that the term “manifold” as used in the context of generic data science is not geometrically rigorous, and can have less structure than a classical smooth manifold we have defined beforehand.<ul>
<li>일반적인 데이터 과학에서 사용되는 manifold는 엄격하지도 않고 더 적은 구조를 가질 수 있음</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Fourier analysis on non-Euclidean domains</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The Laplacian operator is a self-adjoint positive-semidefinite operator, admitting on a compact domain an eigendecomposition with a discrete set of orthonormal eigenfunctions $φ_0, φ_1, . . .$ (satisfying $(φ_i, φ_j)_{i^L2(\cal{X}} = δ_{ij} )$ and non-negative real eigenvalues $0 = λ_0 ≤ λ_1 ≤ . . .$ (referred to as the spectrum of the Laplacian),

$$\bigtriangleup \phi_i = \lambda_i \phi_i , \text{  } i=0,1,\dots \text{  (31)}$$
</li>
<li>The eigenfunctions are the smoothest functions in the sense of the Dirichlet energy (see insert IN3) and can be interpreted as a generalization of the standard Fourier basis (given, in fact, by the eigenfunctions of the 1D Euclidean Laplacian, $−\frac{d^2}{x^2} e^{iωx} = ω^2 e^{iωx}$) to a non-Euclidean domain.</li>
<li>It is important to emphasize that the Laplacian eigenbasis is intrinsic due to the intrinsic construction of the Laplacian itself.<ul>
<li>라플라시안 고유기저는 라플라시안 자체의 고유한 구조 떄문에 고유하다는 건 중요</li>
</ul>
</li>
<li>A square-integrable function $f$ on $\cal{X}$ can be decomposed into Fourier series as

$$f(x) = \sum_{i \geq 0} (f,\phi_i)_{L^2 (\cal{X})} \phi_i (x)$$
</li>
<li>where the projection on the basis functions producing a discrete set of Fourier coefficients $(\hat{f}_i = (f,\phi_i)_{L^2 (\cal{X})})$ generalizes the analysis (forward transform) stage in classical signal processing, and summing up the basis functions with these coefficients is the synthesis (inverse transform) stage.<ul>
<li>푸리에 계수의 이산집합을 만드는 기저 함수의 projection은 고전적 신호 처리에서 분석 단계를 일반화</li>
<li>이 걔수로 기저 함수를 합한 것이 합성 단계</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
<em>A centerpiece of classical Euclidean signal processing is the property of the Fourier transform diagonalizing the convolution operator, colloquially referred to as the Convolution Theorem.</em><ul>
<li><em>컨벌루션 이론: 고전적인 유클리드 신호 처리의 중점이 컨볼루션 연산자를 대각화하는 푸리에 변환의 성질</em></li>
</ul>
</li>
<li>
<em>This property allows to express the convolution $f </em> g$ of two functions in the spectral domain as the element-wise product of their Fourier transforms,*<ul>
<li>
<em>이 특성 때문에 스펙트럼 도메인에서 두 함수의 컨볼루션을 푸리에 변환의 원소별 곱으로 표현 가능!</em>

$$(\widehat{f * g}) (w) = \int_{-\infty}^{\infty} f(x) e^{-iwx} dx \int^{\infty}_{-\infty} g(x) e^{-iwx} dx \text{  (33)} $$
</li>
</ul>
</li>
<li>Unfortunately, in the non-Euclidean case we cannot even define the operation $x − x'$ on the manifold or graph, so the notion of convolution (7) does not directly extend to this case. </li>
<li>
<em>One possibility to generalize convolution to non-Euclidean domains is by using the Convolution Theorem as a definition,</em><ul>
<li>
<em>컨발루션 이론을 정의로 사용함으로써 비유클리드 영역으로 컨볼루션 일반화 가능!</em>

$$(f * g) (x) = \sum_{i \geq 0} &lt; f, \phi_i &gt;_{L^2 ( \cal{X} )} &lt;g,\phi_i&gt;_{L^2 ( \cal{X} ) }\phi_i(x) \text{  (34)}$$
</li>
</ul>
</li>
<li>One of the key differences of such a construction from the classical convolution is the lack of shift-invariance.<ul>
<li>shift-invariance의 부족..</li>
</ul>
</li>
<li>In terms of signal processing, it can be interpreted as a position-dependent filter.<ul>
<li>신호 처리에서 이건 위치 의존적 필터로 해석 가능</li>
</ul>
</li>
<li>While parametrized by a fixed number of coefficients in the frequency domain, the spatial representation of the filter can vary dramatically at different points (see FIGS4).<ul>
<li>주파수 영역에서 고정된 수의 계수에 의해 파라미터화되는 동안 필터의 공간적 표현은 다른 포인트에서 극적으로 달라질 수 있음</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The discussion above also applies to graphs instead of manifolds, where one only has to replace the inner product in equations (32) and (34) with the discrete one (20).<ul>
<li>그래프에서도 적용됨!</li>
</ul>
</li>
<li>All the sums over $i$ would become finite, as the graph Laplacian $∆$ has $n$ eigenvectors.<ul>
<li>그래프 라플라시안이 n개의 고유벡터 가지기 때문에 모든 합이 유한해짐</li>
</ul>
</li>
<li>In matrix-vector notation, the generalized convolution $f * g$ can be expressed as $Gf = Φ diag(\hat{g})Φ^\top f$, where $\hat{g} = (\hat{g}_1, . . . , \hat{g}_n)$ is the spectral representation of the filter and $Φ = (φ_1 , . . . , φ_n)$ denotes the Laplacian eigenvectors (30). </li>
<li>The lack of shift invariance results in the absence of circulant (Toeplitz) structure in the matrix $G$, which characterizes the Euclidean setting. <ul>
<li>토플리츠 행렬; 행의 값이 행마다 각각 같음</li>
</ul>
</li>
<li>Furthermore, it is easy to see that the convolution operation commutes with the Laplacian, $G∆f = ∆Gf$.<ul>
<li>컨벌루션 연산이 라플라시안과 일치!</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Uniqueness and stability</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Finally, it is important to note that the Laplacian eigenfunctions are not uniquely defined.</li>
<li>To start with, they are defined up to sign, i.e., $∆(±φ) = λ(±φ)$.</li>
<li>Thus, even isometric domains might have different Laplacian eigenfunctions. <ul>
<li>isometric 도메인도 다른 라플라시안 고유함수가지기 가능~</li>
</ul>
</li>
<li>Furthermore, if a Laplacian eigenvalue has multiplicity, then the associated eigenfunctions can be defined as orthonormal basis spanning the corresponding eigensubspace (or said differently, the eigenfunctions are defined up to an orthogonal transformation in the eigen-subspace).<ul>
<li>라플라시안 고유값이 다중치라면 연관된 고유함수들은 대응하는 고유부분공간(?)에 걸친 직교 기저로 정의 가능~</li>
</ul>
</li>
<li>A small perturbation of the domain can lead to very large changes in the Laplacian eigenvectors, especially those associated with high frequencies.</li>
<li>At the same time, the definition of heat kernels (36) and diffusion distances (38) does not suffer from these ambiguities – for example, the sign ambiguity disappears as the eigenfunctions are squared. </li>
<li>Heat kernels also appear to be robust to domain perturbations.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>[IN3] Physical interpretation of Laplacian eigenfunctions:</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Given a function $f$ on the domain $\cal{X}$ , the Dirichlet energy

$$\cal{E}_{Dir}(f) = \int_{\cal{X}} \| ∇f(x) \|^{2}_{T_{x} \cal{X}} dx = \int_{\cal{X}} f(x)∆f(x)dx \text{ , (27)}$$
</li>
<li>measures how smooth it is (the last identity in (27) stems from (19)). </li>
<li>We are looking for an orthonormal basis on $\cal{X}$, containing k smoothest possible functions (FIGS3), by solving</li>
<li>the optimization problem 

$$min {φ_0}\cal{E}_{Dir}(φ_0) \text{   s.t.   } \| φ_0 \| = 1 \text{   (28)}$$


$$min_{φ_i} \cal{E}_{Dir}(φ_i) \text{   s.t.   } \| φ_i \| = 1, i = 1, 2, . . . k − 1 \\ φ_i ⊥ span\{ φ_0, . . . , φ_{i−1} \}$$
</li>
<li>In the discrete setting, when the domain is sampled at n points, problem (28) can be rewritten as 

$$min_{Φ_k ∈ \mathbb{R}^{n×k}} trace(Φ_{k}^\top ∆Φ_k) \text{  s.t. } Φ_{k}^\top Φ_k = I \text{ , (29)}$$
</li>
<li>where $Φ_k = (φ_0, . . . φ_{k−1})$.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The solution of (29) is given by the first k eigenvectors of $∆$ satisfying

$$∆Φ_k = Φ_k Λ_k \text{ ,  (30)}$$
</li>
<li>where $Λ_k = diag(λ_0, . . . , λ_{k−1})$ is the diagonal matrix of corresponding eigenvalues.</li>
<li>The eigenvalues $0 = λ_0 ≤ λ_1 ≤ . . . λ_{k−1}$ are non-negative due to the positive-semidefiniteness of the Laplacian and can be interpreted as ‘frequencies’, where $φ_0 =$ const with the corresponding eigenvalue $λ_0 = 0$ play the role of the DC.</li>
<li>The Laplacian eigendecomposition can be carried out in two ways.</li>
<li>First, equation (30) can be rewritten as a generalized eigenproblem $(D − W)Φ_k = AΦ_kΛ_k$, resulting in A-orthogonal eigenvectors, $Φ_k^\top AΦ_k = I$. </li>
<li>Alternatively, introducing a change of variables $Ψ_k = A^{1/2}Φ_k$, we can obtain a standard eigendecomposition problem $A^{−1/2}(D − W)A^{−1/2}Ψ_k = Ψ_kΛ_k$ with orthogonal eigenvectors $Ψ_k^\top Ψ_k = I$. </li>
<li>When $A = D$ is used, the matrix $∆ = A^{−1/2}(D − W)A^{−1/2}$ is referred to as the normalized symmetric Laplacian.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="V.-SPECTRAL-METHODS">
<a class="anchor" href="#V.-SPECTRAL-METHODS" aria-hidden="true"><span class="octicon octicon-link"></span></a>V. SPECTRAL METHODS<a class="anchor-link" href="#V.-SPECTRAL-METHODS"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>[IN4] Heat diffusion on non-Euclidean domains</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>An important application of spectral analysis, and historically, the main motivation for its development by Joseph Fourier, is the solution of partial differential equations (PDEs).</li>
<li>In particular, we are interested in heat propagation on non-Euclidean domains.</li>
<li>This process is governed by the heat diffusion equation, which in the simplest setting of homogeneous and isotropic diffusion has the form 

$$\begin{cases} f_t(x, t) = −c∆f(x, t) \\ f(x, 0) = f_0(x) (Initial condition) \end{cases} \text{ (35)}$$
</li>
<li>with additional boundary conditions if the domain has a boundary.</li>
<li>$f(x, t)$ represents the temperature at point $x$ at time $t$. </li>
<li>Equation (35) encodes the Newton’s law of cooling, according to which the rate of temperature change of a body (lhs) is proportional to the difference between its own temperature and that of the surrounding (rhs). </li>
<li>The proportion coefficient c is referred to as the thermal diffusivity constant; here, we assume it to be equal to one for the sake of simplicity.<ul>
<li>비율 게수 c는 열 전파 상수, 단순성 위해 1이라 가정</li>
</ul>
</li>
<li>The solution of (35) is given by applying the heat operator $H^t = e^{−t∆}$ to the initial condition and can be expressed in the spectral domain as

$$f(x, t) = e^{−t∆}f_0(x) = \sum_{i≥0}&lt;f_0, φ_i&gt;_{L^2(\cal{X} )}e^{−tλ_i}φ_i(x) \text{ (36)}$$


$$ = \int_{\cal{X}}f_0(x') \sum_{i≥0} e^{−tλ_i}φ_i(x)φ_i(x') dx'$$


$$h_t(x,x`) := \sum_{i≥0} e^{−tλ_i}φ_i(x)φ_i(x')$$
</li>
<li>$h_t(x, x`)$ is known as the heat kernel and represents the solution of the heat equation with an initial condition $f_0(x) = δ_{x`}(x)$, or, in signal processing terms, an ‘impulse response’. </li>
<li>In physical terms, $h_t(x, x` )$ describes how much heat flows from a point $x$ to point $x`$ in time $t$.</li>
<li>In the Euclidean case, the heat kernel is shift-invariant, $h_t(x, x`) = h_t(x − x`)$, allowing to interpret the integral in (36) as a convolution $f(x, t) = (f_0  * h_t)(x)$. </li>
<li>In the spectral domain, convolution with the heat kernel amounts to low-pass filtering with frequency response $e^{−tλ}$.</li>
<li>Larger values of diffusion time t result in lower effective cutoff frequency and thus smoother solutions in space (corresponding to the intuition that longer diffusion smoothes more the initial heat distribution</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The ‘cross-talk’ between two heat kernels positioned at points $x$ and $x`$ allows to measure an intrinsic distance 

$$d^{2}_{t}(x, x`) = \int_{\cal{X}}(h_t(x, y) − h_t(x`, y))^2 dy \text{  (37)}$$


$$ = \sum_{i≥0} e^{-2tλ_i}(φ_i(x) − φ_i(x`))^2 \text{ (38)}$$
</li>
<li>referred to as the diffusion distance [30].</li>
<li>Note that interpreting (37) and (38) as spatial- and frequency-domain norms $\| · \|_{L^2(\cal{X} )}$ and $\| · \|_{l^2}$, respectively, their equivalence is the consequence of the Parseval identity.</li>
<li>Unlike geodesic distance that measures the length of the shortest path on the manifold or graph, the diffusion distance has an effect of averaging over different paths. <ul>
<li>확산 거리는 다른 경로에서 평균을 내는 효과 가짐~</li>
</ul>
</li>
<li>It is thus more robust to perturbations of the domain, for example, introduction or removal of edges in a graph, or ‘cuts’ on a manifold.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQaQu0rpJyydFAt6MxDDGUp7hxWsFhFPcXNLw&amp;usqp=CAU" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>열 커널이 다른 위치로 옮겨지면 그 모양이 얼마나 변하는지를 보여주고, 변환 불변성shift-invariance의 부족을 의미함.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Spectral CNN (SCNN)</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Similarly to the convolutional layer (6) of a classical Euclidean CNN, Bruna et al. [52] define a spectral convolutional layer as

$$g_l = ξ \big( \sum^{q}_{l`=1}Φ_kΓ_{l,l`}Φ_{k}^\top f_{l`} \big) \text{, (39)}$$
</li>
<li>where the $n × p$ and $n × q$ matrices $F = (f_1, . . . ,f_p)$ and $G = (g_1, . . . , g_q)$ represent the $p-$ and $q-$dimensional input and output signals on the vertices of the graph, respectively (we use $n = |V|$ to denote the number of vertices in the graph), $Γ_{l,l`}$ is a $k \times k$ diagonal matrix of spectral multipliers representing a filter in the frequency domain, and $ξ$ is a nonlinearity applied on the vertex-wise function values. Using only the first $k$ eigenvectors in (39) sets a cutoff frequency which depends on the intrinsic regularity of the graph and also the sample size.</li>
<li>Typically, $k &lt;&lt; n$, since only the first Laplacian eigenvectors describing the smooth structure of the graph are useful in practice.</li>
</ul>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/chch/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/2022/03/13/GDL.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/chch/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/chch/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/chch/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/seoyeonc" target="_blank" title="seoyeonc"><svg class="svg-icon grey"><use xlink:href="/chch/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
