{
  
    
        "post0": {
            "title": "빅데이터 분석 2주차_2",
            "content": "&#54028;&#51060;&#53664;&#52824;&#47484; &#51060;&#50857;&#54616;&#50668; &#54924;&#44480;&#47784;&#54805; &#54617;&#49845;&#54616;&#44592; . - (1/5) 회귀모형 소개, 손실 함수 . - (2/5) 경사하강법, 경사하강법을 이용하여 회귀계수 1회 업데이트 . - (3/5) 회귀계수 반복 업데이트 . - (4/5) 학습률 . - (5/5) 사과영상 . import torch import numpy as np import matplotlib.pyplot as plt . 로드맵 . 회귀분석 $ to$ 로지스틱 $ to$ 심층신경망(DNN) $ to$ 합성곱신경망(CNN) | . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . w라는 로테이션을 많이 사용하는 딥러닝 | . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(202150754) n=100 ones=torch.ones(n) x,_ = torch.randn(n).sort() # 배열하면 tendor,index가 반환됨. 필요한 x만 반환 X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ #@는 벡터를 곱하라는 뜻..! ytrue = X@W # 우리가 알고 싶은 것은 평균 직선 . plt.plot(x,y,&#39;o&#39;) #우리가 관측한 값 plt.plot(x,ytrue,&#39;--&#39;) # 우리가 추론하고 싶은 값 . [&lt;matplotlib.lines.Line2D at 0x7f1443b129a0&gt;] . &#54617;&#49845; . - 파란점만 주어졌을때, 주황색 점선을 추론하는것. . - 좀 더 정확하게 말하면 given data로 $ begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$를 최대한 $ begin{bmatrix} 2.5 4 end{bmatrix}$와 비슷하게 찾는것. (2.5와 4는 true의 값) . given data : $ big {(x_i,y_i) big }_{i=1}^{n}$ . | parameter: ${ bf W}= begin{bmatrix} w_0 w_1 end{bmatrix}$ . | estimated parameter: ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$ . | . plt.plot(x,y,&#39;o&#39;) # 그림을 보고 &#39;적당한&#39; 추세를 찾는 과정 . [&lt;matplotlib.lines.Line2D at 0x7f1443a1b9d0&gt;] . - 시도: $( hat{w}_0, hat{w}_1)=(-5,10)$을 선택하여 선을 그려보고 적당한지 판단. . $ hat{y}_i=-5 +10 x_i$ 와 같이 $y_i$의 값을 적합시키겠다는 의미 | . plt.plot(x,y,&#39;o&#39;) plt.plot(x,-5+10*x,&#39;--&#39;) # 그림을 보고 판단해보는 단계 . [&lt;matplotlib.lines.Line2D at 0x7f1443994220&gt;] . - 벡터 표현으로주황색 추세선을 계산 . What = torch.tensor([-5.0,10.0]) # float으로 선언해야 함!!! plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@What,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f14438fa310&gt;] . &#54028;&#46972;&#48120;&#53552;&#47484; &#54617;&#49845;&#54616;&#45716; &#48169;&#48277;, &#51593; &#51201;&#45817;&#54620; &#49440;&#50640; &#47582;&#52656;&#44032;&#45716; &#44284;&#51221; . - 이론적으로 추론 (회귀 분석) . - 컴퓨터의 반복계산을 이용하여 추론(경사하강법) . (1) initial value: 임의의 선을 그어봄 . What = torch.tensor([-5.0,10.0],requires_grad=True) What . tensor([-5., 10.], requires_grad=True) . 처음에는 ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}= begin{bmatrix} -5 10 end{bmatrix} $ 를 대입해서 주황색 점선을 적당히 그려보자는 의미 . | 끝에 requires_grad=True는 나중에 미분에 사용되기 위함. . | . yhat=X@What # yhat을 구하면 단지 X(미분X)*What(미분O) = 미분 옵션이 있는 텐션이 되어버린다. yhat.data # 미분 옵션이 사라짐. . tensor([-27.9716, -26.0391, -25.8951, -24.1830, -23.6405, -23.1161, -22.0441, -21.9913, -21.4959, -21.2860, -20.4771, -19.6991, -19.1434, -18.0758, -17.5390, -17.4888, -16.8212, -16.6630, -16.2503, -14.3326, -13.8527, -13.6397, -13.5228, -13.2096, -12.8514, -12.8461, -12.7527, -12.2431, -12.0267, -11.7990, -11.6495, -11.5587, -11.5497, -11.1709, -10.9643, -10.7969, -10.7696, -10.7324, -10.6567, -10.4404, -10.1049, -9.9527, -9.7916, -9.3899, -9.2762, -8.2773, -8.0850, -7.9550, -7.8498, -7.7767, -7.6419, -7.2295, -7.1686, -6.9773, -6.9454, -6.6435, -5.6597, -5.5200, -5.4562, -5.3640, -4.9588, -4.9111, -4.5447, -3.9894, -3.6367, -3.0762, -2.4928, -2.4512, -2.1695, -2.0062, -1.7060, 0.1909, 0.5915, 0.9467, 1.3453, 1.4359, 2.0752, 2.4723, 2.5368, 2.7189, 2.7902, 2.8337, 3.2249, 3.7238, 3.8636, 3.9170, 3.9852, 5.0601, 5.7496, 6.0569, 7.0621, 7.2674, 7.6805, 7.9669, 8.4266, 9.6044, 9.6791, 10.7418, 12.6324, 18.9507]) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f143d0d54f0&gt;] . (2) 첫 번째 수정: 추세선의 적당한 정도를 판단하여 적당한 선으로 업데이트 . - &#39;적당한 정도&#39;를 판단하기 위하 장치로서 loss function 도입 . $loss= sum_{i=1}^{n}(y_i- hat{y}_i)^2= sum_{i=1}^{n}(y_i-( hat{w}_0+ hat{w}_1x_i))^2$ . $=({ bf y}-{ bf hat{y}})^ top({ bf y}-{ bf hat{y}})=({ bf y}-{ bf X}{ bf hat{W}})^ top({ bf y}-{ bf X}{ bf hat{W}})$ . 구현하기 편하게 하기 위해 벡터로 구현 . - loss 함수의 특징 . $y_i approx hat{y}_i$ 일수록 loss값이 작다. | $y_i approx hat{y}_i$ 이 되도록 $( hat{w}_0, hat{w}_1)$을 잘 찍으면 loss값이 작다. | (중요) 주황색 점선이 적당할수록 loss값이 작다. | . loss = torch.sum((y-yhat)**2) # = (y-yhat)@(y-yhat) loss . tensor(11003.1260, grad_fn=&lt;SumBackward0&gt;) . - $loss(=11003.1260)$을 줄이는, 혹은 없애는 것이 목표 $ to$ 아예 모든 조합 $( hat{w}_0, hat{w}_1)$에 대하여 가장 작은 $loss$ 찾기 . - 문제의 치환 . 적당해 보이는 주황색 선을 찾자 $ to$ $loss(w_0,w_1)$을 최소로 하는 $(w_0,w_1$의 값을 찾기 | . - $loss(w_0,w_1)$를 최소로 하는 $(w_0,w_1)$ 구하는 것으로 수정된 목표 . 단순한 수학문제가 되었다. 마치 $loss(w)=w^2-2w+3$ 을 최소화하는 $w$를 찾으라는 것과 같음. | . - 경사하강법, 벡터미분 사용! . &#44221;&#49324;&#54616;&#44053;&#48277; . 경사하강법 아이디어(1차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접선) &lt;-- 미분 . (step 3) 순간기울기(= 미분계수)의 부호를 살펴보고 부호와 반대방향으로 움직인다. (순간기울기와 같은 방향으로 움직이면 점점 커질테니까.) . (Tip) 기울기의 절대값 크기와 비례하여 보폭(= 움직이는 정도)을 조절한다. . 경사하강법 아이디어(2차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접평면) &lt;-- 편미분 . (step 3) 순간기울기(= 여러개의 미분계수)의 부호를 살펴보고 부호와 반대방향으로 각각 움직인다. (순간기울기와 같은 방향으로 움직이면 점점 커질테니까.) . (Tip) 기울기의 절대값 크기와 비례하여 보폭(= 움직이는 정도)을 각각 조절한다. . loss를 줄이도록 $W$를 개선하는 방법 . - $수정값 leftarrow 원래값 - 기울어진 크기(= 미분계수) times alpha$ . 여기에서 $ alpha$는 전체적인 보폭의 크기를 결정한다. 즉, $ alpha$값이 클수록 한 번의 update에 움직이는 양이 크다. | . - ${ bf W} leftarrow { bf W} - alpha times frac{ partial}{ partial { bf W}}loss(w_0,w_1)$ . 마이너스의 의미 기울기의 부호를 보고 반대방향으로 움직여라 . | $ frac{ partial}{ partial { bf W}}loss(w_0,w_1):$ 기울기의 절대값 크기와 비례하여 움직이는 정도를 조정하라. (속도의 조절) . | $ alpha$의 의미: 전체적인 보폭의 속도를 조절, $ alpha$가 크면 전체적으로 빠르게 움직인다. 다리의 길이로 비유할 수 있다. . | . . - 목표: $loss(=11003.1260)$ 값을 줄이는 것. . - 방법: 경사하강법 . - 경사하강법으로 loss를 줄이기 위해서는 $ frac{ partial}{ partial { bf W}}loss(w_0,w_1)$의 계산이 필요한데, 이를 위해서 벡터미분이 필요하다. . requires_grad=True를 가진 텐서로 미분. | . loss=torch.sum((y-yhat)**2)= torch.sum((y-X@What)**2) # 이었고 What=torch.tensor([-5.0,10.0],requires_grad=True) # 이므로 결국 What으로 미분하라는 의미. # 미분한 식이 나오는 것이 아니고, # 그 식에 (-5.0, 10.0)을 대입한 계수값이 계산됨. . loss.backward() # requires_grad=True를 가진 텐서로 미분하라는 의미 # 단지 미분 계수가 계산되어 있음 # 정확하게 말하면 미분을 활용하여 $(-5,10)$에서의 순간기울기를 구했다는 의미임. . What.grad.data . tensor([-1730.4250, 1485.8135]) . 이것이 의미하는 건 $(-5,10)$에서의 순간기울기가 $([-1730.4250, 1485.8135])$ 이라는 의미 (각각 (양수, 음수)로 움직여야 함) | . - 직접 계산하여 검증 . $loss(w_0,w_1)=(y- hat{y})^ top (y- hat{y})=(y-XW)^ top (y-XW)$ . | $ frac{ partial}{ partial W}loss(w_0,w_1)=-2X^ top y+2X^ top X W$ . | . -2 * X.T @ y + 2 * X.T @ X @ What # = What.grad.data . tensor([-1730.4250, 1485.8135], grad_fn=&lt;AddBackward0&gt;) . alpha=0.001 print(&#39;수정 전: &#39; + str(What.data)) # 미분 옵션 없애기! print(&#39;수정하는 폭: &#39; + str(-alpha*What.grad.data)) print(&#39;수정 후: &#39; + str(What.data-alpha*What.grad.data)) print(&#39;*참값: (2.5,4)&#39;) . 수정 전: tensor([-5., 10.]) 수정하는 폭: tensor([ 1.7304, -1.4858]) 수정 후: tensor([-3.2696, 8.5142]) *참값: (2.5,4) . Wbefore = What.data Wafter = What.data-alpha*What.grad.data Wbefore, Wafter . (tensor([-5., 10.]), tensor([-3.2696, 8.5142])) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@Wbefore,&#39;--b&#39;) # 수정 전 파란 점선 plt.plot(x,X@Wafter,&#39;--r&#39;) # 수정 후 빨간 점선 plt.title(&quot;before: blue // after: red&quot;) . Text(0.5, 1.0, &#39;before: blue // after: red&#39;) . (3) Learn (=estimate $ bf hat{W})$: . What= torch.tensor([-5.0,10.0],requires_grad=True) . alpha=0.001 for epoc in range(30): What.grad=None yhat=X@What loss=torch.sum((y-yhat)**2) loss.backward() # What으로 미분하는 과정 What.data = What.data-alpha * What.grad.data # 적정한 선으로 Update! . What.data # true 값은 2.5,4 . tensor([2.5248, 3.9898]) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What.data),&#39;--&#39;) # 순수 데이터만 뽑기 위해 .data꼭 붙이기 plt.plot(x,(X@np.array([2.5,4])),&#39;-&#39;) # 거의 비슷해서 한 선으로 보임 . [&lt;matplotlib.lines.Line2D at 0x7f1435f96310&gt;] . &#54028;&#46972;&#47700;&#53552;&#51032; &#49688;&#51221; &#44284;&#51221;&#51012; &#44288;&#52272;&#54624; &#49688; &#50630;&#45208;.(&#54617;&#49845;&#44284;&#51221; &#47784;&#45768;&#53552;&#47553;) . - 기록 해보기 . losses=[] # 기록하고 싶은 것 1 yhats = [] # 기록하고 싶은 것 2 Whats = [] # 기록하고 싶은 것 3 . What= torch.tensor([-5.0,10.0],requires_grad=True) alpha=0.001 for epoc in range(30): Whats=Whats+[What.data.tolist()] # What을 list화 해서 저장 What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses=losses+[loss.item()] loss.backward() # What으로 미분하는 과정 What.data = What.data-alpha * What.grad.data # 적정한 선으로 Update! . - $ hat{y}$ 관찰 . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[5],&#39;--&#39;) # 5번 업데이트된 추세선 . [&lt;matplotlib.lines.Line2D at 0x7f1435f20490&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[10],&#39;--&#39;) # 10번 업데이트된 추세선 . [&lt;matplotlib.lines.Line2D at 0x7f1435f08100&gt;] . - $ hat{ bf{W}}$ . losses . [11003.1259765625, 6417.849609375, 3748.93798828125, 2195.045166015625, 1290.041015625, 762.7489624023438, 455.38189697265625, 276.1114196777344, 171.48153686523438, 110.3656005859375, 74.63228607177734, 53.71556854248047, 41.45503234863281, 34.25670623779297, 30.02236557006836, 27.52593421936035, 26.050209045410156, 25.175155639648438, 24.654428482055664, 24.34327507019043, 24.156478881835938, 24.043743133544922, 23.975299835205078, 23.93347930908203, 23.907733917236328, 23.891769409179688, 23.881786346435547, 23.8754940032959, 23.871488571166992, 23.868919372558594] . plt.plot(losses) . [&lt;matplotlib.lines.Line2D at 0x7f1435e55fd0&gt;] . Animation . plt.rcParams[&#39;figure.figsize&#39;] = (10,4) # 크기 plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; # 애니메이션 나오게 하는 옵션 . from matplotlib import animation fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect $ alpha$&#50640; &#45824;&#54616;&#50668; ($ alpha$&#45716; &#54617;&#49845;&#47456;) . (1) $ alpha$가 너무 작다면?$ to$ 비효율적이다. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0001 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (1) $ alpha$가 너무 크다면? $ to$ 다른의미에서 비효율적이다 + 위험하다.. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0083 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (3) $ alpha=0.0085$ 아예 모형을 벗어나버린다. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.0085 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (4) $ alpha=0.01$ . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.01 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect (5) $ alpha=0.006$ 숙제 . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . alpha=0.006 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect",
            "url": "https://seoyeonc.github.io/chch/2021/11/13/bd_2%EC%A3%BC%EC%B0%A8_2.html",
            "relUrl": "/2021/11/13/bd_2%EC%A3%BC%EC%B0%A8_2.html",
            "date": " • Nov 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "빅데이터 분석 2주차_1",
            "content": "Path, &#51060;&#48120;&#51648; &#53356;&#47204;&#47553;&#44284; CNN&#47784;&#45944; . - (1/4) Path 설명 . - (2/4) 이미지 크롤링 . - (3/4) 모형학습 및 결과분석 . - (4/4) 테스트 . from fastai.data.all import * from fastai.vision.all import * . path=Path() # 현재 위치 저장 현재폴더=. 상위폴더=.. path . Path(&#39;.&#39;) . path.ls() . (#9) [Path(&#39;bd_1주차.ipynb&#39;),Path(&#39;601f57a1260000bd0c275eca.jpeg&#39;),Path(&#39;2021_09_07_(1주차)_9월7일.ipynb&#39;),Path(&#39;p1065602463397191_754_thum.jpg&#39;),Path(&#39;502104_3008_164.png&#39;),Path(&#39;2021_09_09_(2주차)_9월9일.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2021-09-27-(3주차)_9월27일(1).ipynb&#39;),Path(&#39;bd_2주차.ipynb&#39;)] . (path/&#39;폴더 이름 넣어~&#39;).ls() . path=Path() . (path/&#39;asdf&#39;).mkdir() . (path/&#39;asdf&#39;).ls() . (#0) [] . (path/&#39;asdf&#39;).mkdir() # 이미 있는 폴더면 오류 발생 . FileExistsError Traceback (most recent call last) &lt;ipython-input-19-96a686fc4db7&gt; in &lt;module&gt; -&gt; 1 (path/&#39;asdf&#39;).mkdir() # 이미 있는 폴더면 오류 발생 ~/anaconda3/envs/csy/lib/python3.8/pathlib.py in mkdir(self, mode, parents, exist_ok) 1286 self._raise_closed() 1287 try: -&gt; 1288 self._accessor.mkdir(self, mode) 1289 except FileNotFoundError: 1290 if not parents or self.parent == self: FileExistsError: [Errno 17] File exists: &#39;asdf&#39; . (path/&#39;asdf&#39;).mkdir(exist_ok=True) # 이미 존재하면 무시~ . (path/&#39;asdf&#39;).rmdir() # 생성한 폴더 삭제 . &#51060;&#48120;&#51648; &#53356;&#47204;&#47553; . - 이미지 크롤링 . 검색 2. 이미지 주소를 찾음 3. 해당 주소로 이동하여 저장하는 과정 반복 | | . - 다른방법: 덕덕고를 이용한 이미지 크롤링 . ref: https://github.com/fastai/fastbook/blob/master/utils.py | . def search_images_ddg(key,max_n=200): &quot;&quot;&quot;Search for &#39;key&#39; with DuckDuckGo and return a unique urls of &#39;max_n&#39; images (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api) &quot;&quot;&quot; url = &#39;https://duckduckgo.com/&#39; params = {&#39;q&#39;:key} res = requests.post(url,data=params) searchObj = re.search(r&#39;vqd=([ d-]+) &amp;&#39;,res.text) if not searchObj: print(&#39;Token Parsing Failed !&#39;); return requestUrl = url + &#39;i.js&#39; headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0&#39;} params = ((&#39;l&#39;,&#39;us-en&#39;),(&#39;o&#39;,&#39;json&#39;),(&#39;q&#39;,key),(&#39;vqd&#39;,searchObj.group(1)),(&#39;f&#39;,&#39;,,,&#39;),(&#39;p&#39;,&#39;1&#39;),(&#39;v7exp&#39;,&#39;a&#39;)) urls = [] while True: try: res = requests.get(requestUrl,headers=headers,params=params) data = json.loads(res.text) for obj in data[&#39;results&#39;]: urls.append(obj[&#39;image&#39;]) max_n = max_n - 1 if max_n &lt; 1: return L(set(urls)) # dedupe if &#39;next&#39; not in data: return L(set(urls)) requestUrl = url + data[&#39;next&#39;] except: pass . search_images_ddg(검색어)를 이용하여 검색어에 해당하는 url 얻기m . search_images_ddg(&#39;Holybang&#39;,max_n=5) . (#5) [&#39;https://i.ytimg.com/vi/fcmDi1DCGDs/maxresdefault.jpg&#39;,&#39;http://one-we.cn/uploads/allimg/191218/1-19121Q35R5G9.jpg&#39;,&#39;https://i.ytimg.com/vi/SBWy4-ZU4qQ/maxresdefault.jpg&#39;,&#39;https://www.kpophighindia.com/wp-content/uploads/2021/07/crews-1.jpg&#39;,&#39;https://t1.daumcdn.net/cfile/tistory/993004335A12CB082C&#39;] . path=Path() . path.ls() . (#7) [Path(&#39;bd_1주차.ipynb&#39;),Path(&#39;2021_09_07_(1주차)_9월7일.ipynb&#39;),Path(&#39;bd_1st&#39;),Path(&#39;2021_09_09_(2주차)_9월9일.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2021-09-27-(3주차)_9월27일(1).ipynb&#39;),Path(&#39;bd_2주차.ipynb&#39;)] . download_images(path,urls=search_images_ddg(&#39;Holybang&#39;,max_n=5)) . 현재 working directory에 5개의 이미지가 저장된 모습! | . keywords=&#39;sunmi&#39;, &#39;Hyuna&#39; # 단어 한 개 쓰면 키워드로 입력되어서 알파벳 수대로 폴더 만들어짐.. path=Path(&#39;Singer&#39;) . if not path.exists(): # 현재폴더에 Singer 폴더가 있는지 체크 path.mkdir() # 현재폴더에 Singer 폴더가 만들어짐 for keyword in keywords: # keyword=&#39;sunmi&#39;, keyword=&#39;Hyuna&#39; 일때 아래내용을 반복 lastpath=path/keyword # ./Singer/sunmi or ./Singer/Hyuna lastpath.mkdir(exist_ok=True) # make ./Singer/sunmi or ./Singer/Hyuna urls=search_images_ddg(keyword) # &#39;sunmi&#39; 검색어로 url들의 리스트를 얻음 download_images(lastpath,urls=urls) # 그 url에 해당하는 이미지들을 ./Singer/sunmi or ./Singer/Hyuna 에 저장 . Cleaning Data . 탐색기로 파일들을 살펴보니 조금 이상한 확장자도 있음. . | 조금 이상해보이는 확장자도 열리기는 함. . | . PILImage.create(&#39;./singer/iu/00000015.jpg:large&#39;) . FileNotFoundError Traceback (most recent call last) &lt;ipython-input-44-2622c1a578e7&gt; in &lt;module&gt; -&gt; 1 PILImage.create(&#39;./singer/iu/00000015.jpg:large&#39;) ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/core.py in create(cls, fn, **kwargs) 108 if isinstance(fn,ndarray): return cls(Image.fromarray(fn)) 109 if isinstance(fn,bytes): fn = io.BytesIO(fn) --&gt; 110 return cls(load_image(fn, **merge(cls._open_args, kwargs))) 111 112 def show(self, ctx=None, **kwargs): ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/core.py in load_image(fn, mode) 83 def load_image(fn, mode=None): 84 &#34;Open and load a `PIL.Image` and convert to `mode`&#34; &gt; 85 im = Image.open(fn) 86 im.load() 87 im = im._new(im.im) ~/anaconda3/envs/csy/lib/python3.8/site-packages/PIL/Image.py in open(fp, mode, formats) 2973 2974 if filename: -&gt; 2975 fp = builtins.open(filename, &#34;rb&#34;) 2976 exclusive_fp = True 2977 FileNotFoundError: [Errno 2] No such file or directory: &#39;./singer/iu/00000015.jpg:large&#39; . verify_images(get_image_files(path)) . (#4) [Path(&#39;Singer/sunmi/00000065.jpg&#39;),Path(&#39;Singer/Hyuna/00000034.jpg&#39;),Path(&#39;Singer/Hyuna/00000039.jpg&#39;),Path(&#39;Singer/Hyuna/00000025.jpeg&#39;)] . 위에 해당하는 이미지를 수동으로 지워줌 | 나중에 지우는 함수 배움(조금 까다로움) | . - fastai 가 지원하는 함수로 분석하기 좋게 dls 만들기 . dls=ImageDataLoaders.from_folder( path, train=&#39;singer&#39;, valid_pct=0.2, item_tfms=Resize(224)) . ImageDataLoaders.from_folder(path, train=&#39;train&#39;, valid=&#39;valid&#39;, valid_pct=None, seed=None, vocab=None, item_tfms=None, batch_tfms=None, bs=64, val_bs=None, shuffle=True, device=None) . dls.show_batch(max_n=16) . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(7) . epoch train_loss valid_loss error_rate time . 0 | 1.169335 | 1.660869 | 0.470588 | 00:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.718052 | 0.950780 | 0.397059 | 00:04 | . 1 | 0.634088 | 0.537583 | 0.205882 | 00:04 | . 2 | 0.490461 | 0.444096 | 0.205882 | 00:04 | . 3 | 0.389381 | 0.479742 | 0.161765 | 00:04 | . 4 | 0.317441 | 0.513656 | 0.176471 | 00:04 | . 5 | 0.269008 | 0.556920 | 0.191176 | 00:04 | . 6 | 0.234722 | 0.560562 | 0.205882 | 00:04 | . learn.show_results(max_n=16) . &#50724;&#45813;&#48516;&#49437; . interp=Interpretation.from_learner(learn) interp.plot_top_losses(16) . 수동으로 특정 observation에 대한 예측결과를 확인 | . dls.train_ds . (#275) [(PILImage mode=RGB size=1080x1080, TensorCategory(1)),(PILImage mode=RGB size=1038x1557, TensorCategory(1)),(PILImage mode=RGB size=642x858, TensorCategory(1)),(PILImage mode=RGB size=509x509, TensorCategory(0)),(PILImage mode=RGB size=960x1200, TensorCategory(1)),(PILImage mode=RGB size=800x1200, TensorCategory(0)),(PILImage mode=RGB size=3600x2025, TensorCategory(0)),(PILImage mode=RGB size=1100x1716, TensorCategory(1)),(PILImage mode=RGB size=1280x1920, TensorCategory(1)),(PILImage mode=RGB size=768x1024, TensorCategory(1))...] . training test | . dls.train_ds[0] # 첫 번째 observation, 즉, (x1,y1) . (PILImage mode=RGB size=1080x1080, TensorCategory(1)) . $x_1=$PILImage mode=RGB size=960x960 | $y_1=$TensorCategory(1) | . dls.train_ds[100][0] . $x_{100}$=위의 이미지 | . dls.train_ds[100][1] . TensorCategory(1) . $y_{100}=$TensorCategory(1) | . x100=dls.train_ds[100][0] . learn.predict(x100) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0015, 0.9985])) . Test . path=Path() . if not (path/&#39;test&#39;).exists(): (path/&#39;test&#39;).mkdir() . urls=search_images_ddg(&#39;sunmi 선미&#39;,max_n=20) download_images(path/&#39;test&#39;,urls=urls) testset=get_image_files(path/&#39;test&#39;) testset . (#20) [Path(&#39;test/00000010.jpg&#39;),Path(&#39;test/00000005.jpg&#39;),Path(&#39;test/00000013.jpg&#39;),Path(&#39;test/00000011.jpg&#39;),Path(&#39;test/00000003.jpg&#39;),Path(&#39;test/00000000.jpg&#39;),Path(&#39;test/00000004.jpg&#39;),Path(&#39;test/00000016.jpg&#39;),Path(&#39;test/00000012.jpg&#39;),Path(&#39;test/00000006.jpg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.5452, 0.4548])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.5311, 0.4689])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9941, 0.0059])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0239, 0.9761])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.2874, 0.7126])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.2435, 0.7565])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([4.7129e-04, 9.9953e-01])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0046, 0.9954])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.2166, 0.7834])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0633, 0.9367])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9806, 0.0194])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([9.9916e-01, 8.4005e-04])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9590, 0.0410])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0581, 0.9419])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([5.8807e-04, 9.9941e-01])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.1369, 0.8631])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.1169, 0.8831])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0784, 0.9216])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([7.0567e-07, 1.0000e+00])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.7406, 0.2594])) . 결과를 보니까 sunmi이 많음 → 어느정도 맞추는것 같긴하다 | . PILImage.create(testset[1]) . 실제로 선미인데 현아로 예측한 사진 | . path=Path() . if not (path/&#39;test2&#39;).exists(): (path/&#39;test2&#39;).mkdir() . urls=search_images_ddg(&#39;hyuna 현아&#39;,max_n=20) download_images(path/&#39;test2&#39;,urls=urls) testset=get_image_files(path/&#39;test2&#39;) testset . (#19) [Path(&#39;test2/00000010.jpg&#39;),Path(&#39;test2/00000000.jpeg&#39;),Path(&#39;test2/00000005.jpg&#39;),Path(&#39;test2/00000013.jpg&#39;),Path(&#39;test2/00000011.jpg&#39;),Path(&#39;test2/00000003.jpg&#39;),Path(&#39;test2/00000018.jpeg&#39;),Path(&#39;test2/00000004.jpg&#39;),Path(&#39;test2/00000016.jpg&#39;),Path(&#39;test2/00000009.jpeg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([1.0000e+00, 3.2445e-06])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.4499, 0.5501])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9945, 0.0055])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([9.9906e-01, 9.4329e-04])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9759, 0.0241])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.5947, 0.4053])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9765, 0.0235])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9819, 0.0181])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.3257, 0.6743])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9685, 0.0315])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([9.9974e-01, 2.6368e-04])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9840, 0.0160])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([9.9996e-01, 4.0536e-05])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.2522, 0.7478])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([9.9949e-01, 5.0994e-04])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9084, 0.0916])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.8650, 0.1350])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9987, 0.0013])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.7162, 0.2838])) . 결과를 보니 Hyuna 역시 잘 맞추는 듯 보인다. | . - 정확률이 아쉽긴 하지만 어느정도 유의미한 결과를 얻었다. . PILImage.create(testset[1]) # 현아인데 선미로 예측한 사진 .",
            "url": "https://seoyeonc.github.io/chch/2021/11/13/bd_2%EC%A3%BC%EC%B0%A8_1.html",
            "relUrl": "/2021/11/13/bd_2%EC%A3%BC%EC%B0%A8_1.html",
            "date": " • Nov 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "빅데이터분석",
            "content": "1&#51452;&#52264; 9&#50900; 7&#51068; . https://guebin.github.io/2021BDA/2021/09/07/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%947%EC%9D%BC.html . from fastai.vision.all import * . fastai 깔기, error 뜬다면 런타임에서 GPU 변경 | !pip install --ungrade fastai | . path=untar_data(URLs.PETS)/&#39;images&#39; # To download model **pretrained** weights: . files=get_image_files(path) # 이미지파일들의 이름을 모두 복붙하여 리스트를 만든뒤에 files.txt로 저장하는 과정으로 비유할 수 있음 . files[2] # txt파일의 3번째 목록 . Path(&#39;/home/cgb4/.fastai/data/oxford-iiit-pet/images/leonberger_5.jpg&#39;) . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . 만일 f[0]파일 제목의 첫 글자가 대문자면 고양이, 소문자면 강아지로 반환 | 원래 있던 모델 | . label_func(&#39;asdf&#39;) # 소문자로 시작하니까 강아지로 리턴 . &#39;dog&#39; . dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(224)) . ImageDataLoaders.from_name_func(path, fnames, label_func, valid_pct=0.2, seed=None, item_tfms=None, batch_tfms=None, bs=64, val_bs=None, shuffle=True, device=None) . dls.show_batch(max_n=16) # 16개의 사진들을 보여줘라. . cnn_learner 사용 전 에러뜬다면 설치도 해주고~ . !conda install -c conda-forge jupyterlab_widgets -y . !conda install -c conda-forge ipywidgets -y . !conda install -c conda-forge nodejs -y . learn=cnn_learner(dls,resnet34,metrics=error_rate) . 모형이 만들어진 것. | Resnet34 is a 34 layer convolutional neural network(모형 이름) | metrics=error_rate 평가지표로 삼겠다 | cnn_learner(dls, arch, normalize=True, n_out=None, pretrained=True, config=None, loss_func=None, opt_func=Adam, lr=0.001, splitter=None, cbs=None, metrics=None, path=None, model_dir=&#39;models&#39;, wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95), cut=None, n_in=3, init=kaimingnormal, custom_head=None, concat_pool=True, lin_ftrs=None, ps=0.5, first_bn=True, bn_final=False, lin_first=False, y_range=None) | . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.154754 | 0.033434 | 0.006089 | 00:10 | . epoch train_loss valid_loss error_rate time . 0 | 0.061337 | 0.014634 | 0.004736 | 00:11 | . r과 달리 python은 object(learn) 만들어서 기능 세분화 후 분석 | 학습을 시키라는 뜻. | Learner.fine_tune(epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False) | . - 예측 . learn.predict(files[0]) # 파일중 첫 번째 사진 가져와라. . (&#39;dog&#39;, TensorBase(1), TensorBase([3.5456e-06, 1.0000e+00])) . output: (&#39;dog&#39;, tensor(1), tensor([6.1421e-07, 1.0000e+00])) | 이 파일은 개이며, tensor(1) 우리가 강아지는 1로 저장해놓놨음 tensor(확신 확률, loss확률) | Learner.predict(item, rm_type_tfms=None, with_input=False) | . learn.show_results() . - 오답분석 . interp = Interpretation.from_learner(learn) # Interpretation . interp.plot_top_losses(16) # 잘 틀리는 거 16개 뽑아서 보여주기 # 한 개 나온 결과는 이상한데..! . 1주차 4번째 . PILImage.create(&#39;502104_3008_164.png&#39;) # 귀여워.. . learn.predict(PILImage.create(&#39;502104_3008_164.png&#39;)) # 고양이 tensor=0 으로 잘 예측한 모습! . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 1.4151e-11])) . PILImage.create(&#39;p1065602463397191_754_thum.jpg&#39;) # 너 누가 귀여우래.. . learn.predict(PILImage.create(&#39;p1065602463397191_754_thum.jpg&#39;)) # 잘 맞춘다! . (&#39;dog&#39;, TensorBase(1), TensorBase([0.0017, 0.9983])) . PILImage.create(&#39;601f57a1260000bd0c275eca.jpeg&#39;) # 귀여운 경태 . learn.predict(PILImage.create(&#39;601f57a1260000bd0c275eca.jpeg&#39;)) . (&#39;dog&#39;, TensorBase(1), TensorBase([2.4683e-04, 9.9975e-01])) .",
            "url": "https://seoyeonc.github.io/chch/2021/11/13/bd_1%EC%A3%BC%EC%B0%A8.html",
            "relUrl": "/2021/11/13/bd_1%EC%A3%BC%EC%B0%A8.html",
            "date": " • Nov 13, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://seoyeonc.github.io/chch/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://seoyeonc.github.io/chch/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://seoyeonc.github.io/chch/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}