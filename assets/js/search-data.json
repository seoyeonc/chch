{
  
    
        "post0": {
            "title": "빅데이터 분석 (2주차) 9월14일, 9월16일",
            "content": "&#54028;&#51060;&#53664;&#52824;&#47484; &#51060;&#50857;&#54616;&#50668; &#54924;&#44480;&#47784;&#54805; &#54617;&#49845;&#54616;&#44592; . - (1/5) 회귀모형 소개, 손실 함수 . - (2/5) 경사하강법, 경사하강법을 이용하여 회귀계수 1회 업데이트 . - (3/5) 회귀계수 반복 업데이트 . - (4/5) 학습률 . - (5/5) 사과영상 . import torch import numpy as np import matplotlib.pyplot as plt . 로드맵 . 회귀분석 $ to$ 로지스틱 $ to$ 심층신경망(DNN) $ to$ 합성곱신경망(CNN) | . - model: $y_i= w_0+w_1 x_i + epsilon_i = 2.5 + 4x_i + epsilon_i, quad i=1,2, dots,n$ . w라는 로테이션을 많이 사용하는 딥러닝 | . - model: ${ bf y}={ bf X}{ bf W} + boldsymbol{ epsilon}$ . ${ bf y}= begin{bmatrix} y_1 y_2 dots y_n end{bmatrix}, quad { bf X}= begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots 1 &amp; x_n end{bmatrix}, quad { bf W}= begin{bmatrix} 2.5 4 end{bmatrix}, quad boldsymbol{ epsilon}= begin{bmatrix} epsilon_1 dots epsilon_n end{bmatrix}$ | . torch.manual_seed(202150754) n=100 ones=torch.ones(n) x,_ = torch.randn(n).sort() # 배열하면 tendor,index가 반환됨. 필요한 x만 반환 X = torch.vstack([ones,x]).T W = torch.tensor([2.5,4]) ϵ = torch.randn(n)*0.5 y = X@W + ϵ #@는 벡터를 곱하라는 뜻..! ytrue = X@W # 우리가 알고 싶은 것은 평균 직선 . plt.plot(x,y,&#39;o&#39;) #우리가 관측한 값 plt.plot(x,ytrue,&#39;--&#39;) # 우리가 추론하고 싶은 값 . [&lt;matplotlib.lines.Line2D at 0x7f1443b129a0&gt;] . &#54617;&#49845; . - 파란점만 주어졌을때, 주황색 점선을 추론하는것. . - 좀 더 정확하게 말하면 given data로 $ begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$를 최대한 $ begin{bmatrix} 2.5 4 end{bmatrix}$와 비슷하게 찾는것. (2.5와 4는 true의 값) . given data : $ big {(x_i,y_i) big }_{i=1}^{n}$ . | parameter: ${ bf W}= begin{bmatrix} w_0 w_1 end{bmatrix}$ . | estimated parameter: ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}$ . | . plt.plot(x,y,&#39;o&#39;) # 그림을 보고 &#39;적당한&#39; 추세를 찾는 과정 . [&lt;matplotlib.lines.Line2D at 0x7f1443a1b9d0&gt;] . - 시도: $( hat{w}_0, hat{w}_1)=(-5,10)$을 선택하여 선을 그려보고 적당한지 판단. . $ hat{y}_i=-5 +10 x_i$ 와 같이 $y_i$의 값을 적합시키겠다는 의미 | . plt.plot(x,y,&#39;o&#39;) plt.plot(x,-5+10*x,&#39;--&#39;) # 그림을 보고 판단해보는 단계 . [&lt;matplotlib.lines.Line2D at 0x7f1443994220&gt;] . - 벡터 표현으로주황색 추세선을 계산 . What = torch.tensor([-5.0,10.0]) # float으로 선언해야 함!!! plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@What,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f14438fa310&gt;] . &#54028;&#46972;&#48120;&#53552;&#47484; &#54617;&#49845;&#54616;&#45716; &#48169;&#48277;, &#51593; &#51201;&#45817;&#54620; &#49440;&#50640; &#47582;&#52656;&#44032;&#45716; &#44284;&#51221; . - 이론적으로 추론 (회귀 분석) . - 컴퓨터의 반복계산을 이용하여 추론(경사하강법) . (1) initial value: 임의의 선을 그어봄 . What = torch.tensor([-5.0,10.0],requires_grad=True) What . tensor([-5., 10.], requires_grad=True) . 처음에는 ${ bf hat{W}}= begin{bmatrix} hat{w}_0 hat{w}_1 end{bmatrix}= begin{bmatrix} -5 10 end{bmatrix} $ 를 대입해서 주황색 점선을 적당히 그려보자는 의미 . | 끝에 requires_grad=True는 나중에 미분에 사용되기 위함. . | . yhat=X@What # yhat을 구하면 단지 X(미분X)*What(미분O) = 미분 옵션이 있는 텐션이 되어버린다. yhat.data # 미분 옵션이 사라짐. . tensor([-27.9716, -26.0391, -25.8951, -24.1830, -23.6405, -23.1161, -22.0441, -21.9913, -21.4959, -21.2860, -20.4771, -19.6991, -19.1434, -18.0758, -17.5390, -17.4888, -16.8212, -16.6630, -16.2503, -14.3326, -13.8527, -13.6397, -13.5228, -13.2096, -12.8514, -12.8461, -12.7527, -12.2431, -12.0267, -11.7990, -11.6495, -11.5587, -11.5497, -11.1709, -10.9643, -10.7969, -10.7696, -10.7324, -10.6567, -10.4404, -10.1049, -9.9527, -9.7916, -9.3899, -9.2762, -8.2773, -8.0850, -7.9550, -7.8498, -7.7767, -7.6419, -7.2295, -7.1686, -6.9773, -6.9454, -6.6435, -5.6597, -5.5200, -5.4562, -5.3640, -4.9588, -4.9111, -4.5447, -3.9894, -3.6367, -3.0762, -2.4928, -2.4512, -2.1695, -2.0062, -1.7060, 0.1909, 0.5915, 0.9467, 1.3453, 1.4359, 2.0752, 2.4723, 2.5368, 2.7189, 2.7902, 2.8337, 3.2249, 3.7238, 3.8636, 3.9170, 3.9852, 5.0601, 5.7496, 6.0569, 7.0621, 7.2674, 7.6805, 7.9669, 8.4266, 9.6044, 9.6791, 10.7418, 12.6324, 18.9507]) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.data,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f143d0d54f0&gt;] . (2) 첫 번째 수정: 추세선의 적당한 정도를 판단하여 적당한 선으로 업데이트 . - &#39;적당한 정도&#39;를 판단하기 위하 장치로서 loss function 도입 . $loss= sum_{i=1}^{n}(y_i- hat{y}_i)^2= sum_{i=1}^{n}(y_i-( hat{w}_0+ hat{w}_1x_i))^2$ . $=({ bf y}-{ bf hat{y}})^ top({ bf y}-{ bf hat{y}})=({ bf y}-{ bf X}{ bf hat{W}})^ top({ bf y}-{ bf X}{ bf hat{W}})$ . 구현하기 편하게 하기 위해 벡터로 구현 . - loss 함수의 특징 . $y_i approx hat{y}_i$ 일수록 loss값이 작다. | $y_i approx hat{y}_i$ 이 되도록 $( hat{w}_0, hat{w}_1)$을 잘 찍으면 loss값이 작다. | (중요) 주황색 점선이 적당할수록 loss값이 작다. | . loss = torch.sum((y-yhat)**2) # = (y-yhat)@(y-yhat) loss . tensor(11003.1260, grad_fn=&lt;SumBackward0&gt;) . - $loss(=11003.1260)$을 줄이는, 혹은 없애는 것이 목표 $ to$ 아예 모든 조합 $( hat{w}_0, hat{w}_1)$에 대하여 가장 작은 $loss$ 찾기 . - 문제의 치환 . 적당해 보이는 주황색 선을 찾자 $ to$ $loss(w_0,w_1)$을 최소로 하는 $(w_0,w_1$의 값을 찾기 | . - $loss(w_0,w_1)$를 최소로 하는 $(w_0,w_1)$ 구하는 것으로 수정된 목표 . 단순한 수학문제가 되었다. 마치 $loss(w)=w^2-2w+3$ 을 최소화하는 $w$를 찾으라는 것과 같음. | . - 경사하강법, 벡터미분 사용! . &#44221;&#49324;&#54616;&#44053;&#48277; . 경사하강법 아이디어(1차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접선) &lt;-- 미분 . (step 3) 순간기울기(= 미분계수)의 부호를 살펴보고 부호와 반대방향으로 움직인다. (순간기울기와 같은 방향으로 움직이면 점점 커질테니까.) . (Tip) 기울기의 절대값 크기와 비례하여 보폭(= 움직이는 정도)을 조절한다. . 경사하강법 아이디어(2차원) . (step 1) 임의의 점을 찍는다. . (step 2) 그 점에서 순간기울기를 구한다. (접평면) &lt;-- 편미분 . (step 3) 순간기울기(= 여러개의 미분계수)의 부호를 살펴보고 부호와 반대방향으로 각각 움직인다. (순간기울기와 같은 방향으로 움직이면 점점 커질테니까.) . (Tip) 기울기의 절대값 크기와 비례하여 보폭(= 움직이는 정도)을 각각 조절한다. . loss를 줄이도록 $W$를 개선하는 방법 . - $수정값 leftarrow 원래값 - 기울어진 크기(= 미분계수) times alpha$ . 여기에서 $ alpha$는 전체적인 보폭의 크기를 결정한다. 즉, $ alpha$값이 클수록 한 번의 update에 움직이는 양이 크다. | . - ${ bf W} leftarrow { bf W} - alpha times frac{ partial}{ partial { bf W}}loss(w_0,w_1)$ . 마이너스의 의미 기울기의 부호를 보고 반대방향으로 움직여라 . | $ frac{ partial}{ partial { bf W}}loss(w_0,w_1):$ 기울기의 절대값 크기와 비례하여 움직이는 정도를 조정하라. (속도의 조절) . | $ alpha$의 의미: 전체적인 보폭의 속도를 조절, $ alpha$가 크면 전체적으로 빠르게 움직인다. 다리의 길이로 비유할 수 있다. . | . . - 목표: $loss(=11003.1260)$ 값을 줄이는 것. . - 방법: 경사하강법 . - 경사하강법으로 loss를 줄이기 위해서는 $ frac{ partial}{ partial { bf W}}loss(w_0,w_1)$의 계산이 필요한데, 이를 위해서 벡터미분이 필요하다. . requires_grad=True를 가진 텐서로 미분. | . loss=torch.sum((y-yhat)**2)= torch.sum((y-X@What)**2) # 이었고 What=torch.tensor([-5.0,10.0],requires_grad=True) # 이므로 결국 What으로 미분하라는 의미. # 미분한 식이 나오는 것이 아니고, # 그 식에 (-5.0, 10.0)을 대입한 계수값이 계산됨. . loss.backward() # requires_grad=True를 가진 텐서로 미분하라는 의미 # 단지 미분 계수가 계산되어 있음 # 정확하게 말하면 미분을 활용하여 $(-5,10)$에서의 순간기울기를 구했다는 의미임. . What.grad.data . tensor([-1730.4250, 1485.8135]) . 이것이 의미하는 건 $(-5,10)$에서의 순간기울기가 $([-1730.4250, 1485.8135])$ 이라는 의미 (각각 (양수, 음수)로 움직여야 함) | . - 직접 계산하여 검증 . $loss(w_0,w_1)=(y- hat{y})^ top (y- hat{y})=(y-XW)^ top (y-XW)$ . | $ frac{ partial}{ partial W}loss(w_0,w_1)=-2X^ top y+2X^ top X W$ . | . -2 * X.T @ y + 2 * X.T @ X @ What # = What.grad.data . tensor([-1730.4250, 1485.8135], grad_fn=&lt;AddBackward0&gt;) . alpha=0.001 print(&#39;수정 전: &#39; + str(What.data)) # 미분 옵션 없애기! print(&#39;수정하는 폭: &#39; + str(-alpha*What.grad.data)) print(&#39;수정 후: &#39; + str(What.data-alpha*What.grad.data)) print(&#39;*참값: (2.5,4)&#39;) . 수정 전: tensor([-5., 10.]) 수정하는 폭: tensor([ 1.7304, -1.4858]) 수정 후: tensor([-3.2696, 8.5142]) *참값: (2.5,4) . Wbefore = What.data Wafter = What.data-alpha*What.grad.data Wbefore, Wafter . (tensor([-5., 10.]), tensor([-3.2696, 8.5142])) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,X@Wbefore,&#39;--b&#39;) # 수정 전 파란 점선 plt.plot(x,X@Wafter,&#39;--r&#39;) # 수정 후 빨간 점선 plt.title(&quot;before: blue // after: red&quot;) . Text(0.5, 1.0, &#39;before: blue // after: red&#39;) . (3) Learn (=estimate $ bf hat{W})$: . What= torch.tensor([-5.0,10.0],requires_grad=True) . alpha=0.001 for epoc in range(30): What.grad=None yhat=X@What loss=torch.sum((y-yhat)**2) loss.backward() # What으로 미분하는 과정 What.data = What.data-alpha * What.grad.data # 적정한 선으로 Update! . What.data # true 값은 2.5,4 . tensor([2.5248, 3.9898]) . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@What.data),&#39;--&#39;) # 순수 데이터만 뽑기 위해 .data꼭 붙이기 plt.plot(x,(X@np.array([2.5,4])),&#39;-&#39;) # 거의 비슷해서 한 선으로 보임 . [&lt;matplotlib.lines.Line2D at 0x7f1435f96310&gt;] . &#54028;&#46972;&#47700;&#53552;&#51032; &#49688;&#51221; &#44284;&#51221;&#51012; &#44288;&#52272;&#54624; &#49688; &#50630;&#45208;.(&#54617;&#49845;&#44284;&#51221; &#47784;&#45768;&#53552;&#47553;) . - 기록 해보기 . losses=[] # 기록하고 싶은 것 1 yhats = [] # 기록하고 싶은 것 2 Whats = [] # 기록하고 싶은 것 3 . What= torch.tensor([-5.0,10.0],requires_grad=True) alpha=0.001 for epoc in range(30): Whats=Whats+[What.data.tolist()] # What을 list화 해서 저장 What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses=losses+[loss.item()] loss.backward() # What으로 미분하는 과정 What.data = What.data-alpha * What.grad.data # 적정한 선으로 Update! . - $ hat{y}$ 관찰 . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[5],&#39;--&#39;) # 5번 업데이트된 추세선 . [&lt;matplotlib.lines.Line2D at 0x7f1435f20490&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhats[10],&#39;--&#39;) # 10번 업데이트된 추세선 . [&lt;matplotlib.lines.Line2D at 0x7f1435f08100&gt;] . - $ hat{ bf{W}}$ . losses . [11003.1259765625, 6417.849609375, 3748.93798828125, 2195.045166015625, 1290.041015625, 762.7489624023438, 455.38189697265625, 276.1114196777344, 171.48153686523438, 110.3656005859375, 74.63228607177734, 53.71556854248047, 41.45503234863281, 34.25670623779297, 30.02236557006836, 27.52593421936035, 26.050209045410156, 25.175155639648438, 24.654428482055664, 24.34327507019043, 24.156478881835938, 24.043743133544922, 23.975299835205078, 23.93347930908203, 23.907733917236328, 23.891769409179688, 23.881786346435547, 23.8754940032959, 23.871488571166992, 23.868919372558594] . plt.plot(losses) . [&lt;matplotlib.lines.Line2D at 0x7f1435e55fd0&gt;] . Animation . plt.rcParams[&#39;figure.figsize&#39;] = (10,4) # 크기 plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; # 애니메이션 나오게 하는 옵션 . from matplotlib import animation fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . . &lt;/input&gt; Once Loop Reflect $ alpha$&#50640; &#45824;&#54616;&#50668; ($ alpha$&#45716; &#54617;&#49845;&#47456;) . (1) $ alpha$가 너무 작다면?$ to$ 비효율적이다. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . . alpha=0.0001 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . . &lt;/input&gt; Once Loop Reflect (1) $ alpha$가 너무 크다면? $ to$ 다른의미에서 비효율적이다 + 위험하다.. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . . alpha=0.0083 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . . &lt;/input&gt; Once Loop Reflect (3) $ alpha=0.0085$ 아예 모형을 벗어나버린다. . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . . alpha=0.0085 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . . &lt;/input&gt; Once Loop Reflect (4) $ alpha=0.01$ . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . . alpha=0.01 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . . &lt;/input&gt; Once Loop Reflect (5) $ alpha=0.006$ 숙제 . losses = [] # 기록하고 싶은것 1 yhats = [] # 기록하고 싶은것 2 Whats = [] # 기록하고 싶은것 3 . . alpha=0.006 What= torch.tensor([-5.0,10.0],requires_grad=True) for epoc in range(30): Whats=Whats+[What.data.tolist()] What.grad=None yhat=X@What yhats=yhats+[yhat.data.tolist()] loss=torch.sum((y-yhat)**2) losses = losses + [loss.item()] loss.backward() What.data = What.data-alpha * What.grad.data . . fig = plt.figure() ax1 = fig.add_subplot(1, 2, 1) ax2 = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ## ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,yhats[0]) ## ax2: 오른쪽그림 _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) _w1 = np.arange(-6, 11, 0.5) w1,w0 = np.meshgrid(_w1,_w0) l=w0*0 for i in range(len(_w0)): for j in range(len(_w1)): l[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2) ax2.plot_surface(w0, w1, l, rstride=1, cstride=1, color=&#39;b&#39;,alpha=0.35) ## 파란색곡면을 그리는 코드(끝) ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color=&#39;red&#39;,marker=&#39;*&#39;) ## 최소점을 표시하는 코드 (붉은색 별) ax2.scatter(np.array(Whats)[0,0],np.array(Whats)[0,1],losses[0],color=&#39;b&#39;) ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) ax2.azim = 40 ## 3d plot의 view 조절 ax2.dist = 8 ## 3d plot의 view 조절 ax2.elev = 5 ## 3d plot의 view 조절 def animate(epoc): line.set_ydata(yhats[epoc]) ax2.scatter(np.array(Whats)[epoc,0],np.array(Whats)[epoc,1],losses[epoc],color=&#39;grey&#39;) return line ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . . &lt;/input&gt; Once Loop Reflect",
            "url": "https://seoyeonc.github.io/chch/2021/11/14/_bd_2%EC%A3%BC%EC%B0%A8_2.html",
            "relUrl": "/2021/11/14/_bd_2%EC%A3%BC%EC%B0%A8_2.html",
            "date": " • Nov 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "빅데이터 분석 (2주차) 9월9일",
            "content": "Path, &#51060;&#48120;&#51648; &#53356;&#47204;&#47553;&#44284; CNN&#47784;&#45944; . - (1/4) Path 설명 . - (2/4) 이미지 크롤링 . - (3/4) 모형학습 및 결과분석 . - (4/4) 테스트 . from fastai.data.all import * from fastai.vision.all import * . path=Path() # 현재 위치 저장 현재폴더=. 상위폴더=.. path . Path(&#39;.&#39;) . path.ls() . (#9) [Path(&#39;bd_1주차.ipynb&#39;),Path(&#39;601f57a1260000bd0c275eca.jpeg&#39;),Path(&#39;2021_09_07_(1주차)_9월7일.ipynb&#39;),Path(&#39;p1065602463397191_754_thum.jpg&#39;),Path(&#39;502104_3008_164.png&#39;),Path(&#39;2021_09_09_(2주차)_9월9일.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2021-09-27-(3주차)_9월27일(1).ipynb&#39;),Path(&#39;bd_2주차.ipynb&#39;)] . (path/&#39;폴더 이름 넣어~&#39;).ls() . path=Path() . (path/&#39;asdf&#39;).mkdir() . (path/&#39;asdf&#39;).ls() . (#0) [] . (path/&#39;asdf&#39;).mkdir() # 이미 있는 폴더면 오류 발생 . FileExistsError Traceback (most recent call last) &lt;ipython-input-19-96a686fc4db7&gt; in &lt;module&gt; -&gt; 1 (path/&#39;asdf&#39;).mkdir() # 이미 있는 폴더면 오류 발생 ~/anaconda3/envs/csy/lib/python3.8/pathlib.py in mkdir(self, mode, parents, exist_ok) 1286 self._raise_closed() 1287 try: -&gt; 1288 self._accessor.mkdir(self, mode) 1289 except FileNotFoundError: 1290 if not parents or self.parent == self: FileExistsError: [Errno 17] File exists: &#39;asdf&#39; . (path/&#39;asdf&#39;).mkdir(exist_ok=True) # 이미 존재하면 무시~ . (path/&#39;asdf&#39;).rmdir() # 생성한 폴더 삭제 . &#51060;&#48120;&#51648; &#53356;&#47204;&#47553; . - 이미지 크롤링 . 검색 2. 이미지 주소를 찾음 3. 해당 주소로 이동하여 저장하는 과정 반복 | | . - 다른방법: 덕덕고를 이용한 이미지 크롤링 . ref: https://github.com/fastai/fastbook/blob/master/utils.py | . def search_images_ddg(key,max_n=200): &quot;&quot;&quot;Search for &#39;key&#39; with DuckDuckGo and return a unique urls of &#39;max_n&#39; images (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api) &quot;&quot;&quot; url = &#39;https://duckduckgo.com/&#39; params = {&#39;q&#39;:key} res = requests.post(url,data=params) searchObj = re.search(r&#39;vqd=([ d-]+) &amp;&#39;,res.text) if not searchObj: print(&#39;Token Parsing Failed !&#39;); return requestUrl = url + &#39;i.js&#39; headers = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0&#39;} params = ((&#39;l&#39;,&#39;us-en&#39;),(&#39;o&#39;,&#39;json&#39;),(&#39;q&#39;,key),(&#39;vqd&#39;,searchObj.group(1)),(&#39;f&#39;,&#39;,,,&#39;),(&#39;p&#39;,&#39;1&#39;),(&#39;v7exp&#39;,&#39;a&#39;)) urls = [] while True: try: res = requests.get(requestUrl,headers=headers,params=params) data = json.loads(res.text) for obj in data[&#39;results&#39;]: urls.append(obj[&#39;image&#39;]) max_n = max_n - 1 if max_n &lt; 1: return L(set(urls)) # dedupe if &#39;next&#39; not in data: return L(set(urls)) requestUrl = url + data[&#39;next&#39;] except: pass . . search_images_ddg(검색어)를 이용하여 검색어에 해당하는 url 얻기m . search_images_ddg(&#39;Holybang&#39;,max_n=5) . (#5) [&#39;https://i.ytimg.com/vi/fcmDi1DCGDs/maxresdefault.jpg&#39;,&#39;http://one-we.cn/uploads/allimg/191218/1-19121Q35R5G9.jpg&#39;,&#39;https://i.ytimg.com/vi/SBWy4-ZU4qQ/maxresdefault.jpg&#39;,&#39;https://www.kpophighindia.com/wp-content/uploads/2021/07/crews-1.jpg&#39;,&#39;https://t1.daumcdn.net/cfile/tistory/993004335A12CB082C&#39;] . path=Path() . path.ls() . (#7) [Path(&#39;bd_1주차.ipynb&#39;),Path(&#39;2021_09_07_(1주차)_9월7일.ipynb&#39;),Path(&#39;bd_1st&#39;),Path(&#39;2021_09_09_(2주차)_9월9일.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2021-09-27-(3주차)_9월27일(1).ipynb&#39;),Path(&#39;bd_2주차.ipynb&#39;)] . download_images(path,urls=search_images_ddg(&#39;Holybang&#39;,max_n=5)) . 현재 working directory에 5개의 이미지가 저장된 모습! | . keywords=&#39;sunmi&#39;, &#39;Hyuna&#39; # 단어 한 개 쓰면 키워드로 입력되어서 알파벳 수대로 폴더 만들어짐.. path=Path(&#39;Singer&#39;) . if not path.exists(): # 현재폴더에 Singer 폴더가 있는지 체크 path.mkdir() # 현재폴더에 Singer 폴더가 만들어짐 for keyword in keywords: # keyword=&#39;sunmi&#39;, keyword=&#39;Hyuna&#39; 일때 아래내용을 반복 lastpath=path/keyword # ./Singer/sunmi or ./Singer/Hyuna lastpath.mkdir(exist_ok=True) # make ./Singer/sunmi or ./Singer/Hyuna urls=search_images_ddg(keyword) # &#39;sunmi&#39; 검색어로 url들의 리스트를 얻음 download_images(lastpath,urls=urls) # 그 url에 해당하는 이미지들을 ./Singer/sunmi or ./Singer/Hyuna 에 저장 . Cleaning Data . 탐색기로 파일들을 살펴보니 조금 이상한 확장자도 있음. . | 조금 이상해보이는 확장자도 열리기는 함. . | . PILImage.create(&#39;./singer/iu/00000015.jpg:large&#39;) . FileNotFoundError Traceback (most recent call last) &lt;ipython-input-44-2622c1a578e7&gt; in &lt;module&gt; -&gt; 1 PILImage.create(&#39;./singer/iu/00000015.jpg:large&#39;) ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/core.py in create(cls, fn, **kwargs) 108 if isinstance(fn,ndarray): return cls(Image.fromarray(fn)) 109 if isinstance(fn,bytes): fn = io.BytesIO(fn) --&gt; 110 return cls(load_image(fn, **merge(cls._open_args, kwargs))) 111 112 def show(self, ctx=None, **kwargs): ~/anaconda3/envs/csy/lib/python3.8/site-packages/fastai/vision/core.py in load_image(fn, mode) 83 def load_image(fn, mode=None): 84 &#34;Open and load a `PIL.Image` and convert to `mode`&#34; &gt; 85 im = Image.open(fn) 86 im.load() 87 im = im._new(im.im) ~/anaconda3/envs/csy/lib/python3.8/site-packages/PIL/Image.py in open(fp, mode, formats) 2973 2974 if filename: -&gt; 2975 fp = builtins.open(filename, &#34;rb&#34;) 2976 exclusive_fp = True 2977 FileNotFoundError: [Errno 2] No such file or directory: &#39;./singer/iu/00000015.jpg:large&#39; . verify_images(get_image_files(path)) . (#4) [Path(&#39;Singer/sunmi/00000065.jpg&#39;),Path(&#39;Singer/Hyuna/00000034.jpg&#39;),Path(&#39;Singer/Hyuna/00000039.jpg&#39;),Path(&#39;Singer/Hyuna/00000025.jpeg&#39;)] . 위에 해당하는 이미지를 수동으로 지워줌 | 나중에 지우는 함수 배움(조금 까다로움) | . - fastai 가 지원하는 함수로 분석하기 좋게 dls 만들기 . dls=ImageDataLoaders.from_folder( path, train=&#39;singer&#39;, valid_pct=0.2, item_tfms=Resize(224)) . ImageDataLoaders.from_folder(path, train=&#39;train&#39;, valid=&#39;valid&#39;, valid_pct=None, seed=None, vocab=None, item_tfms=None, batch_tfms=None, bs=64, val_bs=None, shuffle=True, device=None) . dls.show_batch(max_n=16) . learn=cnn_learner(dls,resnet34,metrics=error_rate) learn.fine_tune(7) . epoch train_loss valid_loss error_rate time . 0 | 1.169335 | 1.660869 | 0.470588 | 00:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.718052 | 0.950780 | 0.397059 | 00:04 | . 1 | 0.634088 | 0.537583 | 0.205882 | 00:04 | . 2 | 0.490461 | 0.444096 | 0.205882 | 00:04 | . 3 | 0.389381 | 0.479742 | 0.161765 | 00:04 | . 4 | 0.317441 | 0.513656 | 0.176471 | 00:04 | . 5 | 0.269008 | 0.556920 | 0.191176 | 00:04 | . 6 | 0.234722 | 0.560562 | 0.205882 | 00:04 | . learn.show_results(max_n=16) . &#50724;&#45813;&#48516;&#49437; . interp=Interpretation.from_learner(learn) interp.plot_top_losses(16) . 수동으로 특정 observation에 대한 예측결과를 확인 | . dls.train_ds . (#275) [(PILImage mode=RGB size=1080x1080, TensorCategory(1)),(PILImage mode=RGB size=1038x1557, TensorCategory(1)),(PILImage mode=RGB size=642x858, TensorCategory(1)),(PILImage mode=RGB size=509x509, TensorCategory(0)),(PILImage mode=RGB size=960x1200, TensorCategory(1)),(PILImage mode=RGB size=800x1200, TensorCategory(0)),(PILImage mode=RGB size=3600x2025, TensorCategory(0)),(PILImage mode=RGB size=1100x1716, TensorCategory(1)),(PILImage mode=RGB size=1280x1920, TensorCategory(1)),(PILImage mode=RGB size=768x1024, TensorCategory(1))...] . training test | . dls.train_ds[0] # 첫 번째 observation, 즉, (x1,y1) . (PILImage mode=RGB size=1080x1080, TensorCategory(1)) . $x_1=$PILImage mode=RGB size=960x960 | $y_1=$TensorCategory(1) | . dls.train_ds[100][0] . $x_{100}$=위의 이미지 | . dls.train_ds[100][1] . TensorCategory(1) . $y_{100}=$TensorCategory(1) | . x100=dls.train_ds[100][0] . learn.predict(x100) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0015, 0.9985])) . Test . path=Path() . if not (path/&#39;test&#39;).exists(): (path/&#39;test&#39;).mkdir() . urls=search_images_ddg(&#39;sunmi 선미&#39;,max_n=20) download_images(path/&#39;test&#39;,urls=urls) testset=get_image_files(path/&#39;test&#39;) testset . (#20) [Path(&#39;test/00000010.jpg&#39;),Path(&#39;test/00000005.jpg&#39;),Path(&#39;test/00000013.jpg&#39;),Path(&#39;test/00000011.jpg&#39;),Path(&#39;test/00000003.jpg&#39;),Path(&#39;test/00000000.jpg&#39;),Path(&#39;test/00000004.jpg&#39;),Path(&#39;test/00000016.jpg&#39;),Path(&#39;test/00000012.jpg&#39;),Path(&#39;test/00000006.jpg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.5452, 0.4548])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.5311, 0.4689])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9941, 0.0059])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0239, 0.9761])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.2874, 0.7126])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.2435, 0.7565])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([4.7129e-04, 9.9953e-01])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0046, 0.9954])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.2166, 0.7834])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0633, 0.9367])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9806, 0.0194])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([9.9916e-01, 8.4005e-04])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9590, 0.0410])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0581, 0.9419])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([5.8807e-04, 9.9941e-01])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.1369, 0.8631])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.1169, 0.8831])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.0784, 0.9216])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([7.0567e-07, 1.0000e+00])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.7406, 0.2594])) . 결과를 보니까 sunmi이 많음 → 어느정도 맞추는것 같긴하다 | . PILImage.create(testset[1]) . 실제로 선미인데 현아로 예측한 사진 | . path=Path() . if not (path/&#39;test2&#39;).exists(): (path/&#39;test2&#39;).mkdir() . urls=search_images_ddg(&#39;hyuna 현아&#39;,max_n=20) download_images(path/&#39;test2&#39;,urls=urls) testset=get_image_files(path/&#39;test2&#39;) testset . (#19) [Path(&#39;test2/00000010.jpg&#39;),Path(&#39;test2/00000000.jpeg&#39;),Path(&#39;test2/00000005.jpg&#39;),Path(&#39;test2/00000013.jpg&#39;),Path(&#39;test2/00000011.jpg&#39;),Path(&#39;test2/00000003.jpg&#39;),Path(&#39;test2/00000018.jpeg&#39;),Path(&#39;test2/00000004.jpg&#39;),Path(&#39;test2/00000016.jpg&#39;),Path(&#39;test2/00000009.jpeg&#39;)...] . for i in range(len(testset)): print(learn.predict(PILImage.create(testset[i]))) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([1.0000e+00, 3.2445e-06])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.4499, 0.5501])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9945, 0.0055])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([9.9906e-01, 9.4329e-04])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9759, 0.0241])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.5947, 0.4053])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9765, 0.0235])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9819, 0.0181])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.3257, 0.6743])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9685, 0.0315])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([9.9974e-01, 2.6368e-04])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9840, 0.0160])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([9.9996e-01, 4.0536e-05])) . (&#39;sunmi&#39;, TensorBase(1), TensorBase([0.2522, 0.7478])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([9.9949e-01, 5.0994e-04])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9084, 0.0916])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.8650, 0.1350])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.9987, 0.0013])) . (&#39;Hyuna&#39;, TensorBase(0), TensorBase([0.7162, 0.2838])) . 결과를 보니 Hyuna 역시 잘 맞추는 듯 보인다. | . - 정확률이 아쉽긴 하지만 어느정도 유의미한 결과를 얻었다. . PILImage.create(testset[1]) # 현아인데 선미로 예측한 사진 .",
            "url": "https://seoyeonc.github.io/chch/2021/11/14/_bd_2%EC%A3%BC%EC%B0%A8_1.html",
            "relUrl": "/2021/11/14/_bd_2%EC%A3%BC%EC%B0%A8_1.html",
            "date": " • Nov 14, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "빅데이터분석 (1주차) 21년9월7일",
            "content": "1&#51452;&#52264; 9&#50900; 7&#51068; . https://guebin.github.io/2021BDA/2021/09/07/(1%EC%A3%BC%EC%B0%A8)-9%EC%9B%947%EC%9D%BC.html . - (1/6): 아나콘다 가상환경 만들기, 파이토치 설치, 주피터랩 설치, conda install 과 pip install 의 차이 . - (2/6): 이미지 분석을 위한 데이터셋 준비 및 정리 . - (3/6): 학습 및 예측 . - (4/6): 코랩설명 + 깃허브/블로그 (뒷부분은 화면전환 오류로 설명이 부실함) . - (5/6): 코랩설명 + 깃허브/블로그 . - (6/6): 우리강아지 이미지를 활용한 예측 . from fastai.vision.all import * . fastai 깔기, error 뜬다면 런타임에서 GPU 변경 | !pip install --ungrade fastai | . path=untar_data(URLs.PETS)/&#39;images&#39; # To download model **pretrained** weights: . files=get_image_files(path) # 이미지파일들의 이름을 모두 복붙하여 리스트를 만든뒤에 files.txt로 저장하는 과정으로 비유할 수 있음 . files[2] # txt파일의 3번째 목록 . Path(&#39;/home/cgb4/.fastai/data/oxford-iiit-pet/images/leonberger_5.jpg&#39;) . def label_func(f): if f[0].isupper(): return &#39;cat&#39; else: return &#39;dog&#39; . 만일 f[0]파일 제목의 첫 글자가 대문자면 고양이, 소문자면 강아지로 반환 | 원래 있던 모델 | . label_func(&#39;asdf&#39;) # 소문자로 시작하니까 강아지로 리턴 . &#39;dog&#39; . dls=ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(224)) . ImageDataLoaders.from_name_func(path, fnames, label_func, valid_pct=0.2, seed=None, item_tfms=None, batch_tfms=None, bs=64, val_bs=None, shuffle=True, device=None) . dls.show_batch(max_n=16) # 16개의 사진들을 보여줘라. . cnn_learner 사용 전 에러뜬다면 설치도 해주고~ . !conda install -c conda-forge jupyterlab_widgets -y . !conda install -c conda-forge ipywidgets -y . !conda install -c conda-forge nodejs -y . learn=cnn_learner(dls,resnet34,metrics=error_rate) . 모형이 만들어진 것. | Resnet34 is a 34 layer convolutional neural network(모형 이름) | metrics=error_rate 평가지표로 삼겠다 | cnn_learner(dls, arch, normalize=True, n_out=None, pretrained=True, config=None, loss_func=None, opt_func=Adam, lr=0.001, splitter=None, cbs=None, metrics=None, path=None, model_dir=&#39;models&#39;, wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95, 0.85, 0.95), cut=None, n_in=3, init=kaimingnormal, custom_head=None, concat_pool=True, lin_ftrs=None, ps=0.5, first_bn=True, bn_final=False, lin_first=False, y_range=None) | . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.154754 | 0.033434 | 0.006089 | 00:10 | . epoch train_loss valid_loss error_rate time . 0 | 0.061337 | 0.014634 | 0.004736 | 00:11 | . r과 달리 python은 object(learn) 만들어서 기능 세분화 후 분석 | 학습을 시키라는 뜻. | Learner.fine_tune(epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False) | . - 예측 . learn.predict(files[0]) # 파일중 첫 번째 사진 가져와라. . (&#39;dog&#39;, TensorBase(1), TensorBase([3.5456e-06, 1.0000e+00])) . output: (&#39;dog&#39;, tensor(1), tensor([6.1421e-07, 1.0000e+00])) | 이 파일은 개이며, tensor(1) 우리가 강아지는 1로 저장해놓놨음 tensor(확신 확률, loss확률) | Learner.predict(item, rm_type_tfms=None, with_input=False) | . learn.show_results() . - 오답분석 . interp = Interpretation.from_learner(learn) # Interpretation . interp.plot_top_losses(16) # 잘 틀리는 거 16개 뽑아서 보여주기 # 한 개 나온 결과는 이상한데..! . 1주차 4번째 . PILImage.create(&#39;502104_3008_164.png&#39;) # 귀여워.. . learn.predict(PILImage.create(&#39;502104_3008_164.png&#39;)) # 고양이 tensor=0 으로 잘 예측한 모습! . (&#39;cat&#39;, TensorBase(0), TensorBase([1.0000e+00, 1.4151e-11])) . PILImage.create(&#39;p1065602463397191_754_thum.jpg&#39;) # 너 누가 귀여우래.. . learn.predict(PILImage.create(&#39;p1065602463397191_754_thum.jpg&#39;)) # 잘 맞춘다! . (&#39;dog&#39;, TensorBase(1), TensorBase([0.0017, 0.9983])) . PILImage.create(&#39;601f57a1260000bd0c275eca.jpeg&#39;) # 귀여운 경태 . learn.predict(PILImage.create(&#39;601f57a1260000bd0c275eca.jpeg&#39;)) . (&#39;dog&#39;, TensorBase(1), TensorBase([2.4683e-04, 9.9975e-01])) .",
            "url": "https://seoyeonc.github.io/chch/2021/11/14/_bd_1%EC%A3%BC%EC%B0%A8.html",
            "relUrl": "/2021/11/14/_bd_1%EC%A3%BC%EC%B0%A8.html",
            "date": " • Nov 14, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "데이터시각화 특강 (5주차) 10월12일",
            "content": "- (1/8) qqplot . - (2/8) 정규분포, t분포 qqplot 비교 . - (3/8) 분위수를 구하는 다양한 방법 . - (4/8) lambda . - (5/8) map (1) . - (6/8) map (2) . - (7/8) 에드워드 터프티, 찰스미나드의 도표. . &#50696;&#51228; (qqplot): . - 히스토그램이나 박스플랏보다 분포를 특정하기에 좋은 시각화는 없을까? . import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from scipy import stats . np.random.seed(202150754) x=np.random.normal(size=1000,loc=2,scale=1.5) y=stats.t.rvs(df=10,size=1000)/np.sqrt(10/8)*1.5 + 2 . - 우리가 관측한 $x_1, dots,x_{1000}$이 $N(2,1.5^2)$에서 나온 샘플인지 궁금하다. . - 아이디어 . (1) 관측한 값을 순서대로 나열하여 $x_{(1)},x_{(2)}, dots, x_{(1000)}$을 만든다. . x[:2] . array([1.69112919, 2.6509918 ]) . $x_1=2.57513073, quad x_2=3.62626175$ | . x.sort() # 정렬하고 싶을 때 . x[:2] #전이랑 바뀐 순서 볼 수 있다. . array([-3.68558942, -2.10072397]) . $x_{(1)}= -2.44398446, quad x_{(2)}=-2.14071467$ | . (2) 파이썬이나 R로 $N(2,1.5^2)$에서 1000개의 정규분포를 생성. 그리고 순서대로 나열하여 $ tilde{x}_{(1)}, tilde{x}_{(2)}, dots, tilde{x}_{(1000)}$를 만든다. . (3) $x_{(1)} approx tilde{x}_{(1)}, dots , x_{(1000)} approx tilde{x}_{(1000)}$ 이면 x는 정규분포일것 . - 그런데 $ tilde{x}_{(1)}, tilde{x}_{(2)}, dots, tilde{x}_{(1000)}$은 시뮬레이션을 할때마다 다른값이 나올테니까 불안정한 느낌이 든다. $ to$ 이론적인 값을 계산하자. . xx = (x-np.mean(x)) / np.std(x,ddof=1) xx[:2] . array([-4.00826428, -2.9063461 ]) . 실제우리가 관측한값 | . print(stats.norm.ppf(0.001)) print(stats.norm.ppf(0.002)) . -3.090232306167813 -2.878161739095483 . - stats.norm.ppf(0.001) 확률 0.001에 해당하는 정규분포 상에서의 값 . 이론적인 값 | . - 분위수 . m=[i/1000 for i in np.arange(1000)+1] . q=[] for i in range(len(m)): q=q+[stats.norm.ppf(m[i])] . q[:2] . [-3.090232306167813, -2.878161739095483] . - $xx approx q$ 을 확인하기 위해서 $(q,q)$그래프와 $(q,xx)$의 그래프를 그려서 겹쳐보자. . plt.plot(q,xx,&#39;o&#39;) plt.plot(q,q,&#39;-&#39;) # xx가 q와 비슷한지 확인하기 위해서 그래프 겹쳐서 그려보기 . [&lt;matplotlib.lines.Line2D at 0x7fe41f282100&gt;] . 해석: 점들이 주황색선 근처에 모여있을수록 정규분포에 가깝다. | . - 아래와 같이 쉽게 그릴수도 있다. (우리가 그린그림과 조금 다르게 보인다) . _ = stats.probplot(x,plot=plt) . 자세히보면 조금 다르게 그려지긴 하는데 이는 $m=( frac{1}{1000}, dots, frac{999}{1000}, frac{1000}{1000})$와 같이 계산하지 않고 약간 보정한값을 계산하기 때문임 | stats.probplot? 을 통하여 확인한 결과 아래와 같은 코드로 구현됨### 보정하는방법1 n=len(xx) m=[((i+1)-0.3175)/(n+0.365) for i in range(n)] m[-n]=0.5**(1/n) m[0]=1-m[-n] . | 프로그램에 따라서 아래와 같이 보정하는 경우도 있음### 보정하는방법2 m=[(i-3/8)/(n+1/4) for i in np.arange(1000)+1] . | 또 자세히보면 stats.probplot은 y축에 표준화전의 x값이 있음을 알 수 있음. | . - 정규분포와 t분포의 qqplot을 그려서 비교해보자. . _ = stats.probplot(x,plot=plt) # 정규분포 . 정규분포 | . _ = stats.probplot(y,plot=plt) # t분포 . t분포: 푸른점들이 대체로 붉은선위에 놓여있는듯 하지만 양끝단에서는 그렇지 않다. (중앙부근은 정규분포와 비슷하지만, 꼬리부분은 정규분포와 확실히 다르다) | 왼쪽꼬리: 이론적으로 나와야 할 값보다 더 작은값이 실제로 관측됨 | 오른쪽꼬리: 이론적으로 나와야 할 값보다 더 큰값이 실제로 관측됨 | 해석: 이 분포는 정규분포보다 두꺼운 꼬리를 가진다. | . - 서브플랏팅: 두 분포를 양옆에 나란히 비교하고 싶음 . fig , (ax1,ax2) = plt.subplots(1,2) . _ = stats.probplot(x,plot=ax1) _ = stats.probplot(y,plot=ax2) . fig # 이것은 메트플랏라이브러리 기반이라 같이 그릴 수 있음 . fig.set_figwidth(8) . fig . ax1.set_title(&#39;normal dist&#39;) ax2.set_title(&#39;t dist&#39;) . Text(0.5, 1.0, &#39;t dist&#39;) . fig . &#50696;&#51228;4 (boxplot, histrogram, qqplot) . - 박스플랏, 히스토그램, qqplot을 그려보자. . fig, ax =plt.subplots(2,3) . (ax1,ax2,ax3), (ax4,ax5,ax6) = ax . sns.boxplot(x,ax=ax1) sns.histplot(x,kde=True,ax=ax2) _ = stats.probplot(x,plot=ax3) sns.boxplot(y,ax=ax4) sns.histplot(y,kde=True,ax=ax5) _ = stats.probplot(y,plot=ax6) . /home/cgb4/anaconda3/envs/csy/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( /home/cgb4/anaconda3/envs/csy/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . fig . fig.set_figwidth(10) fig.set_figheight(8) fig.tight_layout() . fig . Appendix: &#48516;&#50948;&#49688;&#47484; &#44396;&#54616;&#45716; &#45796;&#50577;&#54620;&#48169;&#48277; . m=[i/1000 for i in np.arange(1000)+1] . $m= big { frac{i}{1000}: i in {1,2,3, dots,1000 } big }= big { frac{1}{1000}, frac{2}{1000}, dots, frac{1000}{1000} big }$ | . - 방법1 . q=[] for i in range(len(m)): q=q+[stats.norm.ppf(m[i])] q[:5] . [-3.090232306167813, -2.878161739095483, -2.7477813854449926, -2.6520698079021954, -2.575829303548901] . - 방법2 . q=[stats.norm.ppf(m[i]) for i in range(len(m))] . q[:5] # 두 번째 방법 stats.norm.ppf는 여기서 m(i)에 대응하는 정규분포의 값 . [-3.090232306167813, -2.878161739095483, -2.7477813854449926, -2.6520698079021954, -2.575829303548901] . - 방법3 . q=list(map(stats.norm.ppf, m)) q[:5] # 세 번째 방법 list 꼭 해줘야 값들이 나타남 . [-3.090232306167813, -2.878161739095483, -2.7477813854449926, -2.6520698079021954, -2.575829303548901] . - 방법4 . stats.norm.ppf(m)[:5] . array([-3.09023231, -2.87816174, -2.74778139, -2.65206981, -2.5758293 ]) . Appendix: lambda, map . lambda . - 예제1: 사용방법 . f = lambda x,y,z : x+y+z ## lambda 입력:출력 . f(2,3,4) . 9 . - 예제2: 디폴트입력값 . x= (lambda a=&#39;fee&#39;,b=&#39;fie&#39;,c=&#39;foe&#39;: a+b+c) . x(&#39;wee&#39;) # x가 object화 된다. . &#39;weefiefoe&#39; . - 예제3: 람다들의 리스트가능 . l=[lambda x: x**2, lambda x: x**3, lambda x: x**4] . for f in l: print(f(2)) . 4 8 16 . - 예제4: 람다들의 딕셔너리 가능 . dct={&#39;f1&#39;: (lambda x: x+1), &#39;f2&#39;: (lambda x: x+22), &#39;f3&#39;: (lambda x: x+333)} . dct[&#39;f1&#39;](1), dct[&#39;f2&#39;](1), dct[&#39;f3&#39;](1) . (2, 23, 334) . - 예제5: 조건부 출력 . (예비학습) 문자열의 대소비교 . &#39;a&#39; &lt; &#39;b&#39; . True . &#39;c&#39; &lt; &#39;b&#39; . False . (예제시작) . lower = lambda x,y : x if x&lt;y else y . lower(&#39;a&#39;,&#39;b&#39;) . &#39;a&#39; . lower(&#39;c&#39;,&#39;b&#39;) . &#39;b&#39; . - 예제6 : lambda expression 을 return력가능 . def action(x): return (lambda y : x+y) # 예제 6 lambda expression을 return 출력 가능 # lambda y:x+y 자체가 오브젝트라 가능 # lambda 괄호 생략해도 가능! 여기서는 단순히 구분하기 위함 . act = action(99) ## act는 99+y를 수행하는 함수 act2 = action(98) ## act2는 99+y를 수행하는 함수 . action은 마치 함수를 만드는 함수같다.. | . print(act(2)) print(act2(2)) . 101 100 . - 예제7: 예제6의 발전 . action = lambda x: (lambda y: x+y) . act= action(99) act2=action(98) . print(act(2)) print(act2(2)) . 101 100 . 괄호를 생략하여 선언하면 . action = lambda x: lambda y: x+y act= action(99) act2=action(98) print(act(2)) print(act2(2)) # 괄호 생략해도 값은 같은 모습을 볼 수 있다. . 101 100 . map . - 예제1: 사용방법 . def inc(x): return x+1 . list(map(inc,[1,2,3,4])) . [2, 3, 4, 5] . - 예제1의 변형(람다사용) . list(map(lambda x: x+1,[1,2,3,4])) . [2, 3, 4, 5] . list(map(def inc(x): return x+1,[1,2,3,4])) # 이 코드는 오류뜨는 코드 . File &#34;&lt;ipython-input-67-e9cc33ee66ab&gt;&#34;, line 1 list(map(def inc(x): return x+1,[1,2,3,4])) # 이 코드는 오류뜨는 코드 ^ SyntaxError: invalid syntax . 함수명을 쓰는 자리에 lambda로 표현한 오브젝트 자체를 전달할 수 있다. $ to$ 코드가 간단하다. | . - 예제2: map과 리스트컴프리헨션 비교 . (함수선언) . f = lambda x: &#39;X&#39; in x . f(&#39;X1&#39;),f(&#39;X2&#39;),f(&#39;Y1&#39;),f(&#39;Y2&#39;) . (True, True, False, False) . (map) . list(map(f,[&#39;X1&#39;,&#39;X2&#39;,&#39;Y3&#39;,&#39;Y4&#39;])) . [True, True, False, False] . (리스트컴프리헨션과 비교) . [f(x) for x in [&#39;X1&#39;,&#39;X2&#39;,&#39;Y3&#39;,&#39;Y4&#39;]] . [True, True, False, False] . - 예제3: 두개의 입력을 받는 함수(pow) map, 리스트컴프리헨션 비교 . (함수소개) . pow(2,3) # 2의 3제곱 해주는 함수 . 8 . (map) . list(map(pow,[2,2,2,3,3,3],[0,1,2,0,1,2])) . [1, 2, 4, 1, 3, 9] . (리스트컴프리헨션과 비교) . [pow(x,y) for x,y in zip([2,2,2,3,3,3],[0,1,2,0,1,2])] . [1, 2, 4, 1, 3, 9] . - 예제4: map은 (하나의 함수,다양한 입력)인 경우 사용가능 . l=[lambda x: x+1, lambda x: x+2, lambda x: x+3 ] . list(map(l,[100,200,300]))# 오류 뜨는 코드 # l 함수 자체에 매핑해주면 오류가.. . TypeError Traceback (most recent call last) &lt;ipython-input-76-1316ab488d60&gt; in &lt;module&gt; -&gt; 1 list(map(l,[100,200,300]))# 오류 뜨는 코드 2 # l 함수 자체에 매핑해주면 오류가.. TypeError: &#39;list&#39; object is not callable . 리스트컴프리헨션은 (다양한함수,다양한입력)이 가능함 . [l[i](x) for i,x in zip([0,1,2],[100,200,300])] . [101, 202, 303] . - 종합: 리스트컴프리헨션과 비교하면 (1) 반복인덱스를 쓰지 않는 장점이 있는 반면 (2) 좀 더 제약적으로 사용할 수밖에 없다는 단점이 있음 . &#50528;&#46300;&#50892;&#46300; &#53552;&#54532;&#54000; . - 시각화계의 거장 . - 터프티의 이론중 백미: 엄격한 미니멀리즘 . 최소한의 잉크로 많은 정보를 전달할 수 있다면 그것이 바로 좋은 그래프이다. | 작은 지면 내에서 잉크를 최대한 적게 써서 짧은 시간 안에 많은 영감을 주어야 한다. | . - 데이터-잉크비: 데이터를 표현하는데 들아가는 잉크의 양 / 그래픽을 인쇄하는데 들어가는 잉크의 총량 . - 차트정크 (나이젤홈즈의 그래프) . . “Lurking behind chartjunk is contempt both for information and for the audience. Chartjunk promoters imagine that numbers and details are boring, dull, and tedious, requiring ornament to enliven. Cosmetic decoration, which frequently distorts the data, will never salvage an underlying lack of content. If the numbers are boring, then you’ve got the wrong numbers (...) Worse is contempt for our audience, designing as if readers were obtuse and uncaring. In fact, consumers of graphics are often more intelligent about the information at hand than those who fabricate the data decoration (...) The operating moral premise of information design should be that our readers are alert and caring; they may be busy, eager to get on with it, but they are not stupid.” . 차트정크 = 대중을 멸시 + 데이터에 대한 모독 | 차트정크 옹호가는 숫자와 데이터가 지루하여 활기가 필요하다고 생각하는 모양이다.. | . - 별로인 그래프 (왼쪽) / 우수한 그래프 오른쪽 . . - 별로인 그래프 (왼쪽) / 우수한 그래프 오른쪽 . . - 별로인 그래프 (왼쪽) / 우수한 그래프 오른쪽 . . - 글쎼... . &#52272;&#49828;&#48120;&#45208;&#46300;&#51032; &#46020;&#54364; (&#51064;&#47448;&#50669;&#49324;&#49345; &#44032;&#51109; &#54988;&#47469;&#54620; &#49884;&#44033;&#54868;) . . - 터프티의 평 . 지금까지 그려진 최고의 통계 그래픽일지도 모른다. | 여기에서는 군대의 크기, 2차원 평면상의 위치, 군대의 이동방향, 모스코바에서 퇴각하는 동안의 여러날짜, 온도 $ to$ 6차원의 변수 | 백만번에 한번 이런 그림을 그릴수는 있겠지만 이러한 멋진 그래픽을 만드는 방법에 대한 원칙은 없다. $ to$ 미니멀리즘.. | . - 왜 우수한 그래프일까? . 자료를 파악하는 기법은 최근까지도 산점도, 막대그래프, 라인플랏에 의존 | 이러한 플랏의 단점은 고차원의 자료를 분석하기 어렵다는 것임 | 미나드는 여러그램을 그리는 방법 대신에 한 그림에서 패널을 늘리는 방법을 선택함. | . &#50696;&#51228; . x=[44,48,49,58,62,68,69,70,76,79] ## 몸무게 y=[159,160,162,165,167,162,165,175,165,172] ## 키 g= &#39;f&#39;,&#39;f&#39;,&#39;f&#39;,&#39;f&#39;,&#39;m&#39;,&#39;f&#39;,&#39;m&#39;,&#39;m&#39;,&#39;m&#39;,&#39;m&#39; df=pd.DataFrame({&#39;w&#39;:x,&#39;h&#39;:y,&#39;g&#39;:g}) . df . w h g . 0 44 | 159 | f | . 1 48 | 160 | f | . 2 49 | 162 | f | . 3 58 | 165 | f | . 4 62 | 167 | m | . 5 68 | 162 | f | . 6 69 | 165 | m | . 7 70 | 175 | m | . 8 76 | 165 | m | . 9 79 | 172 | m | . - 미나드의 접근방법 . sns.scatterplot(data=df,x=&#39;w&#39;,y=&#39;h&#39;,hue=&#39;g&#39;) . &lt;AxesSubplot:xlabel=&#39;w&#39;, ylabel=&#39;h&#39;&gt; . - 일반적인 사람들 (보통 색깔을 사용할 생각을 못한다.) . figs = sns.FacetGrid(df,col=&#39;g&#39;) # 다중 그래프 그릴때 많이 사용됨 figs.map (sns.scatterplot,&#39;w&#39;,&#39;h&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fe41e51d460&gt; . - 생각보다 데이터가 정리된 형태에 따라서 시각화에 대한 사고방식이 달라진다. 아래와 같은 자료를 받았다고 하자. . df1=df.query(&quot;g ==&#39;f&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 여성.csv df2=df.query(&quot;g ==&#39;m&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 남성.csv #query 함수의 비교 연산자 사용한 경우 . query 함수의 6가지 기능 . 비교 연산자( ==, &gt;, &gt;=, &lt;, &lt;=, != ) | in 연산자( in, ==, not in, != ) | 논리 연산자(and, or, not) | 외부 변수(또는 함수) 참조 연산 | 인덱스 검색 | 문자열 부분검색( str.contains, str.startswith, str.endswith ) | df1 . w h . 0 44 | 159 | . 1 48 | 160 | . 2 49 | 162 | . 3 58 | 165 | . 5 68 | 162 | . df2 . w h . 4 62 | 167 | . 6 69 | 165 | . 7 70 | 175 | . 8 76 | 165 | . 9 79 | 172 | . - 데이터프레임을 바꿀 생각을 하는게 쉽지 않다. . (방법1) . df1[&#39;g&#39;]= &#39;f&#39; # 성별이 여성인 경우만 가져왔기에 성별에 f 표시 . df1 . w h g . 0 44 | 159 | f | . 1 48 | 160 | f | . 2 49 | 162 | f | . 3 58 | 165 | f | . 5 68 | 162 | f | . df2[&#39;g&#39;]= &#39;m&#39; # 성별이 남성인 경우만 가져왔기에 성별에 m 표시 . df2 . w h g . 4 62 | 167 | m | . 6 69 | 165 | m | . 7 70 | 175 | m | . 8 76 | 165 | m | . 9 79 | 172 | m | . pd.concat([df1,df2]) # concat 은 데이터프레임 이어붙여주는 함수 . w h g . 0 44 | 159 | f | . 1 48 | 160 | f | . 2 49 | 162 | f | . 3 58 | 165 | f | . 5 68 | 162 | f | . 4 62 | 167 | m | . 6 69 | 165 | m | . 7 70 | 175 | m | . 8 76 | 165 | m | . 9 79 | 172 | m | . (방법2) . df1=df.query(&quot;g ==&#39;f&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 여성.csv df2=df.query(&quot;g ==&#39;m&#39;&quot;)[[&#39;w&#39;,&#39;h&#39;]] ## 남성.csv . pd.concat([df1,df2],keys=[&#39;f&#39;,&#39;m&#39;]).reset_index().iloc[:,[0,2,3]].rename(columns={&#39;level_0&#39;:&#39;g&#39;}) . g w h . 0 f | 44 | 159 | . 1 f | 48 | 160 | . 2 f | 49 | 162 | . 3 f | 58 | 165 | . 4 f | 68 | 162 | . 5 m | 62 | 167 | . 6 m | 69 | 165 | . 7 m | 70 | 175 | . 8 m | 76 | 165 | . 9 m | 79 | 172 | . - 어려운점: (1) 센스가 없어서 색깔을 넣어서 그룹을 구분할 생각을 못함 (2) 변형해야할 데이터를 생각못함 (3) 데이터를 변형할 생각을 한다고 해도 변형하는 실제적인 코드를 구현할 수 없음 (그래서 엑셀을 킨다..) . (1) 기획력부족 -&gt; 훌륭한 시각화를 많이 볼것 | (2) 데이터프레임에 대한 이해도가 부족 -&gt; tidydata에 대한 개념 | (3) 프로그래밍 능력 부족 -&gt; 코딩공부열심히.. | . - 목표: (2) 어떠한 데이터 형태로 변형해야하는가? (3) 그러한 데이터 형태로 바꾸기 위한 pandas 숙련도 . &#49689;&#51228; . - 자유도가5인 카이제곱분포에서 100개의 랜덤변수를 만들고, boxplot / histogram / qqplot . x=np.random.chisquare(df=5, size=100) . fig,(ax1,ax2,ax3)=plt.subplots(1,3) sns.boxplot(x,ax=ax1) sns.histplot(x, kde=True,ax=ax2) _ = stats.probplot(x,plot=ax3) fig.set_figwidth(10) . /home/cgb4/anaconda3/envs/csy/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( .",
            "url": "https://seoyeonc.github.io/chch/2021/10/12/%EB%8D%B0%EC%8B%9C_5%EC%A3%BC%EC%B0%A8.html",
            "relUrl": "/2021/10/12/%EB%8D%B0%EC%8B%9C_5%EC%A3%BC%EC%B0%A8.html",
            "date": " • Oct 12, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "데이터시각화 특강 (3주차) 9월27일",
            "content": "&#44053;&#51032;&#50689;&#49345; . - (2/8) 이미지 자료에 대한 이해 . - (3/8) 산점도와 상관계수1 . - (4/8) 산점도와 상관계수2 . - (5/8) 여러그림 그리기 . - (6/8) 앤스콤의 플랏 . - (7/8) 앤스콤의 플랏 . (&#51648;&#45212;&#44053;&#51032;&#45432;&#53944; &#48372;&#52649;) &#51060;&#48120;&#51648; &#51088;&#47308;&#50640; &#45824;&#54620; &#51060;&#54644; . - 흑백이미지 . 차원: 세로픽셀수 $ times$ 가로픽셀수 | 값: 0~255 (값이 클수록 흰색) | . - 칼라이미지 . 차원: 세로픽셀수 $ times$ 가로픽셀수 $ times$ 3 | 값: 0~255 (값이 클수록 진한빨강, 진한파랑, 진한녹색) | . import cv2 as cv . hani=cv.imread(&#39;hw_img.png&#39;) . import matplotlib.pyplot as plt plt.imshow(hani) . &lt;matplotlib.image.AxesImage at 0x7fd10d619940&gt; . hani.shape . (531, 468, 3) . import numpy as np hani_red=np.zeros_like(hani) hani_green=np.zeros_like(hani) hani_blue=np.zeros_like(hani) hani_red[:,:,0]=hani[:,:,0] hani_green[:,:,1]=hani[:,:,1] hani_blue[:,:,2]=hani[:,:,2] . plt.imshow(hani_red) . &lt;matplotlib.image.AxesImage at 0x7fd10d5856d0&gt; . plt.imshow(hani_green) . &lt;matplotlib.image.AxesImage at 0x7fd10d56f2b0&gt; . plt.imshow(hani_blue) . &lt;matplotlib.image.AxesImage at 0x7fd10d6ccd60&gt; . plt.imshow(hani_blue+hani_red) . &lt;matplotlib.image.AxesImage at 0x7fd10d7e2190&gt; . plt.imshow(hani_blue+hani_green) . &lt;matplotlib.image.AxesImage at 0x7fd10d6c5550&gt; . plt.imshow(hani_red+hani_green) . &lt;matplotlib.image.AxesImage at 0x7fd10d4d8ee0&gt; . plt.imshow(hani_red+hani_green+hani_blue) . &lt;matplotlib.image.AxesImage at 0x7fd10d4b9bb0&gt; . &#49328;&#51216;&#46020; (scatter plot) . import matplotlib.pyplot as plt . - 산점도: 산점도는 직교 좌표계(도표)를 이용해 좌표상의 점들을 표시함으로써 두 개 변수 간의 관계를 나타내는 그래프 방법 . ref: https://ko.wikipedia.org/wiki/%EC%82%B0%EC%A0%90%EB%8F%84 | . x=[1,2,3,4] y=[2,3,5,5] plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd10d42a070&gt;] . - 산점도는 보통 $X$와 $Y$의 관계를 알고 싶을 경우 그린다. . &#50696;&#51228;: &#47800;&#47924;&#44172;&#50752; &#53412; . - 아래와 같은 자료를 수집하였다고 하자. . 몸무게=[44,48,49,58,62,68,69,70,76,79] | 키=[159,160,162,165,167,162,165,175,165,172] | . x=[44,48,49,58,62,68,69,70,76,79] y=[159,160,162,165,167,162,165,175,165,172] . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd10d38db80&gt;] . 키가 큰 사람일수록 몸무게도 많이 나간다. (반대도 성립) | 키와 몸무게는 관계가 있어보인다. (정비례관계) | . - 얼만큼 정비례 인지? . 이 질문에 대답하기 위해서는 상관계수의 개념을 알아야 한다. | 상관계수에 대한 개념은 산점도를 이해함에 있어서 핵심개념이다. | . &#49345;&#44288;&#44228;&#49688; (&#44036;&#45800;&#54620; &#47532;&#48624;) . - (표본)상관계수 . $$r= frac{ sum_{i=1}^{n}(x_i- bar{x})(y_i- bar{y})}{ sqrt{ sum_{i=1}^{n}(x_i- bar{x})^2 sum_{i=1}^{n}(y_i- bar{y})^2}}$$ . - 복잡해보이지만 아무튼 (1) 분자를 계산하고 (2) 분모를 계산하고 (3) 분자를 분모로 나누면 된다. . - 분모를 계산했다고 치자. 계산한 값을 상수 $c$라고 생각하자. 이 값을 분자의 sum안에 넣으면... . $$r= sum_{i=1}^{n} frac{1}{c}(x_i- bar{x})(y_i- bar{y})$$ . - 이 식을 정리하면 . $$r= sum_{i=1}^{n} Bigg( frac{(x_i- bar{x})}{ sqrt{ sum_{i=1}^{n}(x_i- bar{x})^2}} frac{(y_i- bar{y})}{ sqrt{ sum_{i=1}^{n}(y_i- bar{y})^2}} Bigg)$$ . - 편의상 다음과 같이 정의하자. $ tilde{x}_i = frac{(x_i- bar{x})}{ sqrt{ sum_{i=1}^{n}(x_i- bar{x})^2}}$, $ tilde{y}_i = frac{(y_i- bar{y})}{ sqrt{ sum_{i=1}^{n}(y_i- bar{y})^2}}$ . - 결국 $r$은 아래와 같은 모양이다. . $$r= sum_{i=1}^{n} tilde{x}_i tilde{y}_i$$ . - 의미? . import numpy as np x=np.array(x) y=np.array(y) . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd10d3057f0&gt;] . plt.plot(x-np.mean(x), y-np.mean(y),&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd10d2f7670&gt;] . - $a= sqrt{ sum_{i=1}^{n}(x_i- bar{x})^2}, b= sqrt{ sum_{i=1}^{n}(y_i- bar{y})^2}$ . a=np.sqrt(np.sum((x-np.mean(x))**2)) b=np.sqrt(np.sum((y-np.mean(y))**2)) a,b . (36.58004920718396, 15.218409903797438) . $a&gt;b$ 이므로 $ {x_i }$들이 $ {y_i }$들 보다 좀 더 퍼져있다. (=평균근처에 몰려있지 않다) | . - 사실 $a,b$는 아래와 같이 계산할 수 있다. . $a= sqrt{n} times{ tt np.std(x)}$ . $b= sqrt{n} times{ tt np.std(y)}$ . n=len(x) np.sqrt(n)*np.std(x), np.sqrt(n)*np.std(y) . (36.58004920718397, 15.21840990379744) . ${ tt np.std(x)}= sqrt{ frac{1}{n} sum_{i=1}^{n}(x_i- bar{x})^2}$ | ${ tt np.std(y)}= sqrt{ frac{1}{n} sum_{i=1}^{n}(y_i- bar{y})^2}$ | . . Note: ${ tt np.std(x,ddof=1)}= sqrt{ frac{1}{n-1} sum_{i=1}^{n}(x_i- bar{x})^2}$ . - 이제 $( tilde{x}_i, tilde{y}_i)$를 그려보자. . xx= (x-np.mean(x))/a yy= (y-np.mean(y))/b plt.plot(xx,yy,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd10d267970&gt;] . 평균도 비슷하고 퍼진정도도 비슷하다. | . - 질문1: $r$의 값이 양수인가? 음수인가? . plotly 사용하여 그려보자. . import plotly.express as px from IPython.display import HTML fig=px.scatter(x=xx, y=yy) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;,include_mathjax=False)) . . . $ tilde{x}_i$, $ tilde{y}_i$ 를 곱한값이 양수인것과 음수인것을 체크해보자. | 양수인쪽이 많은지 음수인쪽이 많은지 생각해보자. | $r= sum_{i=1}^{n} tilde{x}_i tilde{y}_i$ 의 부호는? | . - 질문2: 아래와 같은 두개의 데이터set이 있다고 하자. . x1=np.arange(0,10,0.1) y1=x1+np.random.normal(loc=0,scale=1.0,size=len(x1)) . plt.plot(x1,y1,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd10a8fee20&gt;] . x2=np.arange(0,10,0.1) y2=x2+np.random.normal(loc=0,scale=7.0,size=len(x2)) plt.plot(x2,y2,&#39;x&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd10a8e8760&gt;] . plt.plot(x1,y1,&#39;o&#39;) plt.plot(x2,y2,&#39;x&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd10a853b20&gt;] . 각 데이터셋의 표준상관계수를 각각 $r_1$(파란색), $r_2$(주황색)라고 하자. . (1) $r_1$, $r_2$의 부호는 양수인가? 음수인가? . 양수이다 | . (2) $r_1,r_2$의 값중 어떠한 값이 더 절대값이 큰가? . $r_1$이 더 커보인다. 분산이 작아보임 | . n=len(x1) xx1= (x1-np.mean(x1)) / (np.std(x1) * np.sqrt(n)) yy1= (y1-np.mean(y1)) / (np.std(y1) * np.sqrt(n)) xx2= (x2-np.mean(x2)) / (np.std(x2) * np.sqrt(n)) yy2= (y2-np.mean(y2)) / (np.std(y2) * np.sqrt(n)) . plt.plot(xx1,yy1,&#39;o&#39;) ## 파란색 plt.plot(xx2,yy2,&#39;x&#39;) ## 주황색 . [&lt;matplotlib.lines.Line2D at 0x7fd10a7c9400&gt;] . sum(xx1*yy1), sum(xx2*yy2) . (0.947517466375085, 0.37004797528671085) . &#49689;&#51228;1 . - 임의의 이미지를 cv.imread() 로 불러온뒤에 아래와 같이 blue+green의 조합으로 이미지를 변경해볼것 . plt.imshow(hani_blue+hani_green) . &lt;matplotlib.image.AxesImage at 0x7fd10a7ae520&gt; . &#46972;&#51064;&#54540;&#46991;&#51012; &#44536;&#47532;&#45716; &#48169;&#48277; . import matplotlib.pyplot as plt x=[1,2,3,4] y=[1,2,4,3] plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x7fd10a710970&gt;] . matplotlib&#50640;&#49436; &#49328;&#51216;&#46020;&#50752; &#46972;&#51064;&#54540;&#46991; &#44536;&#47532;&#44592; (&#51333;&#54633;) . - plt.plot()를 사용하면 산점도와 라인플랏을 다양한 조합으로 쉽고 편리하게 그릴수 있음 . x=[1,2,3,4] y=[1,2,4,3] plt.plot(x,y,&#39;o:r&#39;) # 20정도의 점의 모양, 4개의 선의모양, 8개의 색깔 . [&lt;matplotlib.lines.Line2D at 0x7fd10a678790&gt;] . &#50668;&#47084;&#44536;&#47548;&#51012; &#44536;&#47532;&#44592; . (1) &#44217;&#52432;&#44536;&#47532;&#44592; . import numpy as np x=np.arange(-5,5,0.1) y=2*x+np.random.normal(loc=0,scale=1,size=100) plt.plot(x,y,&#39;.b&#39;) plt.plot(x,2*x,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd10a6711f0&gt;] . (2) &#46384;&#47196;&#44536;&#47532;&#44592; - subplots . x=[1,2,3,4] y=[1,2,4,3] _, axs = plt.subplots(2,2) axs[0,0].plot(x,y,&#39;o:r&#39;) axs[0,1].plot(x,y,&#39;Xb&#39;) axs[1,0].plot(x,y,&#39;xm&#39;) axs[1,1].plot(x,y,&#39;.--k&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd10a4fbdc0&gt;] . plt.subplots?? . Signature: plt.subplots( nrows=1, ncols=1, *, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw, ) Source: @_api.make_keyword_only(&#34;3.3&#34;, &#34;sharex&#34;) def subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw): &#34;&#34;&#34; Create a figure and a set of subplots. This utility wrapper makes it convenient to create common layouts of subplots, including the enclosing figure object, in a single call. Parameters - nrows, ncols : int, default: 1 Number of rows/columns of the subplot grid. sharex, sharey : bool or {&#39;none&#39;, &#39;all&#39;, &#39;row&#39;, &#39;col&#39;}, default: False Controls sharing of properties among x (*sharex*) or y (*sharey*) axes: - True or &#39;all&#39;: x- or y-axis will be shared among all subplots. - False or &#39;none&#39;: each subplot x- or y-axis will be independent. - &#39;row&#39;: each subplot row will share an x- or y-axis. - &#39;col&#39;: each subplot column will share an x- or y-axis. When subplots have a shared x-axis along a column, only the x tick labels of the bottom subplot are created. Similarly, when subplots have a shared y-axis along a row, only the y tick labels of the first column subplot are created. To later turn other subplots&#39; ticklabels on, use `~matplotlib.axes.Axes.tick_params`. When subplots have a shared axis that has units, calling `~matplotlib.axis.Axis.set_units` will update each axis with the new units. squeeze : bool, default: True - If True, extra dimensions are squeezed out from the returned array of `~matplotlib.axes.Axes`: - if only one subplot is constructed (nrows=ncols=1), the resulting single Axes object is returned as a scalar. - for Nx1 or 1xM subplots, the returned object is a 1D numpy object array of Axes objects. - for NxM, subplots with N&gt;1 and M&gt;1 are returned as a 2D array. - If False, no squeezing at all is done: the returned Axes object is always a 2D array containing Axes instances, even if it ends up being 1x1. subplot_kw : dict, optional Dict with keywords passed to the `~matplotlib.figure.Figure.add_subplot` call used to create each subplot. gridspec_kw : dict, optional Dict with keywords passed to the `~matplotlib.gridspec.GridSpec` constructor used to create the grid the subplots are placed on. **fig_kw All additional keyword arguments are passed to the `.pyplot.figure` call. Returns - fig : `~.figure.Figure` ax : `.axes.Axes` or array of Axes *ax* can be either a single `~matplotlib.axes.Axes` object or an array of Axes objects if more than one subplot was created. The dimensions of the resulting array can be controlled with the squeeze keyword, see above. Typical idioms for handling the return value are:: # using the variable ax for single a Axes fig, ax = plt.subplots() # using the variable axs for multiple Axes fig, axs = plt.subplots(2, 2) # using tuple unpacking for multiple Axes fig, (ax1, ax2) = plt.subplots(1, 2) fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2) The names ``ax`` and pluralized ``axs`` are preferred over ``axes`` because for the latter it&#39;s not clear if it refers to a single `~.axes.Axes` instance or a collection of these. See Also -- .pyplot.figure .pyplot.subplot .pyplot.axes .Figure.subplots .Figure.add_subplot Examples -- :: # First create some toy data: x = np.linspace(0, 2*np.pi, 400) y = np.sin(x**2) # Create just a figure and only one subplot fig, ax = plt.subplots() ax.plot(x, y) ax.set_title(&#39;Simple plot&#39;) # Create two subplots and unpack the output array immediately f, (ax1, ax2) = plt.subplots(1, 2, sharey=True) ax1.plot(x, y) ax1.set_title(&#39;Sharing Y axis&#39;) ax2.scatter(x, y) # Create four polar axes and access them through the returned array fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=&#34;polar&#34;)) axs[0, 0].plot(x, y) axs[1, 1].scatter(x, y) # Share a X axis with each column of subplots plt.subplots(2, 2, sharex=&#39;col&#39;) # Share a Y axis with each row of subplots plt.subplots(2, 2, sharey=&#39;row&#39;) # Share both X and Y axes with all subplots plt.subplots(2, 2, sharex=&#39;all&#39;, sharey=&#39;all&#39;) # Note that this is the same as plt.subplots(2, 2, sharex=True, sharey=True) # Create figure number 10 with a single subplot # and clears it if it already exists. fig, ax = plt.subplots(num=10, clear=True) &#34;&#34;&#34; fig = figure(**fig_kw) axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey, squeeze=squeeze, subplot_kw=subplot_kw, gridspec_kw=gridspec_kw) return fig, axs File: ~/anaconda3/envs/csy/lib/python3.8/site-packages/matplotlib/pyplot.py Type: function . subplots의 리턴값이 (fig,axs) 이 나오게된다. 우리는 뒤의 axs만 관심이 있으므로 앞의 fig는 _로 처리한다. | . Anscombe&#39;s quartet . - 교과서에 나오는 그림임. . - 교훈: 데이터를 분석하기 전에 항상 시각화를 하라. . x = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5] y1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68] y2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74] y3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73] x4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8] y4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89] . _, axs = plt.subplots(2,2) axs[0,0].plot(x,y1,&#39;o&#39;) axs[0,1].plot(x,y2,&#39;o&#39;) axs[1,0].plot(x,y3,&#39;o&#39;) axs[1,1].plot(x4,y4,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd10a392880&gt;] . - 상관계수를 잠깐 복습해보자. . 상관계수는 -1 ~ 1 사이의 값을 가진다. (코쉬슈바르츠 부등식을 사용하여 증명가능) | 완전한 직선이라면 상관계수가 1 또는 -1이다. | 상관계수가 1에 가까우면 양의 상관관계에 있다고 말하고 -1에 가까우면 음의 상관관계에 있다고 말한다. | . - 의문: 자료의 모양이 직선모양에 가까우면 상관계수가 큰것이 맞나? . $x,y$ 값이 모두 큰 하나의 관측치가 상관계수값을 키울 수 있지 않나? | . - 상관계수가 좋은것은 맞나? (=상관계수는 두 변수의 관계를 설명하기에 충분히 적절한 통계량인가?) . n=len(x) # xtilde = (x-np.mean(x)) / (np.std(x)*np.sqrt(n)) y1tilde = (y1-np.mean(y1)) / (np.std(y1)*np.sqrt(n)) . sum(xtilde*y1tilde) . 0.81642051634484 . np.corrcoef(x,y1) . array([[1. , 0.81642052], [0.81642052, 1. ]]) . np.corrcoef([x,y1,y2,y3]) . array([[1. , 0.81642052, 0.81623651, 0.81628674], [0.81642052, 1. , 0.7500054 , 0.46871668], [0.81623651, 0.7500054 , 1. , 0.58791933], [0.81628674, 0.46871668, 0.58791933, 1. ]]) . np.corrcoef([x4,y4]) . array([[1. , 0.81652144], [0.81652144, 1. ]]) . - 위의 4개의 그림에 대한 상관계수는 모두 같다. (0.81652) . - 상관계수는 두 변수의 관계를 설명하기에 부적절하다. . 상관계수는 1번그림과 같이 두 변수가 선형관계에 있을때 그 정도를 나타내는 통계량일뿐이다. | 선형관계가 아닌것처럼 보이는 자료에서는 상관계수를 계산할수는 있겠으나 의미가 없다. | . - 교훈2: 기본적인 통계량들은 실제자료를 분석하기에 부적절할수 있다. (=통계량은 적절한 가정이 동반되어야 의미가 있다) . . Note: 통계학자는 (1) 적절한 가정을 수학적인 언어로 정의하고 (2) 그 가정하에서 통계량이 의미있다는 것을 증명해야 한다. (3) 그리고 그 결과를 시각화하여 설득한다. . &#49689;&#51228;2 . - 앤스콤의 플랏을 붉은색을 사용하여 그려보기! . _, axs = plt.subplots(2,2) axs[0,0].plot(x,y1,&#39;or&#39;) axs[0,1].plot(x,y2,&#39;or&#39;) axs[1,0].plot(x,y3,&#39;or&#39;) axs[1,1].plot(x4,y4,&#39;or&#39;) . [&lt;matplotlib.lines.Line2D at 0x7fd109e50850&gt;] .",
            "url": "https://seoyeonc.github.io/chch/2021/09/27/%EB%8D%B0%EC%8B%9C_3%EC%A3%BC%EC%B0%A8.html",
            "relUrl": "/2021/09/27/%EB%8D%B0%EC%8B%9C_3%EC%A3%BC%EC%B0%A8.html",
            "date": " • Sep 27, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "데이터 시각화 특강 (2주차) 9월13일",
            "content": "- (1/8): 박스플랏: 전북고예제 (평균은 좋은 측정값인가?) (숫자 무시~~) . - (2/8): 박스플랏 기본개념 . - (3/8): plotly . - (4/8): 히스토그램 . - (5/8): 히스토그램 2개 겹쳐서 비교하기 . - (7/8): 히스토그램 평활화 . - (8/8): 히스토그램 평활화 . import . import matplotlib.pyplot as plt import numpy as np . boxplot . &#51204;&#48513;&#44256;&#50696;&#51228;: &#54217;&#44512;&#51008; &#44316;&#52270;&#51008; &#52769;&#51221;&#44050;&#51064;&#44032;? . - 전북고등학교에서 통계학을 수업하는 두 선생님이 있다. 편의상 A선생님과 B선생님이라고 하자. A선생님이 강의한 반의 통계학 점수는 79.1점이고, B선생님이 강의한 반의 통계학 점수는 78.3점 이라고 하자. . - 의사결정: A선생님에게 배운 학생들의 실력이 평균적으로 좋을 것이다. . y1=[75,75,76,76,77,77,79,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들 y2=[76,76,77,77,78,78,80,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 . np.mean(y1),np.mean(y2) . (79.1, 78.3) . - 평균은 A반(=A선생님에게 통계학을 배운 반)이 더 높다. 그런데 98점을 받은 학생때문에 전체평균이 올라간 것이고, 나머지 학생들은 전체적으로 B반 학생들이 점수가 더 높다고 해석할 수 있다. . - 단순한 평균비교보다 분포를 비교해보는 것이 중요하다. 분포를 살펴보는 방법 중 유용한 방법이 박스플랏이다. . plt.boxplot(y1) . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8ef5e430&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8ef5e7c0&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8ef5eb50&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8ef5eee0&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8ef5e0a0&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8ef672b0&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8ef67640&gt;], &#39;means&#39;: []} . A반의 boxplot | 뚝 떨어진 하나의 점은 98점 | 붉은 선은 중앙값 (평균이 아니라 중앙값) | 나머지 점들은 7~80점에 분포되어있다. | . plt.boxplot(y2) . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8eec0ac0&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8eec0e50&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8eecc220&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8eecc5b0&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8eec0700&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8eecc940&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8eecccd0&gt;], &#39;means&#39;: []} . B반의 boxplot | . - 아래와 같이 하면 박스플랏을 나란히 그릴 수 있다. . plt.boxplot([y1,y2]) # 묶어주는 것 잊지 말기.. . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8eeb07c0&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8eeb0b50&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8ee47130&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8ee474c0&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8eeb0ee0&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8ee3c2b0&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8ee47850&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8ee47be0&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8eeb0430&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8ee3cd60&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8ee3c640&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8ee47f70&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a8ee3c9d0&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a8ee55340&gt;], &#39;means&#39;: []} . - 미적인 그래프는 아니지만 이정도는 괜찮은것 같다. . boxplot&#51060;&#46976;? . - ref: https://github.com/mGalarnyk/Python_Tutorials/blob/master/Statistics/boxplot/box_plot.ipynb . np.random.seed(916170) # connection path is here: https://stackoverflow.com/questions/6146290/plotting-a-line-over-several-graphs mu, sigma = 0, 1 # mean and standard deviation s = np.random.normal(mu, sigma, 1000) fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize=(10, 5)) # rectangular box plot bplot = axes.boxplot(s, vert=False, patch_artist=True, showfliers=True, # This would show outliers (the remaining .7% of the data) positions = [0], boxprops = dict(linestyle=&#39;--&#39;, linewidth=2, color=&#39;Black&#39;, facecolor = &#39;red&#39;, alpha = .4), medianprops = dict(linestyle=&#39;-&#39;, linewidth=2, color=&#39;Yellow&#39;), whiskerprops = dict(linestyle=&#39;-&#39;, linewidth=2, color=&#39;Blue&#39;, alpha = .4), capprops = dict(linestyle=&#39;-&#39;, linewidth=2, color=&#39;Black&#39;), flierprops = dict(marker=&#39;o&#39;, markerfacecolor=&#39;green&#39;, markersize=10, linestyle=&#39;none&#39;, alpha = .4), widths = .3, zorder = 1) axes.set_xlim(-4, 4) plt.xticks(fontsize = 14) axes.set_yticks([]) axes.annotate(r&#39;&#39;, xy=(-.73, .205), xycoords=&#39;data&#39;, xytext=(.66, .205), textcoords=&#39;data&#39;, arrowprops=dict(arrowstyle=&quot;|-|&quot;, connectionstyle=&quot;arc3&quot;) ); axes.text(0, .25, &quot;Interquartile Range n(IQR)&quot;, horizontalalignment=&#39;center&#39;, fontsize=18) axes.text(0, -.21, r&quot;Median&quot;, horizontalalignment=&#39;center&#39;, fontsize=16); axes.text(2.65, -.15, &quot; &quot;Maximum &quot;&quot;, horizontalalignment=&#39;center&#39;, fontsize=18); axes.text(-2.65, -.15, &quot; &quot;Minimum &quot;&quot;, horizontalalignment=&#39;center&#39;, fontsize=18); axes.text(-.68, -.24, r&quot;Q1&quot;, horizontalalignment=&#39;center&#39;, fontsize=18); axes.text(-2.65, -.21, r&quot;(Q1 - 1.5*IQR)&quot;, horizontalalignment=&#39;center&#39;, fontsize=16); axes.text(.6745, -.24, r&quot;Q3&quot;, horizontalalignment=&#39;center&#39;, fontsize=18); axes.text(.6745, -.30, r&quot;(75th Percentile)&quot;, horizontalalignment=&#39;center&#39;, fontsize=12); axes.text(-.68, -.30, r&quot;(25th Percentile)&quot;, horizontalalignment=&#39;center&#39;, fontsize=12); axes.text(2.65, -.21, r&quot;(Q3 + 1.5*IQR)&quot;, horizontalalignment=&#39;center&#39;, fontsize=16); axes.annotate(&#39;Outliers&#39;, xy=(2.93,0.015), xytext=(2.52,0.20), fontsize = 18, arrowprops={&#39;arrowstyle&#39;: &#39;-&gt;&#39;, &#39;color&#39;: &#39;black&#39;, &#39;lw&#39;: 2}, va=&#39;center&#39;); axes.annotate(&#39;Outliers&#39;, xy=(-3.01,0.015), xytext=(-3.41,0.20), fontsize = 18, arrowprops={&#39;arrowstyle&#39;: &#39;-&gt;&#39;, &#39;color&#39;: &#39;black&#39;, &#39;lw&#39;: 2}, va=&#39;center&#39;); fig.tight_layout() . . 이상점은 동 떨어진 점들 | 1사분위부터 3사분위까지 박스로 나타냄! | 1사분위수 - 1.5IQR부터 3사분위수 + 1.5IQR까지 선으로 나타냄 | . plotly . !pip install plotly !pip install ipywidgets !pip install jupyter-dash !pip install dash !pip install pandas . import plotly.express as px import pandas as pd from IPython.display import HTML . A=pd.DataFrame({&#39;score&#39;:y1,&#39;class&#39;:[&#39;A&#39;]*len(y1)}) # score열에는 y1을 입력, class 열에는 A를 y1의 길이만큼 반복 B=pd.DataFrame({&#39;score&#39;:y2,&#39;class&#39;:[&#39;B&#39;]*len(y2)}) # score열에는 y2를 입력, class 열에는 B를 y2의 길이만큼 반복 . df=pd.concat([A,B],ignore_index=True) # ignore_index는 순서를 이어주는, 즉 각 데이터 셋의 순서 무시하기 df . score class . 0 75 | A | . 1 75 | A | . 2 76 | A | . 3 76 | A | . 4 77 | A | . 5 77 | A | . 6 79 | A | . 7 79 | A | . 8 79 | A | . 9 98 | A | . 10 76 | B | . 11 76 | B | . 12 77 | B | . 13 77 | B | . 14 78 | B | . 15 78 | B | . 16 80 | B | . 17 80 | B | . 18 80 | B | . 19 81 | B | . fig=px.box(data_frame=df, x=&#39;class&#39;,y=&#39;score&#39;) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;,include_mathjax=False)) # 이거 해줘야 블로그 같은 곳에 나타남/ . . . histogram . &#55176;&#49828;&#53664;&#44536;&#47016;&#51060;&#46976;? . - X축이 변수의 구간, Y축은 그 구간에 포함된 빈도를 의미하는 그림 . - 예를들면 아래와 같음 . plt.hist(np.random.normal(loc=0, scale=1, size=1000000)) # 평균은 0이고 표준편차는 1인 1000000 사이즈의 랜덤 정규분포 . (array([2.40000e+01, 1.21000e+03, 2.13380e+04, 1.39948e+05, 3.51614e+05, 3.39662e+05, 1.27147e+05, 1.80510e+04, 9.87000e+02, 1.90000e+01]), array([-5.05590169, -4.03792348, -3.01994528, -2.00196708, -0.98398887, 0.03398933, 1.05196753, 2.06994573, 3.08792394, 4.10590214, 5.12388034]), &lt;BarContainer object of 10 artists&gt;) . &#51204;&#48513;&#44256;&#50696;&#51228; . - 중심경향값, 집중경향치 (Measure of central tendency): 분포의 중심성을 나타내기 위한 값, 예시로는 평균, 중앙값. . https://en.wikipedia.org/wiki/Central_tendency | . - &#39;평균이 항상 좋은 중심경향값은 아니다.&#39;라는 사실은 이해했음. . - 하지만 특수한 상황을 가정하면 평균이 좋은 중심경향값임 . np.random.seed(43052) y1=np.random.normal(loc=0,scale=1,size=10000) #전북고 A반의 통계학 성적이라 생각하자. y2=np.random.normal(loc=0.5,scale=1,size=10000) #전북고 B반의 통계학 성적이라 생각하자. . np.mean(y1), np.mean(y2) . (-0.011790879905079434, 0.4979147460611458) . (np.mean(y2)-np.mean(y1)).round(3) . 0.51 . plt.boxplot([y1,y2]) . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a86a419d0&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a86a41d60&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a86a59370&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a86a59700&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a86a4f130&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a86a4f4c0&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a86a59a90&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a86a59e20&gt;], &#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a86a41640&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a86a4ffa0&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a86a4f880&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a86a651f0&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x7f6a86a4fc10&gt;, &lt;matplotlib.lines.Line2D at 0x7f6a86a65580&gt;], &#39;means&#39;: []} . 분포의 모양이 거의 비슷하고, 왼쪽그림을 거의 컨트롤+C,V 오른쪽에 붙인다음 위치조정을 한 느낌 | 이런상황에서는 $B반의 성적 approx A반의 성적 + 0.51$ 라고 주장해도 큰 무리가 없음. | . - 정규분포인것은 어떻게 아는가? $ to$ 히스토그램을 그려보아서 종 모양이 나오는지 살펴보자. . plt.hist(y1,bins=50) # 50개의 구간으로 나누어서 히스토그램을 그릴것 . (array([ 1., 1., 3., 0., 1., 4., 5., 12., 14., 26., 32., 52., 67., 89., 144., 171., 238., 282., 325., 378., 489., 492., 561., 635., 652., 636., 626., 606., 573., 539., 475., 444., 350., 250., 232., 172., 137., 80., 58., 47., 30., 23., 17., 12., 9., 4., 4., 0., 1., 1.]), array([-4.12186916, -3.96068404, -3.79949892, -3.6383138 , -3.47712868, -3.31594356, -3.15475844, -2.99357332, -2.8323882 , -2.67120308, -2.51001796, -2.34883284, -2.18764772, -2.0264626 , -1.86527748, -1.70409236, -1.54290724, -1.38172212, -1.220537 , -1.05935188, -0.89816676, -0.73698164, -0.57579652, -0.4146114 , -0.25342628, -0.09224116, 0.06894396, 0.23012908, 0.3913142 , 0.55249932, 0.71368444, 0.87486956, 1.03605468, 1.1972398 , 1.35842492, 1.51961004, 1.68079516, 1.84198028, 2.0031654 , 2.16435052, 2.32553564, 2.48672076, 2.64790588, 2.809091 , 2.97027612, 3.13146124, 3.29264636, 3.45383148, 3.6150166 , 3.77620172, 3.93738684]), &lt;BarContainer object of 50 artists&gt;) . plt.hist(y2,bins=50) . (array([ 1., 0., 3., 2., 4., 5., 5., 10., 16., 25., 33., 56., 74., 116., 119., 152., 244., 272., 351., 362., 438., 509., 531., 621., 624., 690., 636., 571., 564., 514., 462., 402., 356., 297., 233., 184., 144., 113., 80., 55., 38., 34., 21., 18., 4., 3., 2., 4., 1., 1.]), array([-3.5752867 , -3.4164866 , -3.2576865 , -3.0988864 , -2.9400863 , -2.7812862 , -2.6224861 , -2.463686 , -2.3048859 , -2.1460858 , -1.9872857 , -1.8284856 , -1.6696855 , -1.5108854 , -1.3520853 , -1.1932852 , -1.0344851 , -0.875685 , -0.7168849 , -0.5580848 , -0.3992847 , -0.2404846 , -0.0816845 , 0.0771156 , 0.2359157 , 0.3947158 , 0.5535159 , 0.712316 , 0.87111611, 1.02991621, 1.18871631, 1.34751641, 1.50631651, 1.66511661, 1.82391671, 1.98271681, 2.14151691, 2.30031701, 2.45911711, 2.61791721, 2.77671731, 2.93551741, 3.09431751, 3.25311761, 3.41191771, 3.57071781, 3.72951791, 3.88831801, 4.04711811, 4.20591821, 4.36471831]), &lt;BarContainer object of 50 artists&gt;) . plt.hist([y1,y2],bins=200) # 히스토그램 겹쳐 그리기 . (array([[ 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 2., 1., 0., 1., 1., 3., 4., 4., 2., 2., 6., 4., 1., 4., 7., 8., 9., 11., 5., 9., 9., 14., 12., 16., 11., 9., 18., 25., 30., 22., 18., 28., 29., 39., 40., 41., 37., 42., 48., 56., 58., 49., 80., 62., 62., 91., 78., 75., 82., 89., 81., 106., 85., 89., 126., 125., 106., 142., 141., 121., 121., 135., 154., 166., 146., 125., 169., 160., 170., 172., 162., 161., 161., 193., 146., 186., 170., 166., 197., 152., 149., 167., 173., 158., 155., 156., 153., 152., 137., 151., 147., 126., 141., 125., 139., 117., 116., 135., 118., 93., 115., 99., 78., 91., 77., 63., 81., 52., 83., 53., 61., 49., 46., 46., 47., 45., 26., 48., 31., 27., 27., 20., 17., 22., 15., 15., 14., 14., 15., 10., 8., 13., 7., 5., 8., 6., 6., 6., 2., 4., 9., 3., 3., 6., 2., 1., 4., 2., 2., 2., 2., 0., 1., 1., 2., 2., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 3., 1., 2., 1., 1., 1., 2., 1., 2., 2., 1., 6., 1., 6., 3., 7., 7., 5., 6., 10., 5., 7., 16., 9., 11., 13., 28., 21., 16., 20., 27., 25., 32., 33., 28., 31., 31., 39., 42., 34., 43., 44., 64., 56., 80., 64., 74., 77., 69., 82., 92., 101., 99., 81., 90., 115., 110., 106., 108., 127., 127., 138., 145., 138., 121., 159., 135., 145., 156., 186., 158., 164., 172., 182., 147., 194., 178., 176., 195., 162., 182., 164., 164., 163., 145., 150., 143., 155., 144., 142., 161., 141., 146., 129., 115., 132., 120., 118., 128., 103., 88., 111., 104., 97., 82., 77., 83., 83., 80., 77., 67., 58., 48., 47., 54., 50., 43., 36., 43., 33., 33., 42., 29., 24., 28., 19., 22., 16., 18., 14., 14., 11., 10., 9., 7., 12., 10., 8., 8., 9., 4., 7., 4., 6., 3., 8., 1., 1., 1., 0., 1., 0., 2., 1., 0., 2., 0., 0., 2., 2., 0., 0., 0., 0., 1., 0., 0., 0., 1.]]), array([-4.12186916, -4.07943623, -4.03700329, -3.99457035, -3.95213741, -3.90970448, -3.86727154, -3.8248386 , -3.78240567, -3.73997273, -3.69753979, -3.65510685, -3.61267392, -3.57024098, -3.52780804, -3.4853751 , -3.44294217, -3.40050923, -3.35807629, -3.31564335, -3.27321042, -3.23077748, -3.18834454, -3.1459116 , -3.10347867, -3.06104573, -3.01861279, -2.97617986, -2.93374692, -2.89131398, -2.84888104, -2.80644811, -2.76401517, -2.72158223, -2.67914929, -2.63671636, -2.59428342, -2.55185048, -2.50941754, -2.46698461, -2.42455167, -2.38211873, -2.33968579, -2.29725286, -2.25481992, -2.21238698, -2.16995405, -2.12752111, -2.08508817, -2.04265523, -2.0002223 , -1.95778936, -1.91535642, -1.87292348, -1.83049055, -1.78805761, -1.74562467, -1.70319173, -1.6607588 , -1.61832586, -1.57589292, -1.53345998, -1.49102705, -1.44859411, -1.40616117, -1.36372824, -1.3212953 , -1.27886236, -1.23642942, -1.19399649, -1.15156355, -1.10913061, -1.06669767, -1.02426474, -0.9818318 , -0.93939886, -0.89696592, -0.85453299, -0.81210005, -0.76966711, -0.72723417, -0.68480124, -0.6423683 , -0.59993536, -0.55750243, -0.51506949, -0.47263655, -0.43020361, -0.38777068, -0.34533774, -0.3029048 , -0.26047186, -0.21803893, -0.17560599, -0.13317305, -0.09074011, -0.04830718, -0.00587424, 0.0365587 , 0.07899164, 0.12142457, 0.16385751, 0.20629045, 0.24872338, 0.29115632, 0.33358926, 0.3760222 , 0.41845513, 0.46088807, 0.50332101, 0.54575395, 0.58818688, 0.63061982, 0.67305276, 0.7154857 , 0.75791863, 0.80035157, 0.84278451, 0.88521744, 0.92765038, 0.97008332, 1.01251626, 1.05494919, 1.09738213, 1.13981507, 1.18224801, 1.22468094, 1.26711388, 1.30954682, 1.35197976, 1.39441269, 1.43684563, 1.47927857, 1.52171151, 1.56414444, 1.60657738, 1.64901032, 1.69144325, 1.73387619, 1.77630913, 1.81874207, 1.861175 , 1.90360794, 1.94604088, 1.98847382, 2.03090675, 2.07333969, 2.11577263, 2.15820557, 2.2006385 , 2.24307144, 2.28550438, 2.32793732, 2.37037025, 2.41280319, 2.45523613, 2.49766906, 2.540102 , 2.58253494, 2.62496788, 2.66740081, 2.70983375, 2.75226669, 2.79469963, 2.83713256, 2.8795655 , 2.92199844, 2.96443138, 3.00686431, 3.04929725, 3.09173019, 3.13416313, 3.17659606, 3.219029 , 3.26146194, 3.30389487, 3.34632781, 3.38876075, 3.43119369, 3.47362662, 3.51605956, 3.5584925 , 3.60092544, 3.64335837, 3.68579131, 3.72822425, 3.77065719, 3.81309012, 3.85552306, 3.897956 , 3.94038894, 3.98282187, 4.02525481, 4.06768775, 4.11012068, 4.15255362, 4.19498656, 4.2374195 , 4.27985243, 4.32228537, 4.36471831]), &lt;a list of 2 BarContainer objects&gt;) . seaborn . import seaborn as sns . A=pd.DataFrame({&#39;score&#39;:y1,&#39;class&#39;:[&#39;A&#39;]*len(y1)}) B=pd.DataFrame({&#39;score&#39;:y2,&#39;class&#39;:[&#39;B&#39;]*len(y2)}) df=pd.concat([A,B],ignore_index=True) . sns.histplot(df,x=&#39;score&#39;,hue=&#39;class&#39;) . &lt;AxesSubplot:xlabel=&#39;score&#39;, ylabel=&#39;Count&#39;&gt; . plotnine . from plotnine import * . ggplot(df)+geom_histogram(aes(x=&#39;score&#39;,fill=&#39;class&#39;),position=&#39;identity&#39;,alpha=0.5) . /home/cgb4/anaconda3/envs/csy/lib/python3.8/site-packages/plotnine/stats/stat_bin.py:95: PlotnineWarning: &#39;stat_bin()&#39; using &#39;bins = 84&#39;. Pick better value with &#39;binwidth&#39;. . &lt;ggplot: (8755886288517)&gt; . plotly . - 인터랙티브 그래프를 위해서 plotly 홈페이지를 방문하여 적당한 코드를 가져온다. . import plotly.figure_factory as ff import numpy as np . hist_data=[y1,y2] group_labels=[&#39;A&#39;,&#39;B&#39;] fig = ff.create_distplot(hist_data, group_labels,bin_size=.2, show_rug=False) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;,include_mathjax=False)) . . . [&#49689;&#51228;1] . (1) 자기학번으로 np.random.seed(202043052)를 만들고 . (2) y1, y2 // 10만개의 정규분포를 생성해서 저장 . y1: 평균 0, 표준편차=1 | y2: 평균 1, 표준편차=1 | . (3) plotly 를 활용하여 히스토그램을 겹쳐서 그려보는것. . np.random.seed(202150754) y1=np.random.normal(loc=0,scale=1,size=100000) y2=np.random.normal(loc=1,scale=1,size=100000) plt.hist([y1,y2],bins=200) . (array([[1.000e+00, 0.000e+00, 2.000e+00, 1.000e+00, 0.000e+00, 4.000e+00, 0.000e+00, 1.000e+00, 2.000e+00, 4.000e+00, 2.000e+00, 1.000e+00, 7.000e+00, 1.000e+00, 7.000e+00, 6.000e+00, 7.000e+00, 9.000e+00, 1.200e+01, 4.000e+00, 1.100e+01, 1.500e+01, 1.800e+01, 1.900e+01, 2.900e+01, 3.500e+01, 3.300e+01, 3.700e+01, 5.100e+01, 5.100e+01, 6.800e+01, 6.900e+01, 8.300e+01, 8.400e+01, 8.700e+01, 9.400e+01, 1.140e+02, 1.360e+02, 1.450e+02, 1.580e+02, 1.840e+02, 1.850e+02, 2.180e+02, 2.580e+02, 2.700e+02, 2.850e+02, 2.990e+02, 3.320e+02, 3.800e+02, 4.060e+02, 4.270e+02, 5.110e+02, 4.850e+02, 5.380e+02, 6.160e+02, 6.140e+02, 6.540e+02, 7.110e+02, 8.180e+02, 8.170e+02, 8.610e+02, 9.200e+02, 8.990e+02, 9.870e+02, 1.033e+03, 1.116e+03, 1.152e+03, 1.177e+03, 1.252e+03, 1.302e+03, 1.382e+03, 1.430e+03, 1.494e+03, 1.466e+03, 1.572e+03, 1.596e+03, 1.597e+03, 1.632e+03, 1.671e+03, 1.699e+03, 1.755e+03, 1.820e+03, 1.760e+03, 1.796e+03, 1.834e+03, 1.800e+03, 1.936e+03, 1.901e+03, 1.821e+03, 1.899e+03, 1.803e+03, 1.722e+03, 1.673e+03, 1.756e+03, 1.743e+03, 1.728e+03, 1.697e+03, 1.693e+03, 1.594e+03, 1.506e+03, 1.486e+03, 1.529e+03, 1.438e+03, 1.392e+03, 1.394e+03, 1.304e+03, 1.286e+03, 1.232e+03, 1.129e+03, 1.138e+03, 1.029e+03, 1.005e+03, 9.240e+02, 8.700e+02, 8.540e+02, 8.140e+02, 7.090e+02, 7.270e+02, 6.440e+02, 5.890e+02, 5.790e+02, 5.650e+02, 4.890e+02, 4.300e+02, 4.550e+02, 4.110e+02, 3.190e+02, 3.450e+02, 2.970e+02, 2.880e+02, 2.530e+02, 2.410e+02, 2.000e+02, 1.730e+02, 1.620e+02, 1.540e+02, 1.570e+02, 1.320e+02, 1.120e+02, 9.500e+01, 8.100e+01, 8.600e+01, 8.200e+01, 7.200e+01, 6.700e+01, 4.400e+01, 4.500e+01, 4.500e+01, 2.800e+01, 2.000e+01, 2.400e+01, 2.800e+01, 2.000e+01, 2.500e+01, 8.000e+00, 1.500e+01, 1.000e+01, 7.000e+00, 1.000e+01, 7.000e+00, 3.000e+00, 9.000e+00, 4.000e+00, 3.000e+00, 3.000e+00, 1.000e+00, 4.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 1.000e+00, 0.000e+00, 2.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00], [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 1.000e+00, 0.000e+00, 2.000e+00, 1.000e+00, 1.000e+00, 2.000e+00, 1.000e+00, 1.000e+00, 3.000e+00, 3.000e+00, 2.000e+00, 3.000e+00, 6.000e+00, 3.000e+00, 9.000e+00, 1.000e+01, 1.100e+01, 6.000e+00, 9.000e+00, 1.300e+01, 1.800e+01, 1.500e+01, 1.600e+01, 3.600e+01, 2.300e+01, 2.800e+01, 3.300e+01, 4.300e+01, 6.500e+01, 6.000e+01, 7.000e+01, 7.600e+01, 7.000e+01, 6.600e+01, 9.400e+01, 1.060e+02, 1.300e+02, 1.440e+02, 1.430e+02, 1.630e+02, 1.900e+02, 1.890e+02, 2.410e+02, 2.540e+02, 2.720e+02, 2.990e+02, 3.860e+02, 3.220e+02, 3.970e+02, 4.100e+02, 4.230e+02, 4.590e+02, 5.070e+02, 5.340e+02, 6.230e+02, 6.520e+02, 7.010e+02, 6.890e+02, 7.730e+02, 7.950e+02, 8.880e+02, 9.260e+02, 9.080e+02, 1.016e+03, 1.065e+03, 1.098e+03, 1.203e+03, 1.194e+03, 1.307e+03, 1.287e+03, 1.367e+03, 1.408e+03, 1.528e+03, 1.516e+03, 1.550e+03, 1.615e+03, 1.593e+03, 1.610e+03, 1.730e+03, 1.784e+03, 1.786e+03, 1.804e+03, 1.800e+03, 1.802e+03, 1.748e+03, 1.817e+03, 1.796e+03, 1.862e+03, 1.837e+03, 1.776e+03, 1.794e+03, 1.798e+03, 1.770e+03, 1.723e+03, 1.779e+03, 1.712e+03, 1.688e+03, 1.694e+03, 1.524e+03, 1.563e+03, 1.566e+03, 1.514e+03, 1.405e+03, 1.329e+03, 1.360e+03, 1.286e+03, 1.271e+03, 1.131e+03, 1.146e+03, 1.054e+03, 1.044e+03, 9.620e+02, 9.050e+02, 9.360e+02, 8.110e+02, 8.290e+02, 7.080e+02, 7.530e+02, 6.310e+02, 6.010e+02, 5.700e+02, 5.130e+02, 5.080e+02, 4.610e+02, 4.150e+02, 3.810e+02, 3.520e+02, 3.380e+02, 3.040e+02, 2.550e+02, 2.550e+02, 2.160e+02, 2.090e+02, 1.810e+02, 1.650e+02, 1.430e+02, 1.410e+02, 1.200e+02, 1.080e+02, 8.400e+01, 9.200e+01, 7.800e+01, 7.500e+01, 7.500e+01, 5.500e+01, 4.300e+01, 4.700e+01, 4.200e+01, 4.300e+01, 3.700e+01, 1.900e+01, 3.000e+01, 2.100e+01, 1.800e+01, 1.400e+01, 8.000e+00, 1.400e+01, 1.100e+01, 7.000e+00, 5.000e+00, 7.000e+00, 4.000e+00, 5.000e+00, 2.000e+00, 2.000e+00, 4.000e+00, 3.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 2.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]]), array([-4.03250572, -3.98635668, -3.94020763, -3.89405858, -3.84790954, -3.80176049, -3.75561144, -3.7094624 , -3.66331335, -3.6171643 , -3.57101526, -3.52486621, -3.47871716, -3.43256812, -3.38641907, -3.34027002, -3.29412098, -3.24797193, -3.20182288, -3.15567384, -3.10952479, -3.06337574, -3.0172267 , -2.97107765, -2.92492861, -2.87877956, -2.83263051, -2.78648147, -2.74033242, -2.69418337, -2.64803433, -2.60188528, -2.55573623, -2.50958719, -2.46343814, -2.41728909, -2.37114005, -2.324991 , -2.27884195, -2.23269291, -2.18654386, -2.14039481, -2.09424577, -2.04809672, -2.00194767, -1.95579863, -1.90964958, -1.86350053, -1.81735149, -1.77120244, -1.7250534 , -1.67890435, -1.6327553 , -1.58660626, -1.54045721, -1.49430816, -1.44815912, -1.40201007, -1.35586102, -1.30971198, -1.26356293, -1.21741388, -1.17126484, -1.12511579, -1.07896674, -1.0328177 , -0.98666865, -0.9405196 , -0.89437056, -0.84822151, -0.80207246, -0.75592342, -0.70977437, -0.66362533, -0.61747628, -0.57132723, -0.52517819, -0.47902914, -0.43288009, -0.38673105, -0.340582 , -0.29443295, -0.24828391, -0.20213486, -0.15598581, -0.10983677, -0.06368772, -0.01753867, 0.02861037, 0.07475942, 0.12090847, 0.16705751, 0.21320656, 0.25935561, 0.30550465, 0.3516537 , 0.39780274, 0.44395179, 0.49010084, 0.53624988, 0.58239893, 0.62854798, 0.67469702, 0.72084607, 0.76699512, 0.81314416, 0.85929321, 0.90544226, 0.9515913 , 0.99774035, 1.0438894 , 1.09003844, 1.13618749, 1.18233654, 1.22848558, 1.27463463, 1.32078368, 1.36693272, 1.41308177, 1.45923081, 1.50537986, 1.55152891, 1.59767795, 1.643827 , 1.68997605, 1.73612509, 1.78227414, 1.82842319, 1.87457223, 1.92072128, 1.96687033, 2.01301937, 2.05916842, 2.10531747, 2.15146651, 2.19761556, 2.24376461, 2.28991365, 2.3360627 , 2.38221175, 2.42836079, 2.47450984, 2.52065889, 2.56680793, 2.61295698, 2.65910602, 2.70525507, 2.75140412, 2.79755316, 2.84370221, 2.88985126, 2.9360003 , 2.98214935, 3.0282984 , 3.07444744, 3.12059649, 3.16674554, 3.21289458, 3.25904363, 3.30519268, 3.35134172, 3.39749077, 3.44363982, 3.48978886, 3.53593791, 3.58208696, 3.628236 , 3.67438505, 3.72053409, 3.76668314, 3.81283219, 3.85898123, 3.90513028, 3.95127933, 3.99742837, 4.04357742, 4.08972647, 4.13587551, 4.18202456, 4.22817361, 4.27432265, 4.3204717 , 4.36662075, 4.41276979, 4.45891884, 4.50506789, 4.55121693, 4.59736598, 4.64351503, 4.68966407, 4.73581312, 4.78196216, 4.82811121, 4.87426026, 4.9204093 , 4.96655835, 5.0127074 , 5.05885644, 5.10500549, 5.15115454, 5.19730358]), &lt;a list of 2 BarContainer objects&gt;) . Histogram Equalization, HE . - ref: https://en.wikipedia.org/wiki/Histogram_equalization . - 히스토그램 평활화: 이미지의 명암대비 개선 . !pip install opencv-python . import cv2 as cv import matplotlib.pyplot as plt import pandas as pd . img = cv.imread(&#39;Unequalized_Hawkes_Bay_NZ.jpg&#39;,0) . plt.imshow(img,cmap=&#39;gray&#39;,vmin=0,vmax=255) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7f6a713a2280&gt; . - 이미지자료는 사실 0~255 사이의 어떠한 숫자들이 포함된 매트릭스일 뿐이다. . img . array([[127, 145, 149, ..., 168, 167, 166], [165, 152, 143, ..., 168, 169, 168], [171, 145, 140, ..., 156, 154, 151], ..., [147, 132, 134, ..., 146, 145, 144], [146, 130, 132, ..., 146, 145, 144], [145, 128, 129, ..., 146, 145, 144]], dtype=uint8) . 확인: 이미지가 있다고 믿었던 img는 그냥 넘파이 매트릭스 | 위의 매트릭스에 있는 숫자들을 색깔로 표현하여 값이 클수록 하얗게, 값이 작을수록 검게 그린다. 극단적으로 0은 검은색, 255는 흰색이다. | . - 이미지가 넘파이 매트릭스일 뿐이라는 것을 판다스를 활용하면 더 잘 시각화하여 이해할 수 있다. . plt.imshow(img[200:300,400:500],cmap=&#39;gray&#39;,vmin=0,vmax=255) . &lt;matplotlib.image.AxesImage at 0x7fb294190bb0&gt; . df=pd.DataFrame(img) df.iloc[200:300,400:500].style.set_properties(**{&#39;font-size&#39;:&#39;10pt&#39;}).background_gradient(&#39;gray&#39;,vmin=0,vmax=255) . &nbsp; 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 . 200 155 | 155 | 151 | 149 | 152 | 152 | 151 | 152 | 152 | 152 | 152 | 153 | 154 | 155 | 157 | 158 | 164 | 157 | 150 | 148 | 146 | 143 | 139 | 138 | 138 | 140 | 142 | 140 | 138 | 137 | 137 | 137 | 131 | 142 | 147 | 143 | 143 | 150 | 153 | 151 | 149 | 148 | 146 | 143 | 141 | 141 | 142 | 143 | 143 | 143 | 140 | 138 | 137 | 138 | 136 | 133 | 133 | 134 | 133 | 131 | 128 | 127 | 129 | 131 | 128 | 129 | 132 | 129 | 130 | 129 | 125 | 131 | 128 | 128 | 128 | 130 | 132 | 133 | 132 | 131 | 137 | 136 | 135 | 138 | 140 | 133 | 127 | 129 | 132 | 130 | 130 | 132 | 135 | 135 | 134 | 134 | 137 | 138 | 137 | 134 | . 201 139 | 148 | 152 | 149 | 145 | 142 | 147 | 156 | 150 | 152 | 154 | 154 | 153 | 153 | 155 | 157 | 158 | 159 | 155 | 149 | 144 | 142 | 139 | 136 | 138 | 139 | 140 | 139 | 137 | 136 | 136 | 137 | 141 | 140 | 139 | 141 | 147 | 152 | 152 | 150 | 151 | 150 | 149 | 147 | 145 | 143 | 143 | 143 | 141 | 142 | 142 | 141 | 141 | 141 | 138 | 134 | 133 | 132 | 132 | 131 | 130 | 131 | 133 | 135 | 128 | 128 | 132 | 130 | 132 | 131 | 126 | 132 | 129 | 129 | 128 | 129 | 130 | 132 | 132 | 133 | 137 | 135 | 132 | 133 | 134 | 129 | 126 | 130 | 129 | 129 | 131 | 133 | 134 | 133 | 134 | 136 | 136 | 137 | 137 | 135 | . 202 138 | 145 | 149 | 149 | 149 | 144 | 138 | 139 | 144 | 148 | 153 | 155 | 153 | 152 | 153 | 155 | 153 | 158 | 157 | 147 | 140 | 140 | 139 | 136 | 137 | 135 | 135 | 137 | 137 | 135 | 134 | 136 | 142 | 138 | 139 | 145 | 147 | 144 | 145 | 151 | 150 | 150 | 151 | 150 | 149 | 147 | 145 | 143 | 140 | 140 | 140 | 140 | 140 | 141 | 139 | 135 | 133 | 131 | 129 | 128 | 129 | 130 | 132 | 133 | 128 | 128 | 131 | 131 | 134 | 133 | 128 | 133 | 132 | 132 | 131 | 130 | 129 | 131 | 133 | 135 | 133 | 132 | 129 | 129 | 132 | 131 | 133 | 140 | 135 | 136 | 137 | 137 | 134 | 131 | 131 | 134 | 135 | 136 | 136 | 136 | . 203 152 | 151 | 148 | 150 | 156 | 152 | 141 | 134 | 137 | 141 | 147 | 151 | 152 | 152 | 153 | 154 | 152 | 155 | 153 | 144 | 137 | 138 | 139 | 139 | 134 | 130 | 130 | 134 | 135 | 133 | 133 | 136 | 133 | 132 | 137 | 146 | 147 | 141 | 141 | 147 | 146 | 147 | 148 | 149 | 150 | 149 | 147 | 145 | 143 | 142 | 140 | 137 | 137 | 139 | 139 | 137 | 136 | 133 | 129 | 127 | 127 | 128 | 128 | 128 | 129 | 127 | 130 | 129 | 133 | 134 | 130 | 136 | 136 | 136 | 134 | 132 | 130 | 131 | 133 | 135 | 132 | 133 | 132 | 133 | 137 | 137 | 140 | 147 | 144 | 143 | 142 | 140 | 137 | 133 | 134 | 136 | 136 | 136 | 135 | 135 | . 204 155 | 156 | 153 | 151 | 153 | 153 | 152 | 155 | 141 | 141 | 142 | 144 | 147 | 149 | 151 | 151 | 153 | 152 | 149 | 145 | 140 | 137 | 138 | 140 | 133 | 130 | 129 | 131 | 132 | 131 | 132 | 134 | 130 | 127 | 130 | 139 | 147 | 147 | 144 | 141 | 144 | 144 | 144 | 146 | 147 | 147 | 147 | 146 | 147 | 146 | 143 | 140 | 139 | 140 | 139 | 137 | 138 | 134 | 130 | 127 | 128 | 129 | 129 | 128 | 129 | 127 | 129 | 127 | 131 | 134 | 131 | 139 | 139 | 138 | 136 | 133 | 131 | 130 | 132 | 134 | 137 | 140 | 141 | 141 | 143 | 140 | 139 | 143 | 145 | 141 | 138 | 139 | 140 | 141 | 142 | 144 | 138 | 136 | 134 | 133 | . 205 145 | 151 | 152 | 153 | 154 | 152 | 152 | 156 | 151 | 147 | 141 | 139 | 141 | 144 | 147 | 148 | 151 | 149 | 149 | 148 | 145 | 138 | 135 | 136 | 134 | 133 | 131 | 130 | 130 | 130 | 131 | 132 | 134 | 133 | 134 | 136 | 139 | 142 | 143 | 143 | 145 | 144 | 142 | 143 | 144 | 145 | 144 | 144 | 146 | 146 | 145 | 143 | 141 | 140 | 137 | 133 | 134 | 131 | 127 | 126 | 128 | 129 | 130 | 129 | 129 | 128 | 130 | 127 | 130 | 133 | 132 | 141 | 140 | 138 | 136 | 133 | 130 | 130 | 132 | 133 | 139 | 143 | 143 | 143 | 145 | 141 | 137 | 139 | 144 | 138 | 134 | 136 | 141 | 144 | 145 | 145 | 141 | 137 | 134 | 134 | . 206 143 | 147 | 149 | 151 | 155 | 152 | 146 | 145 | 154 | 151 | 146 | 143 | 142 | 143 | 144 | 145 | 144 | 146 | 147 | 146 | 142 | 138 | 136 | 134 | 133 | 135 | 134 | 130 | 129 | 132 | 134 | 134 | 135 | 136 | 137 | 136 | 133 | 135 | 142 | 150 | 147 | 145 | 143 | 143 | 144 | 145 | 144 | 143 | 145 | 145 | 144 | 142 | 141 | 140 | 136 | 132 | 130 | 128 | 126 | 126 | 127 | 128 | 129 | 129 | 129 | 129 | 131 | 129 | 130 | 133 | 133 | 143 | 143 | 139 | 135 | 132 | 131 | 132 | 134 | 136 | 138 | 141 | 140 | 139 | 142 | 141 | 139 | 140 | 143 | 137 | 133 | 136 | 141 | 143 | 143 | 142 | 143 | 139 | 136 | 136 | . 207 148 | 151 | 150 | 149 | 151 | 150 | 149 | 151 | 150 | 150 | 151 | 150 | 148 | 145 | 144 | 144 | 137 | 144 | 145 | 139 | 135 | 137 | 138 | 135 | 131 | 136 | 136 | 130 | 129 | 135 | 138 | 137 | 134 | 129 | 128 | 132 | 137 | 140 | 146 | 153 | 147 | 145 | 144 | 144 | 145 | 146 | 145 | 143 | 147 | 147 | 144 | 142 | 141 | 141 | 139 | 136 | 132 | 131 | 129 | 128 | 128 | 128 | 128 | 129 | 128 | 130 | 133 | 131 | 131 | 133 | 133 | 144 | 145 | 141 | 135 | 132 | 132 | 135 | 138 | 140 | 141 | 141 | 137 | 135 | 139 | 140 | 139 | 140 | 139 | 135 | 133 | 136 | 141 | 143 | 143 | 143 | 144 | 140 | 137 | 138 | . 208 150 | 148 | 151 | 145 | 143 | 147 | 148 | 153 | 158 | 153 | 150 | 152 | 149 | 143 | 141 | 144 | 145 | 145 | 144 | 142 | 140 | 137 | 133 | 131 | 130 | 133 | 131 | 130 | 137 | 143 | 140 | 135 | 141 | 139 | 130 | 134 | 147 | 142 | 135 | 144 | 147 | 143 | 146 | 148 | 144 | 146 | 149 | 144 | 148 | 144 | 140 | 141 | 143 | 144 | 141 | 137 | 132 | 128 | 126 | 127 | 128 | 127 | 125 | 125 | 128 | 127 | 129 | 135 | 138 | 138 | 138 | 139 | 138 | 133 | 129 | 130 | 133 | 135 | 138 | 140 | 140 | 136 | 135 | 137 | 138 | 136 | 136 | 139 | 139 | 137 | 137 | 138 | 139 | 140 | 141 | 143 | 141 | 144 | 143 | 143 | . 209 153 | 147 | 147 | 143 | 144 | 147 | 144 | 146 | 150 | 149 | 150 | 152 | 153 | 151 | 146 | 143 | 142 | 142 | 141 | 139 | 137 | 135 | 134 | 134 | 133 | 131 | 130 | 129 | 130 | 132 | 134 | 133 | 136 | 139 | 137 | 130 | 132 | 146 | 151 | 142 | 145 | 146 | 153 | 156 | 154 | 156 | 156 | 149 | 145 | 148 | 149 | 148 | 143 | 140 | 139 | 140 | 143 | 137 | 130 | 125 | 122 | 122 | 126 | 129 | 128 | 128 | 129 | 132 | 137 | 139 | 139 | 137 | 133 | 130 | 129 | 133 | 138 | 139 | 137 | 135 | 140 | 136 | 134 | 136 | 136 | 133 | 132 | 135 | 137 | 136 | 136 | 137 | 136 | 136 | 137 | 140 | 137 | 140 | 139 | 139 | . 210 154 | 147 | 147 | 145 | 145 | 145 | 141 | 145 | 143 | 145 | 147 | 149 | 153 | 155 | 151 | 144 | 144 | 143 | 141 | 139 | 137 | 136 | 137 | 137 | 136 | 130 | 132 | 135 | 129 | 129 | 133 | 133 | 137 | 128 | 138 | 144 | 135 | 141 | 149 | 139 | 141 | 142 | 146 | 146 | 145 | 152 | 155 | 150 | 145 | 145 | 144 | 144 | 144 | 143 | 141 | 140 | 142 | 139 | 135 | 132 | 127 | 124 | 125 | 127 | 126 | 126 | 127 | 128 | 133 | 137 | 136 | 133 | 132 | 130 | 131 | 137 | 143 | 142 | 138 | 134 | 138 | 135 | 134 | 136 | 136 | 134 | 134 | 135 | 135 | 135 | 136 | 137 | 135 | 134 | 136 | 140 | 137 | 140 | 139 | 138 | . 211 153 | 149 | 152 | 151 | 148 | 146 | 143 | 150 | 143 | 143 | 143 | 145 | 148 | 150 | 150 | 148 | 149 | 146 | 142 | 140 | 139 | 139 | 138 | 137 | 137 | 131 | 137 | 143 | 137 | 139 | 141 | 134 | 145 | 129 | 145 | 167 | 157 | 144 | 144 | 140 | 139 | 142 | 143 | 140 | 141 | 149 | 153 | 151 | 148 | 145 | 141 | 141 | 142 | 143 | 142 | 141 | 139 | 138 | 139 | 139 | 135 | 129 | 124 | 122 | 125 | 124 | 124 | 127 | 130 | 132 | 133 | 134 | 133 | 131 | 132 | 137 | 140 | 140 | 137 | 135 | 138 | 135 | 133 | 134 | 134 | 133 | 134 | 135 | 133 | 134 | 136 | 136 | 135 | 134 | 136 | 140 | 139 | 141 | 139 | 139 | . 212 156 | 151 | 153 | 151 | 150 | 149 | 145 | 149 | 147 | 144 | 144 | 146 | 146 | 143 | 144 | 149 | 149 | 144 | 139 | 138 | 139 | 139 | 136 | 132 | 136 | 132 | 138 | 142 | 141 | 147 | 147 | 132 | 137 | 136 | 145 | 155 | 154 | 147 | 141 | 135 | 133 | 140 | 142 | 141 | 144 | 148 | 148 | 147 | 149 | 151 | 152 | 149 | 143 | 140 | 141 | 144 | 146 | 142 | 140 | 139 | 137 | 133 | 129 | 127 | 127 | 123 | 124 | 128 | 130 | 127 | 130 | 135 | 130 | 131 | 133 | 136 | 136 | 134 | 134 | 134 | 138 | 135 | 132 | 131 | 131 | 131 | 131 | 133 | 131 | 132 | 134 | 135 | 133 | 131 | 134 | 137 | 135 | 139 | 137 | 136 | . 213 160 | 152 | 151 | 149 | 151 | 153 | 145 | 142 | 148 | 145 | 146 | 150 | 148 | 141 | 141 | 147 | 146 | 142 | 137 | 137 | 139 | 139 | 135 | 131 | 134 | 132 | 134 | 134 | 136 | 144 | 143 | 131 | 128 | 132 | 130 | 129 | 134 | 138 | 137 | 137 | 132 | 138 | 138 | 137 | 141 | 143 | 144 | 147 | 148 | 150 | 151 | 152 | 150 | 148 | 145 | 144 | 145 | 142 | 140 | 142 | 143 | 141 | 139 | 137 | 129 | 125 | 123 | 126 | 127 | 125 | 127 | 131 | 129 | 131 | 135 | 138 | 137 | 134 | 133 | 134 | 135 | 133 | 132 | 132 | 133 | 135 | 136 | 138 | 135 | 135 | 136 | 136 | 134 | 132 | 132 | 135 | 133 | 137 | 136 | 134 | . 214 160 | 157 | 157 | 150 | 150 | 153 | 146 | 143 | 146 | 146 | 147 | 149 | 147 | 144 | 143 | 144 | 144 | 142 | 138 | 137 | 138 | 137 | 136 | 134 | 132 | 133 | 133 | 132 | 132 | 135 | 136 | 133 | 131 | 130 | 129 | 131 | 133 | 135 | 139 | 146 | 140 | 142 | 138 | 137 | 141 | 141 | 142 | 152 | 147 | 144 | 143 | 147 | 153 | 155 | 151 | 147 | 142 | 142 | 143 | 146 | 148 | 146 | 143 | 142 | 134 | 132 | 127 | 124 | 126 | 129 | 128 | 124 | 130 | 131 | 134 | 137 | 137 | 134 | 133 | 134 | 135 | 134 | 134 | 135 | 137 | 140 | 141 | 142 | 138 | 137 | 138 | 138 | 136 | 134 | 133 | 135 | 134 | 140 | 139 | 135 | . 215 157 | 162 | 167 | 156 | 148 | 151 | 149 | 151 | 145 | 145 | 145 | 143 | 144 | 146 | 146 | 143 | 143 | 142 | 139 | 137 | 135 | 134 | 134 | 135 | 130 | 134 | 136 | 136 | 134 | 130 | 131 | 138 | 128 | 130 | 138 | 141 | 136 | 138 | 143 | 140 | 137 | 140 | 138 | 140 | 143 | 137 | 134 | 143 | 147 | 147 | 146 | 147 | 148 | 150 | 153 | 155 | 149 | 147 | 146 | 146 | 143 | 141 | 141 | 143 | 141 | 141 | 133 | 124 | 128 | 137 | 134 | 122 | 129 | 128 | 129 | 131 | 132 | 132 | 132 | 133 | 138 | 138 | 138 | 138 | 138 | 139 | 138 | 137 | 137 | 136 | 136 | 137 | 136 | 134 | 133 | 134 | 136 | 142 | 141 | 136 | . 216 156 | 146 | 149 | 163 | 166 | 153 | 146 | 152 | 145 | 144 | 143 | 144 | 145 | 145 | 143 | 141 | 139 | 141 | 136 | 133 | 138 | 136 | 130 | 131 | 131 | 132 | 136 | 137 | 134 | 130 | 130 | 132 | 132 | 135 | 126 | 136 | 141 | 137 | 146 | 141 | 135 | 137 | 138 | 141 | 143 | 138 | 137 | 143 | 146 | 146 | 146 | 148 | 149 | 145 | 143 | 148 | 147 | 147 | 146 | 144 | 142 | 141 | 140 | 140 | 142 | 143 | 143 | 144 | 144 | 143 | 140 | 138 | 123 | 126 | 137 | 137 | 141 | 131 | 133 | 132 | 138 | 141 | 137 | 135 | 138 | 135 | 132 | 137 | 137 | 135 | 135 | 137 | 136 | 132 | 131 | 134 | 134 | 137 | 139 | 137 | . 217 155 | 156 | 158 | 158 | 159 | 158 | 154 | 148 | 150 | 146 | 142 | 143 | 145 | 145 | 142 | 138 | 136 | 135 | 131 | 132 | 137 | 138 | 135 | 133 | 136 | 131 | 129 | 130 | 132 | 131 | 128 | 127 | 143 | 144 | 130 | 132 | 134 | 132 | 144 | 141 | 134 | 138 | 138 | 136 | 137 | 136 | 135 | 138 | 132 | 144 | 151 | 146 | 144 | 150 | 152 | 147 | 142 | 143 | 143 | 143 | 143 | 142 | 142 | 142 | 144 | 144 | 144 | 145 | 146 | 146 | 146 | 145 | 136 | 133 | 132 | 135 | 136 | 136 | 133 | 133 | 133 | 134 | 133 | 135 | 139 | 138 | 135 | 135 | 134 | 133 | 134 | 136 | 134 | 129 | 129 | 132 | 133 | 134 | 135 | 135 | . 218 153 | 161 | 162 | 156 | 157 | 164 | 164 | 156 | 152 | 150 | 148 | 145 | 142 | 141 | 141 | 141 | 141 | 135 | 132 | 133 | 135 | 136 | 135 | 130 | 135 | 130 | 127 | 128 | 130 | 129 | 127 | 124 | 139 | 145 | 136 | 135 | 134 | 132 | 139 | 132 | 132 | 139 | 137 | 132 | 134 | 136 | 137 | 137 | 135 | 141 | 154 | 158 | 153 | 152 | 151 | 144 | 142 | 142 | 143 | 142 | 142 | 140 | 139 | 138 | 144 | 143 | 142 | 142 | 142 | 143 | 144 | 145 | 144 | 138 | 127 | 134 | 132 | 138 | 129 | 127 | 131 | 127 | 129 | 135 | 138 | 137 | 135 | 130 | 130 | 131 | 132 | 134 | 131 | 127 | 127 | 131 | 134 | 134 | 134 | 135 | . 219 153 | 155 | 157 | 158 | 159 | 161 | 164 | 165 | 157 | 155 | 151 | 147 | 144 | 142 | 142 | 142 | 148 | 139 | 137 | 136 | 132 | 131 | 132 | 126 | 129 | 130 | 131 | 131 | 130 | 128 | 126 | 126 | 127 | 137 | 136 | 136 | 135 | 134 | 136 | 127 | 131 | 136 | 136 | 134 | 137 | 139 | 138 | 140 | 133 | 133 | 146 | 155 | 150 | 150 | 152 | 145 | 143 | 142 | 141 | 140 | 139 | 138 | 136 | 135 | 142 | 141 | 140 | 138 | 137 | 137 | 137 | 138 | 152 | 147 | 139 | 143 | 142 | 144 | 135 | 130 | 131 | 125 | 128 | 135 | 134 | 134 | 134 | 129 | 131 | 132 | 134 | 134 | 131 | 128 | 130 | 135 | 133 | 133 | 134 | 136 | . 220 151 | 151 | 153 | 157 | 157 | 156 | 158 | 162 | 164 | 159 | 153 | 150 | 150 | 148 | 144 | 140 | 145 | 137 | 137 | 137 | 130 | 129 | 131 | 129 | 127 | 129 | 131 | 131 | 130 | 129 | 128 | 128 | 125 | 134 | 133 | 130 | 129 | 132 | 136 | 130 | 130 | 133 | 134 | 137 | 140 | 136 | 134 | 137 | 137 | 146 | 156 | 151 | 140 | 147 | 154 | 144 | 141 | 139 | 138 | 137 | 137 | 137 | 137 | 136 | 138 | 138 | 139 | 138 | 137 | 136 | 136 | 137 | 147 | 146 | 149 | 145 | 148 | 142 | 141 | 136 | 136 | 130 | 133 | 138 | 133 | 131 | 134 | 133 | 133 | 135 | 136 | 136 | 133 | 132 | 135 | 140 | 131 | 132 | 133 | 134 | . 221 147 | 151 | 154 | 154 | 155 | 158 | 160 | 159 | 161 | 164 | 164 | 160 | 152 | 146 | 144 | 145 | 142 | 137 | 137 | 136 | 130 | 127 | 129 | 131 | 131 | 129 | 127 | 126 | 128 | 130 | 130 | 127 | 126 | 132 | 133 | 129 | 128 | 132 | 134 | 130 | 131 | 131 | 131 | 135 | 137 | 131 | 127 | 131 | 136 | 148 | 162 | 161 | 146 | 141 | 144 | 142 | 142 | 140 | 136 | 135 | 135 | 135 | 134 | 134 | 131 | 134 | 137 | 138 | 138 | 138 | 139 | 139 | 139 | 140 | 147 | 140 | 143 | 137 | 142 | 141 | 147 | 143 | 144 | 144 | 136 | 129 | 128 | 131 | 131 | 133 | 135 | 135 | 134 | 134 | 137 | 140 | 134 | 134 | 134 | 132 | . 222 146 | 151 | 153 | 152 | 154 | 159 | 160 | 158 | 156 | 162 | 168 | 166 | 157 | 150 | 148 | 150 | 145 | 142 | 138 | 136 | 133 | 127 | 125 | 129 | 131 | 131 | 129 | 126 | 126 | 128 | 128 | 127 | 125 | 129 | 134 | 132 | 132 | 134 | 130 | 127 | 130 | 132 | 131 | 131 | 133 | 132 | 129 | 131 | 130 | 133 | 144 | 155 | 152 | 140 | 138 | 145 | 143 | 140 | 137 | 135 | 134 | 134 | 132 | 130 | 129 | 132 | 137 | 139 | 139 | 140 | 140 | 141 | 141 | 144 | 147 | 143 | 142 | 142 | 145 | 147 | 149 | 149 | 149 | 148 | 142 | 130 | 124 | 129 | 129 | 132 | 135 | 137 | 138 | 138 | 139 | 140 | 138 | 136 | 133 | 131 | . 223 151 | 149 | 149 | 151 | 152 | 151 | 152 | 153 | 159 | 157 | 157 | 162 | 167 | 165 | 155 | 145 | 147 | 146 | 139 | 136 | 137 | 130 | 124 | 130 | 128 | 134 | 136 | 132 | 126 | 124 | 126 | 127 | 125 | 126 | 131 | 129 | 131 | 134 | 131 | 130 | 129 | 133 | 131 | 128 | 132 | 136 | 136 | 136 | 154 | 150 | 143 | 141 | 145 | 146 | 142 | 140 | 138 | 136 | 135 | 134 | 135 | 136 | 134 | 133 | 133 | 136 | 140 | 142 | 141 | 141 | 142 | 142 | 141 | 146 | 143 | 145 | 138 | 144 | 141 | 143 | 140 | 144 | 145 | 148 | 147 | 135 | 127 | 133 | 131 | 134 | 139 | 142 | 144 | 145 | 144 | 142 | 139 | 135 | 130 | 129 | . 224 152 | 151 | 150 | 150 | 150 | 151 | 152 | 152 | 151 | 155 | 160 | 161 | 161 | 161 | 164 | 167 | 146 | 149 | 145 | 136 | 131 | 132 | 131 | 126 | 128 | 126 | 126 | 128 | 130 | 130 | 127 | 124 | 125 | 126 | 127 | 128 | 130 | 133 | 133 | 131 | 128 | 129 | 130 | 133 | 135 | 132 | 131 | 137 | 144 | 143 | 151 | 151 | 146 | 139 | 133 | 137 | 137 | 135 | 136 | 136 | 132 | 131 | 134 | 133 | 133 | 138 | 140 | 139 | 138 | 134 | 133 | 139 | 141 | 142 | 143 | 144 | 142 | 140 | 139 | 141 | 139 | 146 | 146 | 140 | 139 | 138 | 132 | 127 | 129 | 137 | 145 | 145 | 144 | 145 | 147 | 149 | 141 | 127 | 124 | 130 | . 225 151 | 149 | 147 | 146 | 146 | 147 | 149 | 149 | 151 | 153 | 155 | 156 | 157 | 159 | 161 | 164 | 162 | 151 | 144 | 143 | 140 | 132 | 128 | 129 | 132 | 129 | 127 | 127 | 128 | 129 | 128 | 127 | 126 | 128 | 129 | 130 | 131 | 132 | 130 | 127 | 131 | 131 | 130 | 132 | 134 | 130 | 129 | 134 | 130 | 134 | 146 | 147 | 143 | 138 | 132 | 136 | 133 | 132 | 134 | 135 | 130 | 130 | 132 | 130 | 133 | 136 | 136 | 136 | 138 | 136 | 136 | 142 | 144 | 143 | 143 | 143 | 141 | 139 | 138 | 139 | 142 | 143 | 141 | 141 | 142 | 136 | 128 | 127 | 137 | 145 | 148 | 143 | 141 | 143 | 144 | 140 | 141 | 132 | 128 | 128 | . 226 148 | 147 | 146 | 146 | 148 | 150 | 151 | 151 | 152 | 151 | 150 | 151 | 153 | 156 | 159 | 160 | 164 | 162 | 155 | 147 | 142 | 140 | 135 | 130 | 125 | 125 | 126 | 127 | 129 | 129 | 128 | 127 | 131 | 132 | 131 | 131 | 132 | 132 | 130 | 127 | 132 | 132 | 129 | 129 | 131 | 128 | 127 | 132 | 131 | 138 | 149 | 146 | 140 | 137 | 132 | 133 | 131 | 131 | 134 | 135 | 131 | 131 | 132 | 130 | 131 | 134 | 133 | 133 | 135 | 135 | 139 | 147 | 147 | 144 | 142 | 142 | 141 | 138 | 137 | 138 | 144 | 141 | 138 | 140 | 139 | 130 | 127 | 135 | 139 | 146 | 146 | 139 | 138 | 144 | 143 | 136 | 141 | 137 | 132 | 126 | . 227 148 | 149 | 149 | 150 | 151 | 151 | 150 | 149 | 150 | 150 | 150 | 151 | 153 | 155 | 156 | 157 | 157 | 165 | 166 | 156 | 148 | 146 | 143 | 137 | 126 | 126 | 126 | 126 | 126 | 126 | 127 | 128 | 133 | 133 | 131 | 129 | 130 | 131 | 131 | 129 | 129 | 130 | 128 | 127 | 128 | 126 | 128 | 134 | 144 | 148 | 153 | 145 | 139 | 140 | 136 | 135 | 133 | 132 | 135 | 136 | 132 | 132 | 134 | 132 | 129 | 132 | 133 | 132 | 133 | 133 | 137 | 147 | 147 | 144 | 140 | 140 | 139 | 138 | 136 | 136 | 143 | 141 | 139 | 137 | 132 | 125 | 131 | 147 | 142 | 144 | 142 | 138 | 140 | 145 | 145 | 140 | 141 | 139 | 135 | 130 | . 228 153 | 152 | 150 | 149 | 148 | 146 | 145 | 143 | 147 | 149 | 151 | 153 | 153 | 153 | 154 | 155 | 154 | 157 | 163 | 166 | 160 | 150 | 146 | 148 | 140 | 136 | 130 | 125 | 122 | 124 | 127 | 130 | 130 | 130 | 129 | 129 | 130 | 131 | 130 | 128 | 126 | 129 | 129 | 127 | 127 | 127 | 132 | 140 | 146 | 147 | 150 | 143 | 142 | 147 | 142 | 139 | 134 | 133 | 135 | 135 | 131 | 131 | 133 | 132 | 131 | 131 | 130 | 131 | 136 | 136 | 135 | 138 | 143 | 139 | 137 | 137 | 137 | 136 | 135 | 135 | 140 | 141 | 138 | 134 | 132 | 131 | 138 | 150 | 148 | 144 | 141 | 142 | 144 | 143 | 142 | 142 | 143 | 140 | 141 | 141 | . 229 155 | 152 | 149 | 147 | 146 | 146 | 147 | 147 | 144 | 146 | 149 | 151 | 150 | 150 | 151 | 153 | 153 | 155 | 158 | 162 | 163 | 161 | 154 | 149 | 141 | 138 | 134 | 131 | 129 | 128 | 127 | 127 | 127 | 129 | 131 | 133 | 134 | 135 | 132 | 128 | 127 | 131 | 130 | 128 | 128 | 130 | 134 | 143 | 142 | 144 | 148 | 146 | 148 | 151 | 143 | 137 | 132 | 131 | 132 | 132 | 128 | 128 | 131 | 130 | 135 | 132 | 128 | 132 | 142 | 142 | 134 | 130 | 137 | 135 | 135 | 136 | 136 | 135 | 134 | 135 | 137 | 138 | 134 | 132 | 136 | 140 | 140 | 143 | 145 | 140 | 139 | 143 | 143 | 138 | 137 | 140 | 143 | 141 | 144 | 148 | . 230 154 | 153 | 151 | 149 | 149 | 149 | 149 | 149 | 145 | 145 | 145 | 145 | 147 | 149 | 151 | 152 | 151 | 157 | 157 | 153 | 157 | 166 | 163 | 154 | 141 | 139 | 137 | 136 | 136 | 134 | 131 | 128 | 129 | 131 | 132 | 133 | 134 | 135 | 133 | 129 | 131 | 132 | 129 | 126 | 128 | 130 | 132 | 137 | 139 | 140 | 147 | 147 | 148 | 148 | 138 | 133 | 131 | 130 | 132 | 132 | 128 | 129 | 131 | 129 | 134 | 134 | 134 | 139 | 145 | 142 | 134 | 132 | 135 | 135 | 137 | 139 | 139 | 137 | 137 | 138 | 134 | 136 | 134 | 133 | 137 | 138 | 138 | 140 | 138 | 137 | 139 | 141 | 140 | 136 | 134 | 136 | 138 | 138 | 142 | 144 | . 231 156 | 155 | 155 | 154 | 152 | 148 | 145 | 142 | 148 | 144 | 141 | 141 | 145 | 149 | 152 | 153 | 155 | 153 | 152 | 154 | 156 | 158 | 163 | 168 | 154 | 147 | 139 | 135 | 134 | 136 | 136 | 136 | 130 | 130 | 128 | 127 | 129 | 131 | 131 | 129 | 134 | 133 | 127 | 123 | 127 | 128 | 128 | 130 | 135 | 135 | 143 | 143 | 143 | 142 | 133 | 130 | 131 | 130 | 134 | 135 | 131 | 131 | 133 | 131 | 129 | 137 | 144 | 146 | 144 | 138 | 135 | 140 | 136 | 137 | 141 | 143 | 143 | 140 | 140 | 141 | 131 | 136 | 136 | 135 | 132 | 130 | 133 | 143 | 139 | 143 | 145 | 143 | 139 | 135 | 133 | 131 | 131 | 134 | 138 | 137 | . 232 156 | 156 | 155 | 153 | 153 | 152 | 150 | 146 | 147 | 146 | 146 | 147 | 147 | 147 | 146 | 145 | 150 | 152 | 153 | 153 | 154 | 156 | 159 | 161 | 170 | 158 | 142 | 133 | 134 | 137 | 136 | 132 | 132 | 134 | 135 | 132 | 129 | 127 | 129 | 131 | 136 | 135 | 134 | 132 | 132 | 133 | 134 | 135 | 135 | 135 | 136 | 139 | 141 | 139 | 136 | 135 | 134 | 131 | 129 | 134 | 140 | 136 | 130 | 132 | 132 | 135 | 142 | 144 | 141 | 143 | 146 | 143 | 147 | 138 | 138 | 142 | 140 | 136 | 135 | 135 | 132 | 129 | 129 | 131 | 131 | 130 | 134 | 140 | 143 | 139 | 136 | 137 | 137 | 135 | 132 | 132 | 133 | 134 | 134 | 133 | . 233 153 | 157 | 160 | 158 | 153 | 151 | 150 | 149 | 146 | 148 | 149 | 149 | 147 | 145 | 143 | 142 | 144 | 146 | 148 | 150 | 151 | 154 | 156 | 158 | 163 | 165 | 161 | 150 | 139 | 134 | 136 | 139 | 137 | 136 | 135 | 136 | 135 | 133 | 129 | 126 | 129 | 132 | 135 | 139 | 142 | 143 | 144 | 143 | 144 | 141 | 139 | 137 | 134 | 131 | 131 | 133 | 130 | 137 | 140 | 138 | 132 | 126 | 130 | 142 | 131 | 130 | 134 | 139 | 137 | 137 | 140 | 141 | 140 | 134 | 136 | 141 | 140 | 139 | 138 | 137 | 133 | 130 | 129 | 130 | 129 | 127 | 130 | 135 | 141 | 140 | 141 | 142 | 140 | 136 | 134 | 134 | 129 | 130 | 132 | 133 | . 234 157 | 155 | 154 | 153 | 155 | 156 | 152 | 147 | 144 | 147 | 149 | 148 | 145 | 144 | 144 | 145 | 144 | 145 | 146 | 149 | 152 | 154 | 155 | 155 | 156 | 161 | 166 | 163 | 156 | 148 | 141 | 136 | 137 | 135 | 135 | 136 | 138 | 137 | 134 | 131 | 128 | 128 | 129 | 130 | 133 | 135 | 138 | 140 | 146 | 146 | 148 | 148 | 145 | 141 | 140 | 141 | 131 | 137 | 139 | 137 | 134 | 130 | 130 | 136 | 137 | 131 | 134 | 140 | 139 | 136 | 140 | 144 | 143 | 139 | 139 | 138 | 135 | 135 | 134 | 130 | 132 | 130 | 129 | 131 | 130 | 129 | 131 | 135 | 135 | 138 | 141 | 142 | 138 | 133 | 132 | 134 | 132 | 133 | 133 | 133 | . 235 159 | 156 | 151 | 151 | 155 | 158 | 155 | 150 | 148 | 149 | 148 | 146 | 144 | 144 | 145 | 147 | 147 | 147 | 146 | 148 | 151 | 153 | 153 | 153 | 154 | 155 | 157 | 161 | 165 | 163 | 153 | 143 | 140 | 139 | 137 | 136 | 135 | 136 | 138 | 139 | 137 | 135 | 132 | 130 | 130 | 131 | 133 | 135 | 135 | 136 | 139 | 142 | 143 | 141 | 141 | 142 | 137 | 136 | 131 | 129 | 135 | 136 | 132 | 129 | 134 | 129 | 130 | 135 | 133 | 132 | 135 | 137 | 140 | 138 | 138 | 135 | 133 | 138 | 141 | 137 | 131 | 129 | 129 | 130 | 130 | 129 | 130 | 132 | 132 | 135 | 138 | 137 | 133 | 130 | 131 | 133 | 137 | 137 | 135 | 134 | . 236 155 | 159 | 161 | 159 | 155 | 156 | 158 | 160 | 156 | 155 | 153 | 150 | 148 | 147 | 145 | 144 | 144 | 143 | 142 | 143 | 145 | 148 | 150 | 150 | 155 | 156 | 156 | 155 | 157 | 160 | 163 | 164 | 154 | 149 | 143 | 139 | 139 | 140 | 141 | 141 | 138 | 139 | 139 | 139 | 137 | 135 | 133 | 132 | 134 | 129 | 125 | 126 | 128 | 130 | 132 | 134 | 137 | 137 | 131 | 126 | 128 | 131 | 133 | 136 | 130 | 129 | 132 | 132 | 129 | 130 | 132 | 129 | 139 | 139 | 139 | 137 | 135 | 140 | 142 | 137 | 131 | 129 | 128 | 128 | 127 | 125 | 125 | 125 | 136 | 137 | 135 | 133 | 131 | 131 | 132 | 133 | 133 | 134 | 135 | 136 | . 237 154 | 158 | 162 | 161 | 160 | 159 | 158 | 157 | 157 | 157 | 156 | 156 | 156 | 154 | 150 | 147 | 141 | 140 | 139 | 140 | 143 | 146 | 149 | 150 | 151 | 154 | 155 | 153 | 153 | 156 | 160 | 162 | 162 | 154 | 147 | 146 | 149 | 151 | 148 | 143 | 140 | 140 | 139 | 138 | 136 | 134 | 132 | 131 | 137 | 131 | 126 | 127 | 129 | 130 | 130 | 130 | 133 | 135 | 132 | 131 | 133 | 133 | 131 | 134 | 131 | 134 | 138 | 137 | 133 | 134 | 135 | 131 | 137 | 137 | 139 | 138 | 134 | 135 | 134 | 128 | 131 | 129 | 128 | 128 | 128 | 129 | 129 | 129 | 139 | 138 | 135 | 131 | 130 | 132 | 133 | 132 | 128 | 132 | 135 | 138 | . 238 160 | 158 | 157 | 157 | 161 | 162 | 158 | 153 | 154 | 155 | 156 | 157 | 157 | 157 | 156 | 156 | 146 | 146 | 145 | 144 | 144 | 146 | 148 | 149 | 148 | 150 | 152 | 153 | 155 | 156 | 154 | 151 | 159 | 156 | 152 | 151 | 151 | 152 | 151 | 150 | 150 | 148 | 145 | 142 | 140 | 138 | 138 | 138 | 132 | 130 | 130 | 132 | 134 | 133 | 131 | 130 | 132 | 130 | 129 | 134 | 139 | 135 | 129 | 128 | 128 | 130 | 136 | 137 | 132 | 129 | 131 | 130 | 130 | 130 | 134 | 136 | 135 | 136 | 137 | 134 | 130 | 130 | 129 | 130 | 133 | 135 | 136 | 136 | 139 | 139 | 136 | 132 | 131 | 131 | 131 | 130 | 129 | 132 | 135 | 136 | . 239 163 | 163 | 161 | 158 | 157 | 158 | 159 | 159 | 155 | 156 | 156 | 154 | 153 | 154 | 157 | 161 | 153 | 153 | 152 | 149 | 146 | 144 | 144 | 145 | 149 | 152 | 153 | 151 | 150 | 152 | 155 | 157 | 156 | 159 | 159 | 152 | 145 | 142 | 147 | 153 | 147 | 148 | 150 | 150 | 149 | 146 | 142 | 140 | 137 | 135 | 133 | 134 | 135 | 135 | 136 | 138 | 135 | 131 | 127 | 129 | 132 | 128 | 128 | 134 | 128 | 127 | 134 | 139 | 133 | 126 | 129 | 133 | 140 | 137 | 138 | 138 | 135 | 135 | 137 | 137 | 132 | 131 | 130 | 131 | 133 | 136 | 137 | 137 | 139 | 140 | 139 | 135 | 133 | 132 | 131 | 129 | 129 | 130 | 131 | 132 | . 240 158 | 159 | 162 | 165 | 165 | 163 | 158 | 155 | 166 | 156 | 150 | 153 | 155 | 153 | 153 | 157 | 156 | 153 | 150 | 149 | 150 | 150 | 148 | 146 | 144 | 148 | 151 | 151 | 150 | 150 | 153 | 157 | 162 | 154 | 153 | 158 | 156 | 146 | 141 | 143 | 148 | 147 | 148 | 150 | 150 | 147 | 145 | 146 | 140 | 134 | 131 | 133 | 136 | 136 | 135 | 135 | 141 | 133 | 128 | 129 | 130 | 129 | 128 | 129 | 124 | 132 | 136 | 140 | 132 | 123 | 132 | 140 | 142 | 135 | 140 | 142 | 145 | 131 | 134 | 139 | 133 | 132 | 130 | 129 | 130 | 132 | 133 | 133 | 141 | 140 | 139 | 137 | 135 | 133 | 131 | 130 | 126 | 130 | 131 | 129 | . 241 156 | 157 | 159 | 161 | 163 | 163 | 161 | 160 | 158 | 162 | 161 | 154 | 151 | 155 | 157 | 156 | 152 | 155 | 157 | 154 | 149 | 147 | 148 | 150 | 147 | 146 | 146 | 148 | 152 | 155 | 156 | 156 | 152 | 160 | 162 | 157 | 157 | 159 | 152 | 141 | 135 | 143 | 146 | 142 | 144 | 149 | 146 | 136 | 140 | 134 | 130 | 130 | 131 | 131 | 132 | 133 | 136 | 130 | 127 | 130 | 133 | 131 | 130 | 130 | 130 | 129 | 126 | 133 | 135 | 132 | 137 | 139 | 127 | 135 | 142 | 137 | 135 | 132 | 134 | 132 | 130 | 130 | 129 | 130 | 132 | 133 | 132 | 132 | 136 | 135 | 134 | 133 | 131 | 130 | 129 | 128 | 131 | 131 | 129 | 128 | . 242 155 | 155 | 155 | 157 | 160 | 162 | 164 | 164 | 160 | 160 | 161 | 161 | 157 | 152 | 154 | 158 | 155 | 155 | 154 | 154 | 154 | 152 | 150 | 148 | 148 | 146 | 145 | 147 | 151 | 155 | 155 | 153 | 155 | 156 | 156 | 155 | 156 | 157 | 155 | 151 | 135 | 133 | 131 | 131 | 133 | 137 | 140 | 141 | 145 | 140 | 135 | 132 | 131 | 131 | 133 | 135 | 135 | 131 | 130 | 133 | 135 | 132 | 128 | 127 | 130 | 130 | 127 | 130 | 130 | 128 | 137 | 140 | 137 | 135 | 133 | 137 | 135 | 134 | 129 | 127 | 131 | 130 | 129 | 129 | 130 | 130 | 130 | 130 | 135 | 135 | 133 | 132 | 130 | 130 | 129 | 129 | 128 | 129 | 128 | 127 | . 243 155 | 154 | 153 | 154 | 157 | 160 | 162 | 164 | 164 | 157 | 157 | 162 | 162 | 155 | 153 | 156 | 156 | 153 | 152 | 153 | 156 | 156 | 153 | 150 | 149 | 149 | 149 | 148 | 147 | 147 | 149 | 151 | 157 | 153 | 151 | 154 | 156 | 155 | 157 | 161 | 161 | 152 | 143 | 138 | 134 | 131 | 135 | 142 | 145 | 143 | 140 | 136 | 134 | 132 | 133 | 134 | 134 | 132 | 131 | 134 | 135 | 132 | 128 | 127 | 130 | 133 | 130 | 129 | 126 | 123 | 132 | 135 | 147 | 139 | 130 | 136 | 133 | 135 | 127 | 128 | 131 | 129 | 127 | 126 | 127 | 128 | 130 | 131 | 135 | 135 | 134 | 133 | 132 | 131 | 130 | 130 | 125 | 130 | 131 | 129 | . 244 155 | 154 | 153 | 154 | 155 | 157 | 159 | 159 | 159 | 163 | 162 | 158 | 158 | 163 | 160 | 153 | 151 | 153 | 155 | 154 | 152 | 151 | 155 | 158 | 153 | 152 | 151 | 149 | 147 | 146 | 147 | 148 | 148 | 152 | 153 | 153 | 156 | 161 | 161 | 159 | 159 | 164 | 161 | 149 | 142 | 143 | 142 | 136 | 140 | 141 | 140 | 138 | 135 | 132 | 131 | 131 | 130 | 128 | 128 | 130 | 132 | 131 | 130 | 129 | 132 | 133 | 128 | 128 | 129 | 128 | 132 | 128 | 134 | 143 | 139 | 133 | 123 | 132 | 132 | 131 | 129 | 128 | 128 | 127 | 127 | 128 | 130 | 131 | 130 | 130 | 130 | 130 | 130 | 129 | 128 | 128 | 129 | 133 | 134 | 130 | . 245 154 | 153 | 153 | 153 | 154 | 155 | 155 | 155 | 158 | 162 | 164 | 162 | 160 | 161 | 161 | 160 | 154 | 154 | 153 | 152 | 152 | 153 | 155 | 157 | 157 | 154 | 150 | 150 | 151 | 152 | 150 | 148 | 146 | 143 | 143 | 146 | 149 | 150 | 154 | 159 | 152 | 157 | 160 | 157 | 152 | 149 | 146 | 142 | 141 | 143 | 144 | 143 | 141 | 139 | 136 | 134 | 133 | 130 | 128 | 127 | 127 | 127 | 127 | 126 | 126 | 132 | 131 | 131 | 131 | 131 | 137 | 136 | 135 | 139 | 137 | 136 | 130 | 130 | 130 | 131 | 133 | 133 | 133 | 131 | 129 | 127 | 126 | 125 | 127 | 128 | 128 | 128 | 128 | 128 | 128 | 128 | 131 | 131 | 130 | 130 | . 246 152 | 152 | 152 | 153 | 154 | 154 | 154 | 154 | 159 | 156 | 158 | 164 | 165 | 160 | 159 | 162 | 161 | 156 | 151 | 151 | 154 | 156 | 155 | 152 | 156 | 154 | 152 | 152 | 153 | 154 | 152 | 151 | 152 | 143 | 138 | 141 | 142 | 140 | 146 | 157 | 163 | 159 | 159 | 163 | 160 | 152 | 147 | 149 | 148 | 149 | 149 | 147 | 146 | 145 | 143 | 141 | 140 | 137 | 132 | 128 | 126 | 126 | 125 | 124 | 121 | 132 | 133 | 132 | 129 | 131 | 142 | 144 | 143 | 132 | 131 | 136 | 139 | 129 | 129 | 134 | 138 | 137 | 136 | 133 | 129 | 128 | 127 | 128 | 134 | 133 | 131 | 129 | 128 | 128 | 128 | 128 | 128 | 128 | 129 | 132 | . 247 150 | 150 | 150 | 152 | 153 | 155 | 155 | 155 | 154 | 156 | 156 | 156 | 162 | 168 | 163 | 152 | 159 | 159 | 158 | 155 | 151 | 150 | 152 | 154 | 152 | 155 | 157 | 155 | 151 | 149 | 152 | 155 | 152 | 156 | 153 | 144 | 142 | 149 | 151 | 147 | 145 | 151 | 154 | 156 | 160 | 164 | 159 | 150 | 149 | 149 | 147 | 145 | 144 | 145 | 144 | 142 | 142 | 139 | 135 | 131 | 129 | 129 | 128 | 128 | 127 | 131 | 127 | 127 | 131 | 137 | 145 | 141 | 133 | 131 | 135 | 129 | 133 | 128 | 136 | 139 | 136 | 135 | 132 | 130 | 129 | 131 | 136 | 140 | 141 | 138 | 134 | 131 | 128 | 127 | 127 | 128 | 127 | 130 | 134 | 135 | . 248 150 | 151 | 150 | 149 | 150 | 152 | 154 | 153 | 154 | 152 | 152 | 154 | 157 | 160 | 164 | 168 | 159 | 159 | 163 | 164 | 157 | 148 | 147 | 151 | 152 | 154 | 155 | 157 | 157 | 156 | 154 | 153 | 155 | 153 | 153 | 155 | 154 | 149 | 146 | 144 | 147 | 154 | 156 | 154 | 155 | 162 | 166 | 165 | 158 | 152 | 147 | 147 | 149 | 148 | 146 | 145 | 146 | 145 | 141 | 136 | 135 | 135 | 132 | 128 | 130 | 128 | 126 | 126 | 129 | 132 | 132 | 129 | 132 | 133 | 132 | 134 | 136 | 133 | 134 | 140 | 138 | 130 | 128 | 137 | 144 | 142 | 137 | 136 | 133 | 131 | 130 | 129 | 128 | 126 | 127 | 129 | 129 | 130 | 130 | 129 | . 249 149 | 150 | 150 | 149 | 149 | 151 | 153 | 152 | 154 | 152 | 152 | 153 | 154 | 155 | 158 | 161 | 162 | 161 | 162 | 164 | 166 | 164 | 157 | 150 | 153 | 153 | 153 | 153 | 153 | 153 | 153 | 152 | 155 | 154 | 154 | 154 | 152 | 149 | 151 | 154 | 152 | 144 | 142 | 148 | 151 | 151 | 157 | 165 | 165 | 161 | 157 | 153 | 151 | 150 | 147 | 145 | 146 | 145 | 143 | 141 | 140 | 139 | 136 | 131 | 133 | 132 | 129 | 127 | 129 | 132 | 133 | 132 | 134 | 133 | 131 | 131 | 132 | 130 | 130 | 137 | 138 | 134 | 131 | 133 | 137 | 141 | 144 | 146 | 146 | 137 | 131 | 133 | 135 | 132 | 129 | 129 | 129 | 128 | 128 | 128 | . 250 147 | 148 | 149 | 148 | 148 | 151 | 151 | 151 | 155 | 154 | 153 | 155 | 155 | 154 | 155 | 156 | 156 | 160 | 160 | 157 | 159 | 164 | 166 | 163 | 155 | 155 | 153 | 152 | 151 | 151 | 151 | 151 | 147 | 152 | 158 | 160 | 157 | 154 | 154 | 156 | 146 | 137 | 135 | 140 | 139 | 135 | 143 | 158 | 161 | 163 | 163 | 159 | 157 | 156 | 154 | 151 | 149 | 150 | 149 | 148 | 147 | 144 | 139 | 135 | 137 | 136 | 134 | 130 | 130 | 132 | 133 | 133 | 133 | 132 | 128 | 128 | 129 | 127 | 128 | 135 | 135 | 135 | 132 | 126 | 126 | 135 | 145 | 150 | 143 | 135 | 129 | 131 | 134 | 131 | 129 | 129 | 129 | 127 | 125 | 126 | . 251 144 | 146 | 147 | 146 | 148 | 150 | 150 | 150 | 153 | 152 | 153 | 155 | 155 | 154 | 154 | 156 | 153 | 157 | 157 | 153 | 152 | 158 | 164 | 166 | 157 | 158 | 158 | 156 | 152 | 150 | 151 | 152 | 151 | 152 | 153 | 154 | 154 | 155 | 158 | 161 | 154 | 149 | 143 | 141 | 146 | 151 | 150 | 145 | 150 | 158 | 162 | 160 | 159 | 160 | 159 | 155 | 153 | 153 | 154 | 154 | 153 | 150 | 146 | 142 | 139 | 140 | 139 | 136 | 133 | 133 | 133 | 133 | 131 | 130 | 126 | 127 | 129 | 127 | 128 | 135 | 133 | 134 | 132 | 126 | 125 | 131 | 140 | 146 | 138 | 138 | 135 | 133 | 130 | 129 | 129 | 129 | 130 | 128 | 126 | 125 | . 252 143 | 145 | 145 | 145 | 146 | 149 | 149 | 149 | 149 | 148 | 150 | 152 | 154 | 153 | 153 | 154 | 157 | 156 | 156 | 157 | 159 | 159 | 157 | 156 | 155 | 159 | 162 | 160 | 156 | 153 | 153 | 154 | 149 | 149 | 151 | 154 | 158 | 159 | 156 | 152 | 148 | 158 | 162 | 156 | 152 | 155 | 153 | 148 | 147 | 154 | 157 | 154 | 153 | 156 | 157 | 155 | 153 | 154 | 155 | 157 | 157 | 155 | 153 | 152 | 140 | 143 | 144 | 142 | 138 | 136 | 134 | 132 | 131 | 130 | 127 | 128 | 131 | 129 | 129 | 134 | 133 | 132 | 133 | 135 | 136 | 137 | 141 | 145 | 144 | 148 | 145 | 137 | 133 | 135 | 133 | 128 | 128 | 129 | 129 | 128 | . 253 143 | 145 | 145 | 144 | 145 | 147 | 148 | 147 | 149 | 148 | 149 | 151 | 152 | 152 | 152 | 154 | 155 | 155 | 156 | 157 | 157 | 155 | 157 | 160 | 153 | 156 | 160 | 160 | 159 | 157 | 156 | 157 | 150 | 150 | 150 | 153 | 156 | 157 | 153 | 147 | 142 | 152 | 161 | 160 | 153 | 148 | 149 | 151 | 148 | 150 | 150 | 147 | 148 | 153 | 156 | 156 | 157 | 156 | 156 | 157 | 156 | 154 | 153 | 153 | 143 | 146 | 148 | 146 | 143 | 140 | 136 | 133 | 133 | 132 | 129 | 131 | 133 | 130 | 127 | 132 | 130 | 128 | 131 | 139 | 145 | 145 | 143 | 143 | 140 | 141 | 137 | 131 | 133 | 139 | 134 | 125 | 126 | 129 | 132 | 132 | . 254 145 | 146 | 145 | 144 | 144 | 145 | 146 | 146 | 150 | 148 | 148 | 150 | 151 | 151 | 152 | 154 | 150 | 155 | 156 | 152 | 149 | 153 | 160 | 165 | 156 | 155 | 155 | 157 | 160 | 160 | 159 | 157 | 159 | 156 | 150 | 143 | 142 | 148 | 154 | 158 | 160 | 152 | 148 | 153 | 160 | 161 | 156 | 150 | 151 | 149 | 147 | 148 | 151 | 154 | 156 | 156 | 157 | 156 | 156 | 156 | 154 | 151 | 150 | 151 | 148 | 150 | 150 | 148 | 145 | 143 | 140 | 138 | 135 | 133 | 130 | 131 | 133 | 129 | 126 | 130 | 128 | 127 | 129 | 135 | 143 | 146 | 142 | 136 | 132 | 129 | 125 | 125 | 130 | 134 | 133 | 129 | 130 | 131 | 131 | 131 | . 255 147 | 147 | 146 | 143 | 143 | 144 | 145 | 144 | 147 | 146 | 145 | 147 | 148 | 148 | 150 | 153 | 153 | 156 | 154 | 150 | 153 | 161 | 159 | 151 | 162 | 157 | 153 | 154 | 160 | 163 | 160 | 156 | 147 | 154 | 159 | 156 | 150 | 148 | 148 | 149 | 158 | 160 | 159 | 153 | 150 | 154 | 160 | 163 | 155 | 151 | 150 | 154 | 157 | 155 | 152 | 150 | 151 | 151 | 152 | 154 | 154 | 152 | 151 | 153 | 152 | 152 | 151 | 148 | 146 | 145 | 144 | 142 | 135 | 133 | 129 | 130 | 132 | 129 | 126 | 130 | 129 | 131 | 130 | 130 | 137 | 143 | 139 | 129 | 133 | 129 | 127 | 131 | 131 | 130 | 133 | 140 | 137 | 133 | 128 | 126 | . 256 146 | 147 | 148 | 147 | 146 | 146 | 148 | 151 | 143 | 146 | 147 | 147 | 147 | 148 | 149 | 148 | 149 | 151 | 149 | 146 | 146 | 149 | 151 | 149 | 157 | 157 | 158 | 156 | 149 | 145 | 156 | 171 | 161 | 151 | 150 | 157 | 161 | 160 | 155 | 148 | 154 | 150 | 151 | 154 | 153 | 150 | 155 | 163 | 163 | 158 | 156 | 159 | 158 | 153 | 150 | 151 | 151 | 152 | 151 | 150 | 152 | 156 | 156 | 154 | 152 | 152 | 152 | 151 | 150 | 148 | 147 | 146 | 138 | 139 | 137 | 132 | 128 | 127 | 127 | 126 | 130 | 130 | 132 | 136 | 138 | 138 | 141 | 144 | 142 | 138 | 133 | 130 | 129 | 129 | 129 | 129 | 140 | 127 | 127 | 128 | . 257 144 | 145 | 146 | 146 | 145 | 145 | 146 | 148 | 149 | 148 | 146 | 142 | 141 | 143 | 146 | 146 | 146 | 147 | 146 | 145 | 146 | 148 | 150 | 150 | 148 | 153 | 157 | 156 | 151 | 150 | 153 | 156 | 166 | 163 | 159 | 150 | 145 | 152 | 159 | 159 | 143 | 142 | 145 | 152 | 158 | 159 | 154 | 150 | 157 | 161 | 163 | 160 | 160 | 163 | 162 | 158 | 150 | 152 | 153 | 152 | 153 | 153 | 151 | 147 | 154 | 153 | 152 | 150 | 149 | 148 | 148 | 148 | 145 | 143 | 138 | 133 | 130 | 130 | 129 | 128 | 131 | 129 | 129 | 131 | 134 | 136 | 141 | 146 | 144 | 141 | 137 | 133 | 131 | 130 | 129 | 128 | 131 | 124 | 129 | 131 | . 258 143 | 144 | 146 | 146 | 145 | 145 | 146 | 147 | 149 | 148 | 144 | 139 | 138 | 140 | 142 | 143 | 146 | 144 | 144 | 146 | 147 | 147 | 149 | 151 | 146 | 154 | 157 | 153 | 153 | 158 | 156 | 150 | 158 | 160 | 162 | 158 | 150 | 151 | 155 | 156 | 169 | 157 | 142 | 135 | 143 | 155 | 159 | 156 | 156 | 161 | 162 | 158 | 157 | 160 | 162 | 161 | 162 | 160 | 155 | 149 | 146 | 148 | 151 | 152 | 153 | 153 | 152 | 152 | 151 | 151 | 150 | 150 | 147 | 144 | 138 | 132 | 131 | 132 | 131 | 129 | 130 | 128 | 128 | 129 | 131 | 133 | 138 | 142 | 139 | 137 | 134 | 132 | 130 | 128 | 126 | 124 | 128 | 123 | 132 | 136 | . 259 143 | 145 | 146 | 146 | 145 | 145 | 145 | 144 | 145 | 146 | 145 | 143 | 142 | 143 | 144 | 143 | 147 | 145 | 145 | 148 | 149 | 147 | 149 | 152 | 151 | 155 | 153 | 148 | 151 | 159 | 161 | 156 | 151 | 146 | 153 | 166 | 165 | 156 | 150 | 149 | 149 | 161 | 170 | 165 | 154 | 148 | 148 | 151 | 154 | 153 | 155 | 160 | 159 | 154 | 153 | 156 | 157 | 159 | 161 | 160 | 158 | 155 | 154 | 153 | 150 | 151 | 153 | 154 | 154 | 153 | 151 | 150 | 144 | 143 | 139 | 135 | 133 | 133 | 132 | 130 | 128 | 128 | 129 | 130 | 130 | 130 | 131 | 133 | 130 | 129 | 128 | 127 | 126 | 125 | 124 | 123 | 128 | 124 | 131 | 139 | . 260 143 | 146 | 146 | 145 | 144 | 145 | 144 | 142 | 145 | 147 | 149 | 148 | 147 | 148 | 148 | 147 | 149 | 146 | 147 | 149 | 150 | 147 | 147 | 150 | 153 | 153 | 150 | 147 | 149 | 154 | 158 | 158 | 156 | 145 | 145 | 155 | 158 | 157 | 157 | 157 | 159 | 153 | 149 | 155 | 163 | 163 | 157 | 151 | 149 | 148 | 152 | 160 | 162 | 158 | 154 | 154 | 153 | 155 | 159 | 161 | 161 | 160 | 160 | 161 | 152 | 153 | 153 | 154 | 153 | 153 | 152 | 151 | 145 | 147 | 146 | 142 | 138 | 135 | 133 | 131 | 128 | 129 | 130 | 130 | 129 | 128 | 128 | 127 | 127 | 127 | 126 | 125 | 125 | 125 | 126 | 126 | 127 | 123 | 128 | 134 | . 261 147 | 149 | 148 | 145 | 145 | 147 | 147 | 145 | 146 | 148 | 148 | 146 | 146 | 147 | 148 | 148 | 147 | 147 | 147 | 148 | 148 | 147 | 147 | 147 | 149 | 150 | 152 | 153 | 153 | 152 | 153 | 155 | 159 | 155 | 150 | 144 | 144 | 155 | 163 | 162 | 155 | 154 | 156 | 158 | 156 | 153 | 156 | 161 | 154 | 156 | 153 | 150 | 152 | 158 | 160 | 156 | 161 | 158 | 154 | 153 | 153 | 155 | 160 | 164 | 158 | 157 | 155 | 153 | 151 | 151 | 152 | 153 | 151 | 154 | 155 | 151 | 145 | 141 | 137 | 134 | 134 | 134 | 133 | 130 | 128 | 129 | 130 | 130 | 128 | 128 | 127 | 125 | 124 | 124 | 126 | 127 | 127 | 128 | 129 | 127 | . 262 147 | 149 | 148 | 144 | 144 | 148 | 149 | 147 | 145 | 146 | 146 | 145 | 144 | 145 | 146 | 145 | 144 | 146 | 147 | 147 | 147 | 148 | 148 | 147 | 146 | 149 | 152 | 155 | 155 | 153 | 153 | 154 | 154 | 159 | 160 | 153 | 148 | 154 | 158 | 152 | 162 | 162 | 159 | 155 | 154 | 155 | 157 | 158 | 161 | 161 | 156 | 148 | 148 | 154 | 157 | 156 | 156 | 155 | 156 | 159 | 159 | 156 | 154 | 154 | 160 | 159 | 157 | 155 | 154 | 153 | 153 | 153 | 153 | 155 | 155 | 152 | 149 | 147 | 144 | 142 | 141 | 141 | 138 | 133 | 130 | 131 | 132 | 132 | 130 | 130 | 129 | 127 | 125 | 124 | 125 | 126 | 128 | 131 | 130 | 124 | . 263 144 | 145 | 143 | 140 | 141 | 146 | 147 | 145 | 143 | 146 | 148 | 148 | 147 | 147 | 147 | 145 | 142 | 146 | 147 | 146 | 147 | 150 | 150 | 147 | 144 | 148 | 150 | 150 | 151 | 154 | 156 | 156 | 150 | 154 | 162 | 165 | 160 | 154 | 149 | 141 | 148 | 160 | 166 | 162 | 157 | 158 | 159 | 156 | 155 | 155 | 158 | 163 | 160 | 153 | 151 | 153 | 156 | 154 | 153 | 156 | 159 | 159 | 160 | 161 | 157 | 158 | 159 | 159 | 159 | 157 | 154 | 152 | 150 | 150 | 149 | 149 | 150 | 152 | 151 | 149 | 147 | 147 | 144 | 138 | 134 | 134 | 133 | 132 | 132 | 133 | 132 | 131 | 128 | 127 | 127 | 127 | 125 | 128 | 128 | 125 | . 264 149 | 142 | 139 | 143 | 147 | 147 | 146 | 146 | 146 | 147 | 147 | 146 | 145 | 145 | 147 | 148 | 148 | 146 | 146 | 146 | 146 | 146 | 148 | 151 | 149 | 148 | 148 | 149 | 149 | 149 | 151 | 155 | 155 | 155 | 156 | 159 | 163 | 161 | 152 | 142 | 149 | 150 | 159 | 167 | 163 | 156 | 156 | 158 | 157 | 160 | 159 | 156 | 158 | 162 | 160 | 153 | 155 | 155 | 156 | 156 | 156 | 156 | 156 | 155 | 162 | 160 | 158 | 157 | 158 | 159 | 159 | 158 | 152 | 151 | 151 | 152 | 153 | 154 | 153 | 152 | 150 | 150 | 148 | 144 | 140 | 138 | 137 | 138 | 135 | 135 | 135 | 134 | 131 | 129 | 127 | 127 | 128 | 129 | 130 | 131 | . 265 145 | 145 | 146 | 146 | 144 | 142 | 144 | 146 | 143 | 144 | 146 | 147 | 146 | 145 | 145 | 145 | 149 | 147 | 146 | 146 | 145 | 144 | 145 | 148 | 148 | 148 | 149 | 151 | 151 | 150 | 151 | 154 | 153 | 153 | 155 | 158 | 163 | 165 | 161 | 155 | 152 | 143 | 144 | 154 | 163 | 165 | 162 | 156 | 156 | 157 | 158 | 158 | 158 | 159 | 159 | 158 | 155 | 155 | 154 | 154 | 154 | 155 | 155 | 155 | 158 | 158 | 158 | 157 | 157 | 157 | 158 | 158 | 162 | 160 | 157 | 154 | 152 | 153 | 155 | 157 | 151 | 152 | 152 | 150 | 146 | 143 | 141 | 141 | 138 | 137 | 137 | 137 | 137 | 134 | 130 | 127 | 129 | 132 | 133 | 131 | . 266 141 | 144 | 144 | 140 | 137 | 137 | 140 | 142 | 144 | 145 | 146 | 147 | 146 | 146 | 146 | 147 | 148 | 146 | 146 | 145 | 144 | 143 | 144 | 146 | 147 | 147 | 149 | 152 | 152 | 150 | 150 | 152 | 150 | 151 | 151 | 153 | 158 | 163 | 164 | 162 | 160 | 152 | 147 | 146 | 150 | 158 | 165 | 165 | 160 | 157 | 155 | 157 | 158 | 157 | 158 | 162 | 158 | 157 | 156 | 155 | 155 | 154 | 154 | 154 | 156 | 157 | 159 | 158 | 157 | 156 | 157 | 159 | 159 | 157 | 154 | 150 | 146 | 146 | 149 | 152 | 151 | 153 | 154 | 153 | 151 | 148 | 146 | 145 | 145 | 142 | 139 | 139 | 141 | 140 | 135 | 131 | 131 | 130 | 133 | 142 | . 267 151 | 151 | 147 | 142 | 142 | 146 | 147 | 145 | 146 | 146 | 144 | 143 | 144 | 145 | 147 | 149 | 145 | 144 | 144 | 145 | 145 | 144 | 145 | 148 | 146 | 147 | 149 | 150 | 151 | 150 | 150 | 150 | 150 | 151 | 150 | 149 | 152 | 156 | 159 | 160 | 166 | 166 | 163 | 153 | 144 | 148 | 159 | 163 | 166 | 160 | 155 | 155 | 157 | 158 | 159 | 160 | 161 | 160 | 159 | 158 | 156 | 154 | 152 | 151 | 154 | 156 | 157 | 158 | 158 | 157 | 158 | 159 | 155 | 156 | 156 | 154 | 152 | 150 | 150 | 150 | 150 | 151 | 152 | 152 | 152 | 150 | 149 | 149 | 151 | 147 | 142 | 140 | 141 | 141 | 139 | 137 | 131 | 132 | 132 | 134 | . 268 146 | 147 | 144 | 139 | 139 | 142 | 141 | 138 | 144 | 143 | 142 | 141 | 142 | 143 | 145 | 145 | 144 | 143 | 143 | 145 | 145 | 145 | 146 | 148 | 146 | 147 | 147 | 147 | 148 | 149 | 149 | 148 | 151 | 152 | 151 | 150 | 150 | 154 | 157 | 158 | 163 | 165 | 168 | 164 | 157 | 155 | 153 | 147 | 161 | 162 | 161 | 157 | 156 | 157 | 158 | 158 | 160 | 160 | 160 | 159 | 157 | 155 | 152 | 150 | 152 | 152 | 153 | 155 | 158 | 159 | 159 | 159 | 155 | 155 | 157 | 158 | 159 | 157 | 154 | 151 | 151 | 150 | 149 | 149 | 150 | 151 | 152 | 152 | 152 | 150 | 146 | 144 | 142 | 142 | 141 | 141 | 140 | 137 | 133 | 128 | . 269 145 | 149 | 152 | 150 | 146 | 143 | 142 | 141 | 141 | 142 | 143 | 145 | 146 | 145 | 143 | 141 | 144 | 143 | 143 | 145 | 145 | 144 | 145 | 147 | 146 | 147 | 146 | 145 | 146 | 148 | 148 | 147 | 147 | 149 | 149 | 149 | 150 | 153 | 156 | 156 | 156 | 159 | 166 | 168 | 165 | 164 | 160 | 152 | 151 | 160 | 165 | 162 | 157 | 156 | 156 | 157 | 159 | 160 | 161 | 161 | 161 | 159 | 157 | 156 | 151 | 150 | 150 | 152 | 156 | 158 | 159 | 158 | 155 | 155 | 154 | 156 | 157 | 157 | 154 | 152 | 154 | 152 | 150 | 149 | 150 | 152 | 153 | 154 | 149 | 150 | 151 | 150 | 147 | 144 | 143 | 142 | 146 | 143 | 140 | 139 | . 270 140 | 143 | 146 | 148 | 146 | 141 | 139 | 139 | 143 | 144 | 146 | 148 | 149 | 147 | 143 | 140 | 143 | 142 | 143 | 144 | 144 | 143 | 144 | 146 | 146 | 147 | 146 | 145 | 146 | 148 | 148 | 146 | 145 | 146 | 147 | 147 | 148 | 152 | 154 | 154 | 154 | 156 | 162 | 165 | 162 | 164 | 167 | 166 | 154 | 155 | 157 | 159 | 160 | 157 | 155 | 154 | 159 | 159 | 160 | 160 | 161 | 161 | 161 | 161 | 156 | 155 | 153 | 153 | 153 | 155 | 156 | 157 | 159 | 158 | 158 | 157 | 158 | 158 | 157 | 157 | 157 | 155 | 153 | 152 | 153 | 153 | 153 | 152 | 149 | 151 | 153 | 153 | 151 | 148 | 147 | 146 | 145 | 148 | 149 | 143 | . 271 147 | 142 | 141 | 146 | 150 | 149 | 145 | 144 | 144 | 144 | 145 | 146 | 147 | 146 | 143 | 140 | 141 | 141 | 142 | 143 | 143 | 143 | 143 | 145 | 146 | 147 | 146 | 145 | 146 | 149 | 148 | 145 | 146 | 147 | 147 | 147 | 148 | 151 | 152 | 151 | 154 | 151 | 155 | 161 | 162 | 164 | 165 | 162 | 166 | 152 | 144 | 152 | 162 | 161 | 155 | 151 | 157 | 157 | 156 | 156 | 156 | 157 | 158 | 159 | 162 | 161 | 159 | 155 | 152 | 151 | 153 | 155 | 154 | 156 | 157 | 156 | 155 | 155 | 156 | 157 | 158 | 157 | 156 | 155 | 155 | 153 | 151 | 149 | 151 | 152 | 152 | 153 | 152 | 151 | 151 | 151 | 151 | 149 | 148 | 148 | . 272 148 | 147 | 145 | 144 | 145 | 146 | 148 | 149 | 146 | 144 | 142 | 142 | 143 | 145 | 146 | 146 | 143 | 144 | 145 | 145 | 148 | 149 | 147 | 143 | 146 | 145 | 146 | 146 | 143 | 141 | 143 | 147 | 148 | 147 | 147 | 148 | 149 | 150 | 150 | 150 | 152 | 152 | 154 | 158 | 160 | 160 | 161 | 162 | 168 | 167 | 157 | 150 | 155 | 157 | 155 | 158 | 150 | 154 | 157 | 157 | 157 | 157 | 157 | 155 | 159 | 159 | 161 | 162 | 161 | 158 | 155 | 152 | 153 | 154 | 155 | 155 | 154 | 154 | 155 | 155 | 154 | 155 | 155 | 154 | 153 | 153 | 155 | 156 | 153 | 152 | 151 | 151 | 151 | 151 | 151 | 151 | 151 | 150 | 148 | 147 | . 273 147 | 147 | 146 | 145 | 145 | 146 | 147 | 148 | 144 | 143 | 143 | 144 | 145 | 146 | 145 | 145 | 143 | 144 | 144 | 144 | 145 | 146 | 144 | 141 | 144 | 142 | 141 | 142 | 142 | 141 | 141 | 143 | 144 | 144 | 145 | 147 | 149 | 151 | 152 | 152 | 152 | 151 | 152 | 156 | 159 | 159 | 159 | 159 | 157 | 166 | 170 | 164 | 157 | 152 | 153 | 158 | 151 | 153 | 155 | 156 | 156 | 155 | 156 | 157 | 156 | 156 | 157 | 158 | 159 | 158 | 157 | 156 | 156 | 156 | 156 | 155 | 154 | 154 | 156 | 157 | 155 | 155 | 156 | 156 | 155 | 154 | 155 | 156 | 154 | 153 | 151 | 150 | 149 | 148 | 147 | 145 | 151 | 150 | 149 | 149 | . 274 145 | 146 | 146 | 146 | 146 | 145 | 145 | 145 | 146 | 145 | 144 | 144 | 143 | 142 | 141 | 140 | 143 | 144 | 143 | 142 | 143 | 144 | 143 | 140 | 146 | 143 | 141 | 143 | 145 | 145 | 144 | 143 | 144 | 144 | 145 | 147 | 147 | 148 | 148 | 148 | 151 | 150 | 151 | 155 | 157 | 158 | 157 | 158 | 160 | 164 | 169 | 169 | 164 | 160 | 157 | 152 | 153 | 151 | 152 | 154 | 155 | 153 | 155 | 158 | 155 | 155 | 154 | 155 | 157 | 158 | 159 | 160 | 157 | 157 | 157 | 156 | 155 | 155 | 155 | 156 | 154 | 154 | 155 | 155 | 154 | 154 | 154 | 154 | 154 | 152 | 151 | 151 | 153 | 153 | 151 | 150 | 150 | 150 | 149 | 149 | . 275 143 | 144 | 145 | 145 | 145 | 144 | 143 | 143 | 148 | 147 | 145 | 143 | 142 | 142 | 142 | 142 | 143 | 144 | 143 | 142 | 142 | 144 | 143 | 141 | 147 | 145 | 144 | 146 | 148 | 148 | 147 | 146 | 145 | 146 | 147 | 148 | 147 | 147 | 146 | 146 | 150 | 150 | 151 | 154 | 156 | 156 | 157 | 159 | 164 | 157 | 159 | 163 | 166 | 171 | 168 | 157 | 160 | 153 | 150 | 154 | 155 | 153 | 154 | 157 | 157 | 156 | 155 | 156 | 157 | 159 | 160 | 160 | 156 | 158 | 159 | 159 | 158 | 156 | 154 | 154 | 154 | 154 | 154 | 154 | 154 | 154 | 155 | 155 | 152 | 151 | 150 | 152 | 155 | 156 | 154 | 152 | 151 | 151 | 151 | 150 | . 276 143 | 144 | 145 | 146 | 146 | 145 | 144 | 143 | 145 | 145 | 146 | 146 | 145 | 145 | 145 | 146 | 143 | 144 | 144 | 142 | 143 | 145 | 145 | 144 | 143 | 144 | 144 | 144 | 144 | 144 | 145 | 145 | 142 | 144 | 146 | 148 | 149 | 149 | 150 | 151 | 147 | 148 | 151 | 153 | 153 | 153 | 155 | 159 | 157 | 154 | 158 | 160 | 159 | 166 | 174 | 172 | 167 | 159 | 153 | 156 | 159 | 156 | 155 | 157 | 158 | 157 | 156 | 157 | 158 | 158 | 159 | 159 | 159 | 159 | 160 | 160 | 159 | 157 | 157 | 156 | 157 | 156 | 155 | 155 | 155 | 156 | 157 | 157 | 154 | 152 | 151 | 152 | 153 | 152 | 150 | 148 | 151 | 151 | 152 | 153 | . 277 144 | 145 | 146 | 147 | 147 | 146 | 146 | 145 | 143 | 145 | 147 | 148 | 146 | 144 | 141 | 140 | 142 | 144 | 144 | 143 | 144 | 146 | 147 | 146 | 140 | 143 | 144 | 143 | 140 | 141 | 142 | 144 | 140 | 142 | 144 | 146 | 146 | 146 | 147 | 149 | 145 | 146 | 149 | 151 | 150 | 150 | 152 | 156 | 156 | 158 | 160 | 159 | 156 | 159 | 166 | 168 | 170 | 164 | 160 | 161 | 162 | 160 | 158 | 158 | 157 | 156 | 156 | 157 | 158 | 158 | 158 | 157 | 162 | 162 | 161 | 159 | 159 | 159 | 160 | 162 | 158 | 156 | 155 | 155 | 155 | 155 | 155 | 155 | 157 | 156 | 155 | 155 | 155 | 154 | 153 | 152 | 153 | 152 | 151 | 151 | . 278 144 | 145 | 145 | 145 | 146 | 146 | 146 | 146 | 146 | 147 | 148 | 147 | 144 | 141 | 139 | 138 | 141 | 143 | 144 | 143 | 143 | 145 | 146 | 145 | 141 | 145 | 146 | 143 | 140 | 141 | 143 | 144 | 143 | 144 | 145 | 144 | 141 | 140 | 141 | 142 | 147 | 146 | 146 | 149 | 150 | 150 | 150 | 152 | 157 | 159 | 157 | 155 | 157 | 158 | 158 | 158 | 165 | 165 | 164 | 163 | 161 | 159 | 159 | 160 | 158 | 158 | 158 | 158 | 158 | 158 | 157 | 156 | 160 | 160 | 160 | 160 | 160 | 160 | 161 | 162 | 159 | 158 | 157 | 157 | 157 | 156 | 153 | 152 | 155 | 156 | 157 | 157 | 156 | 157 | 158 | 159 | 156 | 154 | 152 | 151 | . 279 143 | 143 | 143 | 143 | 144 | 145 | 146 | 146 | 149 | 149 | 148 | 146 | 144 | 144 | 146 | 148 | 140 | 142 | 143 | 143 | 143 | 144 | 145 | 144 | 143 | 146 | 146 | 143 | 141 | 143 | 144 | 144 | 146 | 146 | 147 | 145 | 142 | 140 | 141 | 142 | 151 | 147 | 145 | 148 | 151 | 151 | 150 | 149 | 148 | 155 | 154 | 152 | 157 | 158 | 157 | 163 | 158 | 163 | 166 | 163 | 159 | 157 | 159 | 160 | 162 | 161 | 160 | 159 | 159 | 158 | 157 | 156 | 154 | 157 | 160 | 162 | 162 | 160 | 159 | 158 | 164 | 163 | 163 | 162 | 162 | 159 | 155 | 152 | 152 | 153 | 154 | 153 | 151 | 151 | 153 | 155 | 159 | 158 | 157 | 156 | . 280 148 | 145 | 140 | 138 | 140 | 143 | 143 | 141 | 142 | 143 | 144 | 146 | 147 | 147 | 147 | 146 | 143 | 141 | 142 | 145 | 146 | 145 | 146 | 147 | 143 | 145 | 146 | 145 | 145 | 146 | 144 | 142 | 140 | 142 | 145 | 146 | 146 | 146 | 146 | 146 | 146 | 147 | 148 | 148 | 147 | 148 | 150 | 152 | 152 | 150 | 152 | 156 | 156 | 154 | 155 | 158 | 162 | 158 | 162 | 166 | 164 | 165 | 160 | 146 | 152 | 159 | 162 | 159 | 160 | 164 | 161 | 153 | 151 | 157 | 159 | 159 | 160 | 157 | 155 | 157 | 161 | 159 | 157 | 158 | 160 | 161 | 160 | 159 | 156 | 154 | 152 | 152 | 153 | 155 | 154 | 152 | 154 | 154 | 153 | 154 | . 281 144 | 144 | 142 | 139 | 138 | 141 | 143 | 144 | 141 | 143 | 144 | 145 | 146 | 146 | 147 | 148 | 147 | 145 | 145 | 146 | 147 | 145 | 146 | 147 | 143 | 144 | 145 | 144 | 144 | 146 | 145 | 144 | 140 | 141 | 142 | 142 | 144 | 145 | 147 | 148 | 144 | 146 | 149 | 149 | 148 | 147 | 147 | 148 | 145 | 146 | 148 | 150 | 152 | 152 | 154 | 156 | 159 | 158 | 162 | 163 | 160 | 165 | 171 | 168 | 145 | 147 | 151 | 155 | 158 | 158 | 158 | 158 | 160 | 158 | 153 | 152 | 158 | 161 | 161 | 162 | 158 | 158 | 158 | 159 | 161 | 161 | 161 | 161 | 161 | 159 | 156 | 154 | 153 | 153 | 154 | 154 | 155 | 153 | 152 | 152 | . 282 143 | 146 | 146 | 142 | 138 | 139 | 143 | 145 | 143 | 144 | 145 | 146 | 145 | 146 | 147 | 148 | 148 | 147 | 146 | 146 | 145 | 144 | 143 | 144 | 143 | 144 | 144 | 143 | 143 | 145 | 147 | 146 | 146 | 145 | 143 | 143 | 143 | 143 | 143 | 143 | 142 | 144 | 147 | 149 | 148 | 147 | 146 | 146 | 144 | 146 | 148 | 148 | 150 | 153 | 155 | 155 | 153 | 155 | 161 | 163 | 160 | 163 | 167 | 166 | 175 | 158 | 148 | 152 | 154 | 150 | 152 | 160 | 162 | 164 | 161 | 158 | 157 | 156 | 157 | 162 | 159 | 160 | 160 | 160 | 159 | 158 | 158 | 158 | 162 | 162 | 161 | 157 | 153 | 151 | 150 | 150 | 155 | 153 | 152 | 152 | . 283 148 | 150 | 150 | 146 | 142 | 141 | 142 | 142 | 143 | 145 | 147 | 147 | 145 | 145 | 145 | 147 | 146 | 146 | 147 | 146 | 145 | 144 | 143 | 143 | 144 | 144 | 143 | 142 | 142 | 145 | 147 | 147 | 149 | 148 | 147 | 147 | 146 | 144 | 141 | 139 | 141 | 143 | 145 | 147 | 147 | 147 | 147 | 148 | 147 | 151 | 151 | 149 | 149 | 153 | 154 | 153 | 151 | 153 | 157 | 161 | 162 | 161 | 158 | 154 | 167 | 171 | 159 | 137 | 132 | 147 | 154 | 148 | 154 | 159 | 162 | 163 | 162 | 158 | 156 | 160 | 159 | 160 | 161 | 159 | 157 | 156 | 156 | 157 | 158 | 159 | 160 | 160 | 158 | 155 | 152 | 151 | 152 | 152 | 153 | 153 | . 284 149 | 149 | 148 | 146 | 147 | 147 | 143 | 139 | 141 | 143 | 145 | 145 | 145 | 144 | 144 | 144 | 144 | 146 | 148 | 149 | 148 | 147 | 146 | 145 | 145 | 146 | 144 | 142 | 142 | 145 | 146 | 147 | 146 | 147 | 147 | 148 | 149 | 148 | 147 | 146 | 144 | 144 | 144 | 144 | 145 | 146 | 148 | 149 | 148 | 151 | 151 | 148 | 147 | 150 | 151 | 150 | 154 | 154 | 154 | 155 | 159 | 160 | 160 | 161 | 155 | 162 | 165 | 155 | 142 | 138 | 143 | 149 | 150 | 148 | 145 | 152 | 163 | 166 | 163 | 164 | 159 | 160 | 161 | 160 | 158 | 157 | 159 | 162 | 156 | 156 | 157 | 159 | 162 | 162 | 161 | 159 | 153 | 154 | 154 | 154 | . 285 147 | 146 | 144 | 145 | 148 | 149 | 146 | 141 | 139 | 139 | 141 | 142 | 143 | 144 | 143 | 143 | 140 | 144 | 147 | 148 | 147 | 147 | 146 | 145 | 146 | 147 | 145 | 143 | 143 | 145 | 146 | 146 | 146 | 147 | 146 | 146 | 146 | 148 | 150 | 152 | 148 | 146 | 145 | 143 | 143 | 145 | 147 | 148 | 147 | 148 | 149 | 148 | 148 | 149 | 150 | 150 | 150 | 155 | 156 | 156 | 157 | 156 | 158 | 166 | 164 | 156 | 161 | 173 | 168 | 151 | 148 | 161 | 159 | 152 | 144 | 143 | 151 | 155 | 159 | 166 | 163 | 164 | 164 | 163 | 160 | 159 | 160 | 161 | 163 | 160 | 157 | 157 | 159 | 160 | 160 | 158 | 158 | 159 | 158 | 156 | . 286 147 | 147 | 146 | 145 | 145 | 147 | 146 | 144 | 142 | 141 | 140 | 141 | 143 | 144 | 144 | 143 | 140 | 143 | 146 | 145 | 144 | 145 | 146 | 145 | 146 | 147 | 146 | 144 | 144 | 146 | 146 | 145 | 145 | 146 | 147 | 145 | 143 | 144 | 147 | 150 | 147 | 146 | 145 | 144 | 144 | 145 | 145 | 145 | 145 | 146 | 147 | 149 | 149 | 149 | 150 | 152 | 147 | 154 | 155 | 157 | 160 | 155 | 153 | 159 | 150 | 156 | 158 | 155 | 161 | 171 | 170 | 160 | 165 | 163 | 157 | 152 | 148 | 144 | 148 | 159 | 164 | 165 | 166 | 165 | 163 | 161 | 160 | 160 | 166 | 163 | 160 | 158 | 157 | 156 | 154 | 153 | 160 | 159 | 158 | 158 | . 287 150 | 152 | 152 | 147 | 142 | 142 | 144 | 146 | 149 | 145 | 142 | 141 | 143 | 145 | 145 | 143 | 142 | 146 | 147 | 145 | 144 | 145 | 147 | 147 | 146 | 147 | 147 | 145 | 145 | 147 | 147 | 145 | 140 | 143 | 146 | 146 | 143 | 143 | 145 | 148 | 144 | 144 | 145 | 145 | 146 | 145 | 144 | 144 | 144 | 142 | 144 | 148 | 149 | 147 | 147 | 151 | 150 | 152 | 149 | 152 | 160 | 158 | 153 | 157 | 152 | 153 | 159 | 162 | 158 | 152 | 155 | 163 | 161 | 162 | 161 | 162 | 161 | 152 | 147 | 150 | 157 | 159 | 162 | 164 | 165 | 164 | 163 | 163 | 161 | 162 | 164 | 163 | 161 | 158 | 156 | 155 | 156 | 154 | 154 | 157 | . 288 144 | 147 | 151 | 151 | 147 | 143 | 145 | 150 | 144 | 148 | 148 | 144 | 142 | 144 | 144 | 142 | 144 | 144 | 144 | 143 | 143 | 145 | 145 | 144 | 146 | 148 | 147 | 144 | 143 | 147 | 148 | 147 | 144 | 147 | 147 | 145 | 146 | 148 | 147 | 144 | 145 | 144 | 143 | 140 | 137 | 140 | 144 | 141 | 142 | 144 | 146 | 146 | 146 | 146 | 147 | 149 | 148 | 149 | 149 | 149 | 150 | 154 | 156 | 156 | 158 | 152 | 148 | 151 | 155 | 156 | 155 | 155 | 163 | 159 | 155 | 157 | 163 | 165 | 159 | 151 | 149 | 158 | 161 | 161 | 164 | 165 | 164 | 164 | 161 | 164 | 162 | 158 | 159 | 164 | 165 | 161 | 159 | 157 | 156 | 157 | . 289 148 | 141 | 136 | 140 | 147 | 150 | 148 | 146 | 146 | 150 | 151 | 145 | 140 | 141 | 144 | 144 | 143 | 145 | 144 | 142 | 141 | 143 | 144 | 143 | 141 | 143 | 145 | 144 | 144 | 145 | 145 | 144 | 146 | 146 | 144 | 142 | 143 | 145 | 146 | 145 | 144 | 144 | 146 | 145 | 141 | 142 | 143 | 140 | 143 | 144 | 145 | 145 | 145 | 146 | 148 | 150 | 148 | 149 | 148 | 147 | 148 | 151 | 152 | 152 | 155 | 160 | 156 | 143 | 138 | 147 | 156 | 156 | 154 | 157 | 159 | 158 | 157 | 160 | 163 | 165 | 169 | 159 | 149 | 153 | 162 | 162 | 160 | 163 | 165 | 163 | 165 | 169 | 166 | 158 | 159 | 166 | 165 | 162 | 159 | 158 | . 290 173 | 165 | 155 | 148 | 145 | 145 | 146 | 148 | 140 | 144 | 147 | 148 | 149 | 150 | 146 | 140 | 146 | 147 | 147 | 144 | 143 | 143 | 144 | 144 | 143 | 143 | 144 | 145 | 145 | 143 | 142 | 143 | 145 | 144 | 145 | 148 | 148 | 147 | 147 | 147 | 145 | 145 | 147 | 147 | 144 | 144 | 145 | 141 | 142 | 142 | 142 | 142 | 143 | 144 | 145 | 146 | 148 | 149 | 148 | 147 | 147 | 149 | 150 | 149 | 150 | 155 | 157 | 152 | 144 | 141 | 145 | 150 | 152 | 155 | 156 | 155 | 155 | 157 | 160 | 161 | 154 | 163 | 163 | 158 | 159 | 160 | 158 | 156 | 161 | 164 | 165 | 164 | 162 | 162 | 163 | 164 | 165 | 162 | 159 | 159 | . 291 174 | 174 | 171 | 161 | 149 | 142 | 143 | 147 | 151 | 150 | 148 | 145 | 145 | 146 | 146 | 143 | 146 | 148 | 149 | 146 | 144 | 143 | 144 | 143 | 147 | 144 | 143 | 145 | 146 | 145 | 145 | 148 | 143 | 142 | 143 | 146 | 146 | 142 | 142 | 144 | 146 | 145 | 147 | 147 | 144 | 145 | 148 | 146 | 142 | 141 | 141 | 142 | 142 | 142 | 142 | 141 | 148 | 149 | 150 | 149 | 149 | 150 | 149 | 148 | 146 | 147 | 152 | 159 | 158 | 149 | 141 | 139 | 148 | 150 | 152 | 153 | 154 | 156 | 155 | 154 | 160 | 151 | 150 | 164 | 171 | 159 | 152 | 159 | 159 | 158 | 154 | 151 | 156 | 165 | 166 | 162 | 164 | 160 | 158 | 160 | . 292 168 | 170 | 173 | 173 | 169 | 160 | 148 | 140 | 142 | 146 | 149 | 149 | 147 | 147 | 148 | 150 | 144 | 146 | 147 | 145 | 143 | 142 | 142 | 140 | 145 | 142 | 142 | 145 | 147 | 146 | 147 | 149 | 150 | 146 | 144 | 145 | 143 | 141 | 143 | 148 | 146 | 144 | 147 | 148 | 145 | 145 | 147 | 145 | 141 | 142 | 143 | 144 | 144 | 144 | 143 | 143 | 145 | 148 | 150 | 150 | 150 | 150 | 149 | 147 | 145 | 150 | 151 | 149 | 154 | 159 | 153 | 141 | 139 | 146 | 151 | 151 | 149 | 151 | 155 | 157 | 158 | 161 | 155 | 147 | 151 | 161 | 164 | 163 | 165 | 153 | 147 | 153 | 158 | 157 | 158 | 163 | 167 | 164 | 161 | 162 | . 293 168 | 170 | 173 | 176 | 178 | 176 | 169 | 161 | 147 | 144 | 142 | 144 | 149 | 152 | 150 | 146 | 145 | 146 | 146 | 144 | 144 | 144 | 143 | 141 | 146 | 144 | 144 | 146 | 146 | 144 | 140 | 139 | 142 | 143 | 146 | 148 | 148 | 147 | 147 | 149 | 145 | 144 | 148 | 151 | 147 | 145 | 144 | 142 | 139 | 141 | 144 | 144 | 144 | 143 | 144 | 144 | 143 | 145 | 147 | 148 | 149 | 150 | 149 | 147 | 147 | 148 | 148 | 148 | 151 | 156 | 156 | 153 | 144 | 144 | 143 | 142 | 144 | 149 | 151 | 152 | 154 | 154 | 155 | 157 | 154 | 147 | 153 | 169 | 163 | 159 | 158 | 159 | 158 | 154 | 154 | 158 | 166 | 163 | 162 | 164 | . 294 156 | 160 | 162 | 163 | 167 | 174 | 180 | 183 | 181 | 167 | 151 | 143 | 144 | 146 | 146 | 143 | 148 | 147 | 145 | 143 | 144 | 147 | 147 | 145 | 148 | 148 | 147 | 146 | 145 | 144 | 140 | 135 | 132 | 137 | 141 | 143 | 143 | 143 | 144 | 143 | 145 | 143 | 147 | 151 | 148 | 145 | 145 | 143 | 139 | 142 | 144 | 143 | 141 | 140 | 142 | 145 | 143 | 145 | 145 | 145 | 146 | 148 | 149 | 148 | 148 | 145 | 146 | 152 | 153 | 150 | 152 | 157 | 154 | 148 | 141 | 139 | 144 | 149 | 147 | 142 | 143 | 151 | 152 | 149 | 156 | 163 | 156 | 146 | 157 | 162 | 163 | 160 | 158 | 158 | 157 | 154 | 160 | 159 | 159 | 161 | . 295 158 | 159 | 159 | 158 | 160 | 165 | 169 | 170 | 173 | 177 | 179 | 173 | 161 | 150 | 147 | 148 | 149 | 147 | 142 | 140 | 142 | 147 | 148 | 146 | 146 | 147 | 146 | 144 | 146 | 149 | 148 | 144 | 148 | 149 | 145 | 138 | 136 | 140 | 145 | 146 | 145 | 142 | 145 | 148 | 146 | 145 | 148 | 148 | 143 | 146 | 147 | 144 | 140 | 138 | 142 | 145 | 145 | 145 | 145 | 144 | 145 | 148 | 150 | 149 | 147 | 150 | 149 | 145 | 146 | 152 | 154 | 150 | 154 | 154 | 152 | 148 | 147 | 147 | 148 | 147 | 142 | 147 | 146 | 143 | 148 | 153 | 156 | 159 | 158 | 151 | 152 | 161 | 164 | 159 | 157 | 161 | 157 | 156 | 157 | 158 | . 296 154 | 156 | 156 | 156 | 155 | 156 | 158 | 161 | 164 | 166 | 169 | 174 | 179 | 179 | 171 | 162 | 147 | 148 | 150 | 149 | 144 | 140 | 143 | 148 | 146 | 145 | 145 | 147 | 147 | 146 | 143 | 141 | 147 | 146 | 145 | 145 | 143 | 140 | 134 | 129 | 140 | 143 | 146 | 146 | 144 | 144 | 147 | 150 | 143 | 144 | 143 | 141 | 139 | 139 | 137 | 135 | 140 | 145 | 147 | 142 | 139 | 142 | 147 | 149 | 144 | 147 | 150 | 150 | 149 | 148 | 149 | 151 | 149 | 153 | 154 | 152 | 149 | 148 | 147 | 146 | 144 | 144 | 146 | 148 | 148 | 146 | 147 | 150 | 169 | 159 | 149 | 148 | 154 | 161 | 161 | 159 | 161 | 159 | 160 | 163 | . 297 155 | 157 | 158 | 157 | 156 | 157 | 159 | 161 | 164 | 164 | 163 | 164 | 169 | 175 | 178 | 178 | 164 | 152 | 142 | 142 | 148 | 151 | 147 | 143 | 146 | 148 | 146 | 141 | 141 | 145 | 147 | 146 | 146 | 145 | 145 | 146 | 147 | 146 | 145 | 143 | 135 | 138 | 140 | 141 | 142 | 145 | 149 | 153 | 146 | 145 | 142 | 138 | 137 | 139 | 141 | 141 | 144 | 141 | 140 | 141 | 144 | 145 | 145 | 144 | 144 | 145 | 147 | 147 | 147 | 146 | 146 | 147 | 150 | 150 | 149 | 149 | 152 | 154 | 152 | 147 | 146 | 145 | 145 | 147 | 146 | 145 | 147 | 150 | 146 | 152 | 156 | 155 | 151 | 150 | 154 | 158 | 157 | 162 | 163 | 160 | . 298 161 | 162 | 162 | 162 | 161 | 161 | 163 | 164 | 162 | 163 | 163 | 161 | 160 | 163 | 166 | 167 | 180 | 177 | 171 | 160 | 148 | 141 | 144 | 150 | 142 | 143 | 143 | 142 | 140 | 140 | 143 | 146 | 148 | 148 | 147 | 147 | 147 | 147 | 147 | 148 | 141 | 141 | 141 | 141 | 141 | 143 | 146 | 148 | 147 | 146 | 142 | 138 | 137 | 139 | 142 | 142 | 147 | 140 | 138 | 143 | 147 | 145 | 142 | 141 | 145 | 145 | 146 | 146 | 146 | 146 | 146 | 146 | 148 | 148 | 147 | 148 | 152 | 155 | 154 | 151 | 148 | 147 | 147 | 147 | 147 | 147 | 150 | 154 | 145 | 149 | 155 | 157 | 156 | 155 | 155 | 156 | 154 | 158 | 159 | 156 | . 299 157 | 158 | 158 | 158 | 157 | 156 | 157 | 158 | 157 | 161 | 163 | 163 | 162 | 164 | 165 | 165 | 157 | 162 | 168 | 171 | 167 | 158 | 150 | 145 | 137 | 139 | 145 | 149 | 142 | 131 | 133 | 142 | 145 | 146 | 146 | 145 | 144 | 144 | 144 | 144 | 149 | 147 | 145 | 144 | 143 | 143 | 144 | 144 | 147 | 148 | 147 | 144 | 143 | 143 | 142 | 140 | 143 | 143 | 144 | 146 | 144 | 141 | 140 | 141 | 144 | 144 | 144 | 145 | 146 | 147 | 147 | 147 | 145 | 148 | 150 | 149 | 147 | 148 | 151 | 154 | 151 | 151 | 150 | 149 | 148 | 148 | 150 | 152 | 156 | 152 | 150 | 153 | 159 | 163 | 161 | 158 | 155 | 151 | 150 | 155 | . - 결국 이 이미지는 683 $ times$ 1024 개의 숫자의 모임 . - 이 이미지를 벡터로 만든다음 히스토그램을 그려보자. . img.flatten().shape . (699392,) . fig1=plt.hist(img.flatten(),256,[0,256]) . - 히스토그램을 그려보니 120~200 사이에 너무 값들이 모여있음 . - 원래 0~255까지의 색을 표현할 수 있는데 컴퓨터가 표현가능한 색상보다 적은 조합만을 사용하고 있음. . - 아이디어: 좀 더 많은 색상을 표현할 수 없을까? $ to$ 위의 히스토그램은 좀 평평하게 만들면 되지 않을까? . img2=cv.equalizeHist(img) . fig2_1=plt.hist(img2.flatten(),256,[0,256]) # 256개의 구간으로 히스토그램 만들어 . fig2_2=plt.hist(img2.flatten(),10,[0,256]) . plt.imshow(img2,cmap=&#39;gray&#39;,vmin=0,vmax=255) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7f6a6e40d1f0&gt; . - 변환전과 변환후를 나란히 보게 되면? . import numpy as np . _img=np.hstack((img,img2)) . plt.imshow(_img,cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f6a6967c760&gt; . &#49689;&#51228;2 . - 아래 이미지를 HE(histogram equalization)로 보정하고 스샷제출 . ref: https://ukdevguy.com/histogram-equalization-in-python/ | . hw_img=cv.imread(&#39;hw_img.png&#39;,0) plt.imshow(hw_img,cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f6a69661df0&gt; . - 이미지는 https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/hw_img.png 에서 다운받을 수 있다. . hw_img2=cv.equalizeHist(hw_img) . plt.imshow(hw_img2,cmap=&#39;gray&#39;,vmin=0,vmax=255) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7f6a69572b20&gt; . _img_hw=np.hstack((hw_img,hw_img2)) . plt.imshow(_img_hw,cmap=&#39;gray&#39;) plt.colorbar() . &lt;matplotlib.colorbar.Colorbar at 0x7f6a694ad280&gt; .",
            "url": "https://seoyeonc.github.io/chch/2021/09/13/%EB%8D%B0%EC%8B%9C_2%EC%A3%BC%EC%B0%A8.html",
            "relUrl": "/2021/09/13/%EB%8D%B0%EC%8B%9C_2%EC%A3%BC%EC%B0%A8.html",
            "date": " • Sep 13, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . (_pages/about에서 수정) . 학력 . 학사…석사…. . 경력 . 어디..ㅇ디.. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://seoyeonc.github.io/chch/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://seoyeonc.github.io/chch/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}