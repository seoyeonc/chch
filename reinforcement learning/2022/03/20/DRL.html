<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>(not_done_review)그로킹 심층 강화학습 | Seoyeon Choi</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="(not_done_review)그로킹 심층 강화학습" />
<meta name="author" content="최서연" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="강찬석" />
<meta property="og:description" content="강찬석" />
<link rel="canonical" href="https://seoyeonc.github.io/chch/reinforcement%20learning/2022/03/20/DRL.html" />
<meta property="og:url" content="https://seoyeonc.github.io/chch/reinforcement%20learning/2022/03/20/DRL.html" />
<meta property="og:site_name" content="Seoyeon Choi" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-20T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"강찬석","url":"https://seoyeonc.github.io/chch/reinforcement%20learning/2022/03/20/DRL.html","@type":"BlogPosting","headline":"(not_done_review)그로킹 심층 강화학습","dateModified":"2022-03-20T00:00:00-05:00","datePublished":"2022-03-20T00:00:00-05:00","author":{"@type":"Person","name":"최서연"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://seoyeonc.github.io/chch/reinforcement%20learning/2022/03/20/DRL.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/chch/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://seoyeonc.github.io/chch/feed.xml" title="Seoyeon Choi" /><link rel="shortcut icon" type="image/x-icon" href="/chch/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/chch/">Seoyeon Choi</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/chch/about/">About Me</a><a class="page-link" href="/chch/search/">Search</a><a class="page-link" href="/chch/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">(not_done_review)그로킹 심층 강화학습</h1><p class="page-description">강찬석</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-03-20T00:00:00-05:00" itemprop="datePublished">
        Mar 20, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">최서연</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/chch/categories/#Reinforcement Learning">Reinforcement Learning</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/seoyeonc/chch/tree/master/_notebooks/2022-03-20-DRL.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/chch/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/seoyeonc/chch/master?filepath=_notebooks%2F2022-03-20-DRL.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/chch/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/seoyeonc/chch/blob/master/_notebooks/2022-03-20-DRL.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/chch/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#$\star$-목표:-일주일에-몇-장씩이라도-보기!">$\star$ 목표: 일주일에 몇 장씩이라도 보기! </a>
<ul>
<li class="toc-entry toc-h3"><a href="#1">1 </a></li>
<li class="toc-entry toc-h3"><a href="#2">2 </a></li>
<li class="toc-entry toc-h3"><a href="#3">3 </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-03-20-DRL.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="$\star$-목표:-일주일에-몇-장씩이라도-보기!">
<a class="anchor" href="#%24%5Cstar%24-%EB%AA%A9%ED%91%9C:-%EC%9D%BC%EC%A3%BC%EC%9D%BC%EC%97%90-%EB%AA%87-%EC%9E%A5%EC%94%A9%EC%9D%B4%EB%9D%BC%EB%8F%84-%EB%B3%B4%EA%B8%B0!" aria-hidden="true"><span class="octicon octicon-link"></span></a>$\star$ 목표: 일주일에 몇 장씩이라도 보기!<a class="anchor-link" href="#%24%5Cstar%24-%EB%AA%A9%ED%91%9C:-%EC%9D%BC%EC%A3%BC%EC%9D%BC%EC%97%90-%EB%AA%87-%EC%9E%A5%EC%94%A9%EC%9D%B4%EB%9D%BC%EB%8F%84-%EB%B3%B4%EA%B8%B0!"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ref: <a href="https://goodboychan.github.io/book">https://goodboychan.github.io/book</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1">
<a class="anchor" href="#1" aria-hidden="true"><span class="octicon octicon-link"></span></a>1<a class="anchor-link" href="#1"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>심층강화학습 deep reinforcement learning DRL 이란 머신 러닝 기법 중 하나.</p>
<ul>
<li>지능이 요구되는 문제를 해결할 수 있도록 인공지능 컴퓨터 프로그램을 개발하는데 사용</li>
<li>시행착오를 통해 얻은 반응을 학습</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>심층 강화학습은 문제에 대한 접근법이다.</p>
<ul>
<li>
<strong>에이전트agent</strong>: 의사를 결정하는 객체 자체<ul>
<li>ex) 사물을 집는 로봇 학습시킬때 의사 결정을 좌우하는 코드와 연관</li>
</ul>
</li>
<li>
<strong>환경unvironmnet</strong>: 에이전트(의사 결정) 이외의 모든 것<ul>
<li>ex) 의사결정하는 로봇(객체) 제외한 모든 것이 환경의 일부 </li>
</ul>
</li>
<li>
<strong>싱태 영역state space</strong>: 변수가 가질 수 있는 모든 값들의 집합</li>
<li>
<strong>관찰observation</strong>: 에이전트가 관찰할 수 있는 상태의 일부</li>
<li>
<strong>전이 함수transition function</strong> 에이전트와 환경 사이의 관계를 정의한 함수</li>
<li>
<strong>보상 함수reward function</strong>: 행동에 대한 반응으로 환경이 제공한 보상 신호와 관련된 함수</li>
<li>
<strong>모델model</strong>: 전이와 보상함수를 표현 가능</li>
</ul>
<p>에이전트의 3단계 과정</p>
<ol>
<li>환경과 상호작용을 나누고</li>
<li>행동에 대해서 평가를 하며,</li>
<li>받은 반응을 개선시킨다.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
<strong>에피소드형 업무episodic task</strong>: 게임과 같이 자연적으로 끝나는 업무</li>
<li>
<p><strong>연속형업무continuing task</strong>: 앞으로 가는 동작을 학습하는 경우</p>
</li>
<li>
<p>연속형 피드백이 야기하는 문제</p>
<ul>
<li>
<strong>시긴적 가치 할당 문제tamporal credit assignment problem</strong>:문제에 시간적 개념이 들어가 있고, 행동에도 지연된 속성이 담겨 있으면, 보상에 대한 가치를 부여하기 어렵다. </li>
</ul>
</li>
<li>평가 가능한 피드백이 야기하는 문제<ul>
<li>
<strong>탐험과 착취 간의 트레이드 오프exploration versus explotation trade-off</strong>: 에이전트는 현재 가지고 있는 정보에서 얻을 수 있는 가장 좋은 것과 정보를 새로 얻는 것 간의 균형을 맞출 수 있어야 한다. </li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2">
<a class="anchor" href="#2" aria-hidden="true"><span class="octicon octicon-link"></span></a>2<a class="anchor-link" href="#2"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>에이전트의 세 단계 과정</p>
<ol>
<li>모든 에이전트는 <em>상호작용</em> 요소를 가지고 있고, 학습에 필요한 데이터를 수집한다.</li>
<li>모든 에이전트들은 현재 취하고 있는 행동을 <em>평가</em>하고, </li>
<li>전체적인 성능을 개선하기 위해 만들어진 무언가를 <em>개선</em>한다.</li>
</ol>
<ul>
<li>
<em>상태 영역state space</em>: 표현할 수 있는 변수들의 모든 값에 대한 조합</li>
<li>
<em>관찰observation</em>: 에이전트가 어떤 특정 시간에 얻을 수 있는 변수들의 집합</li>
<li>
<em>관찰 영역observation space</em>: 변수들이 가질 수 있는 모든 값들의 조합</li>
<li>
<em>행동 영역action space</em>: 모든 상태에서 취할 수 있는 모든 행동에 대한 집합</li>
<li>
<em>전이 함수transition function</em>: 환경은 에이전트의 행동에 대한 반응으로 상태를 변화할 수 있는데 이와 관련된 함수</li>
<li>
<em>보상 함수reward function</em>: 행동과 관련된 보상에 대한 함수</li>
<li>
<em>모델mpdel</em>: 전이와 보상 함수에 </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>보상 신호가 밀집되어 있을수록, 에이전트의 직관성과 학습속도가 높아지지만, 에이전트에게 평견을 주입하게 되어, 결국 에이전트가 예상하지 못한 행동을 할 가능성은 적어지게 된다. 반면, 보상 신호가 적을수록 직관성이 낮아져 에이전트가 새로운 행동을 취할 확률이 높아지지만, 그만큼 에이전트르르 학습시키는데 오래 걸리게 될 것이다.</p>
<ul>
<li>
<em>타임 스텝time step</em>: 상호작용이 진행되는 사이클, 시간의 단위</li>
<li>
<em>경험 튜플experience tuple</em>: 관찰 또는 상태, 행동, 보상 그리고 새로운 관찰</li>
<li>
<em>에피소드형 업무episodic task</em>: 게임과 같이 자연적으로 끝나는 업무 - <em>에피소드episode</em>
</li>
<li>
<em>연속형 업무continuing task</em>: 앞으로 전진하는 동작과 같이 자연적으로 끝나는 업무</li>
<li>
<em>반환값return</em>: 에피소드 동안 수집된 보상의 총합</li>
<li>
<em>상태state</em>: 문제에 포함되어 있는 독특하고, 자기 자신만의 설정이 담긴 요소</li>
<li>
<em>상태 영역state square</em>: 모든 가능한 상태, 집합 S로 표현,</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>마르코프 결정 과정Markov decision process MDP</strong></p>
<ul>
<li>수학 프레임워크, 이를 이용해서 강화학습 환경을 연속적인 의사결정 문제로 표현하는 방법 학습.</li>
<li>일반적인 형태는 불확실성상에 놓여진 상황에서 어떠한 복잡한 연속적 결정을 가상으로 모델링해서, 강화학습 에이전트가 모델과 반응을 주고받고, 경험을 통해서 스스로 학습할 수 있게 해준다.</li>
<li>모든 상태들의 집합S</li>
<li>모든 <em>시작 상태starting state</em> 혹은 <em>초기 상태initial state</em>라고 부르는 S+의 부분집합이 있음</li>
<li>MDP와의 상호작용 시작하면서 어떤 S에서의 특정 상태에서 어떤 확률 붙포 간의 관계를 그릴 수 있는데 이때 이 확률 분포는 무엇이든 될 수 있지만, 학습이 이뤄지는 동안에는 고정되어 있어야 한다.<ul>
<li>즉, 이 확률 분포에서 샘플링된 확률은 학습과 에이전트 검증의 처름 에피소드부터 마지막 에피소드까지는 항상 동일해야 한다는 것</li>
</ul>
</li>
<li>
<em>흡수absorbing</em> 또는 <em>종료 상태terminal state</em> 라는 특별한 상태도 존재.<ul>
<li>종료 상태(꼭 하나는 아님) 이외의 모든 상태들을 S라고 표현</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>MDP에서는</p>
<ul>
<li>
<strong>A</strong>: 상태에 따라 결정되는 행동의 집합<ul>
<li>$\therefore$ 특정 상태에서 허용되지 않는 행동도 존재한다는 말.</li>
<li>상태(s)를 인자로 받는 함수.</li>
<li>A(s); 상태(s)에서 취할 수 있는 행동들의 집합 반환, 상수 정의 가능.</li>
</ul>
</li>
<li>행동 영역은 유한 혹은 무한 가능,<ul>
<li>단일 행동에 대한 변수들의 집합은 한 개 이상의 료소를 가질 수 있고, 유한해야 함.</li>
<li>$\therefore$ 대부분의 환경에서 모든 상태에서 취할 수 있는 행동의 수는 같도록 설계됨</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>상태-전이 확률state-transition probability</em> = <em>전이 함수</em></p>
<ul>
<li>$T(s,a,s')$</li>
<li>전이함수$T$는 각 전이 튜플인 $(s,a,s')$을 확률과 연결시켜준다.</li>
<li>어떤 상태 s에서 행동 a를 취하고 다음 상태가 s'이 되었을 때, 이때의 확률을 반환해준다는 뜻</li>
<li>이 확률의 합은 1

$$p(s'|s,a) = P(S_t = s'|S_{t-1} = s,A_{t-1}=a)$$


$$\sum_{s' \in S}p(s'|s,a) = a, \forall s \in S, \forall a \in A(s)$$
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>보상 함수$R$</em></p>
<ul>
<li>전이 튜플 $s,a,s'$을 특정한 스칼라 값으로 매핑,<ul>
<li>양수 보상 = 수익 또는 이득</li>
<li>음수 보상 = 비용, 벌칙, 패널티</li>
</ul>
</li>
<li>$R(s,a,s')$ = $R(s,a)$ = $R(s)$</li>
<li>
<em>보상 함수를 표현하는 가장 명확한 방법은 상태와 행동 그리고 다음 상태, 이 세 가지를 함께 쓰는 것</em>

$$r(s,a) = \mathbb{E} [R_t|S_{t-1} = s,A_{t-1} = a]$$


$$r(s,a,s') = \mathbb{E} [R_t|S_{t-1} = s, A_{t-1} = a, S_t= s']$$


$$R_t \in \cal{R} \subset \mathbb{R}$$
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>호라이즌horixon</em></p>
<ul>
<li>계획 호라이즌planning horizon: 에피소드형 업무나 연속적 업무를 에이전트의 관점에서 정의<ul>
<li>유한 호라이즌finite horizon: 에이전트가 유한한 타임 스템 내에 업무가 종료된다는 것을 알고 있는 계획 호라이즌</li>
<li>탐욕 호라이즌greedy horizon: 계획 호라이즌이 1인 경우</li>
<li>무한 호라이즌infinite horiaon 에이전트한테 미리 정의된 타임 스텝에 대한 제한이 없어 에이전트가 무한하게 계획할 수 있음<ul>
<li>특히, 무한한 계획 호라이즌을 가지는 업무는 무기한 호라이즌 업무indefinite horizon task 라고 부름</li>
</ul>
</li>
</ul>
</li>
<li>에이전트가 타임 스텝 루프에 빠지는 것을 막기 위해 타임 스텝을 제한하기도 한다.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>감가율discount factor  = 감마gamma</em></p>
<ul>
<li>받았던 보상의 시점이 미래로 더 멀어질수록, 현재 시점에서는 이에 대한 가치를 더 낮게 평가

$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots R_T$$


$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots +\gamma^{T-1} R_T$$


$$G_t = \sum^{\infty}_{k=0} \gamma^k R_{t+k+1}$$


$$G_t = R_{t+1}+ \gamma G_{t+1}$$
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3">
<a class="anchor" href="#3" aria-hidden="true"><span class="octicon octicon-link"></span></a>3<a class="anchor-link" href="#3"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>에이전트의 목표: 반환값(보상의 총합)을 최대화할 수 있는 행동의 집합을 찾는 것, 정책이 필요하다.</p>
<ul>
<li>
<em>정책policy</em>: 가능한 모든 상태를 포괄한 전체적인 계획.</li>
<li>확률적 혹은 결정적</li>
<li>정책을 비교하기 위해 시작 상태를 포함한 모든 상태들에 대해 기대 반환값을 계산할 수 있어야 한다.</li>
</ul>
<ul>
<li>정책$\pi$를 수행할 때, 상태 s에 대한 가치를 정의할 수 있다.<ul>
<li>에이전트가 정책 $\pi$를 따르면서 상태 s에서 시작했을 때, 상태 s의 가치는 반환값의 기대치라고 할 수 있다.</li>
<li>가치함수는 상태 s에서 정책$\pi$를 따르면서 시작했을 때의 반환값에 대한 기대치를 나타낸다.

$$v_{\pi} (s) = \mathbb{E}_{\pi} [G_t|S_t = s]$$


$$v_{\pi} (s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{+3} + \dots |S_t = s]$$


$$v_{\pi} (s) = \sum_a \pi (a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma v_\pi (s')] \forall s \in S$$
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>행동-가치 함수action-value function</em> = Q 함수 = $Q^{\pi}(s,a)$</p>
<ul>
<li>상태 s에서 행동 a를 취했을 때, 에이전트가 정책$\pi$를 수행하면서 얻을 수 있는 기대 반환값</li>
<li>MDP 없이도 정책을 개선시킬 수 있게 해준다.

$$q_{\pi} (s,a) = \mathbb{E}_{\pi} [G_t|S_t = s,A_t = a]$$


$$q_{\pi}(s,a) = \mathbb{E}_{\pi}[R_t + \gamma G_{t+1} | S_t = s, A_t = a]$$


$$q_{\pi}(s,a) = \sum_{s',r} p(s',r|s,a)[r+\gamma v_{\pi} (s')], \forall s \in S, \forall a \in A(s)$$
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>행동-이점 함수action-advamtage function</em> = 이점함수advantage function = $A$</p>
<ul>
<li>상태 s에서 행동를 취했을 때의 가치와 정책 $\pi$에서 상태 s에 대한 상태-가치 함수 간의 차이

$$a_{\pi}(s,a) = q_{\pi}(s,a) - v_{\pi}(s)$$
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>이상적인 정책optimal policy</p>
<ul>
<li>모든 상태에 대해서 다른 정책들보다 기대 반환값이 같거나 더 크게 얻을 수 있는 정책

$$v_{*}(s) = max_{\pi} v_{\pi} (s), \forall_s \in S$$


$$q_{*}(s,a) = max_{\pi} q_{\pi}(s,a), \forall s \in S, \forall a \in A(s)$$


$$v_{*}(s) = max_{a} \sum_{s',r} p(s',r|s,a)[r+\gamma v_{*} (s')]$$


$$q_{*}(s,a) = \sum_{s',r}p(s',r|s,a) [r + \gamma max_{a'} q_{*}(s',a')]$$
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>p.114</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/chch/reinforcement%20learning/2022/03/20/DRL.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/chch/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/chch/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/chch/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/seoyeonc" target="_blank" title="seoyeonc"><svg class="svg-icon grey"><use xlink:href="/chch/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
