{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a731bb-d55c-411d-87f8-f6c782603246",
   "metadata": {
    "id": "cac470df-29e7-4148-9bbd-d8b9a32fa570",
    "tags": []
   },
   "source": [
    "# (not_done_review)그로킹 심층 강화학습\n",
    "> 강찬석\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: 최서연\n",
    "- categories: [Reinforcement Learning]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6bfdb-de3e-4f44-8936-9297c79d6356",
   "metadata": {},
   "source": [
    "# $\\star$ 목표: 일주일에 몇 장씩이라도 보기!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fd0aa-0a6e-4dda-a5ed-bc6c93cda730",
   "metadata": {},
   "source": [
    "ref: https://goodboychan.github.io/book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227222c9-74ff-4207-a917-1216fafbf433",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e663b-943e-4eda-b806-717edf24908e",
   "metadata": {},
   "source": [
    "심층강화학습 deep reinforcement learning DRL 이란 머신 러닝 기법 중 하나.\n",
    "- 지능이 요구되는 문제를 해결할 수 있도록 인공지능 컴퓨터 프로그램을 개발하는데 사용\n",
    "- 시행착오를 통해 얻은 반응을 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a885a96-06a3-4b7b-a777-ad43f8e7c498",
   "metadata": {},
   "source": [
    "심층 강화학습은 문제에 대한 접근법이다.\n",
    "- **에이전트agent**: 의사를 결정하는 객체 자체\n",
    "    - ex) 사물을 집는 로봇 학습시킬때 의사 결정을 좌우하는 코드와 연관\n",
    "- **환경unvironmnet**: 에이전트(의사 결정) 이외의 모든 것\n",
    "    - ex) 의사결정하는 로봇(객체) 제외한 모든 것이 환경의 일부 \n",
    "- **싱태 영역state space**: 변수가 가질 수 있는 모든 값들의 집합\n",
    "- **관찰observation**: 에이전트가 관찰할 수 있는 상태의 일부\n",
    "- **전이 함수transition function** 에이전트와 환경 사이의 관계를 정의한 함수\n",
    "- **보상 함수reward function**: 행동에 대한 반응으로 환경이 제공한 보상 신호와 관련된 함수\n",
    "- **모델model**: 전이와 보상함수를 표현 가능\n",
    "\n",
    "에이전트의 3단계 과정\n",
    "1. 환경과 상호작용을 나누고\n",
    "2. 행동에 대해서 평가를 하며,\n",
    "3. 받은 반응을 개선시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc58edf-5115-4c33-ab82-38db6699e40a",
   "metadata": {},
   "source": [
    "- **에피소드형 업무episodic task**: 게임과 같이 자연적으로 끝나는 업무\n",
    "- **연속형업무continuing task**: 앞으로 가는 동작을 학습하는 경우\n",
    "\n",
    "- 연속형 피드백이 야기하는 문제\n",
    "    - **시긴적 가치 할당 문제tamporal credit assignment problem**:문제에 시간적 개념이 들어가 있고, 행동에도 지연된 속성이 담겨 있으면, 보상에 대한 가치를 부여하기 어렵다. \n",
    "- 평가 가능한 피드백이 야기하는 문제\n",
    "    - **탐험과 착취 간의 트레이드 오프exploration versus explotation trade-off**: 에이전트는 현재 가지고 있는 정보에서 얻을 수 있는 가장 좋은 것과 정보를 새로 얻는 것 간의 균형을 맞출 수 있어야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98653189-f50f-49d4-a114-3410e7627488",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e407f-5d1d-426e-ae60-47f7c6f43fbc",
   "metadata": {},
   "source": [
    "에이전트의 세 단계 과정\n",
    "1. 모든 에이전트는 *상호작용* 요소를 가지고 있고, 학습에 필요한 데이터를 수집한다.\n",
    "2. 모든 에이전트들은 현재 취하고 있는 행동을 *평가*하고, \n",
    "3. 전체적인 성능을 개선하기 위해 만들어진 무언가를 *개선*한다.\n",
    "\n",
    "- *상태 영역state space*: 표현할 수 있는 변수들의 모든 값에 대한 조합\n",
    "- *관찰observation*: 에이전트가 어떤 특정 시간에 얻을 수 있는 변수들의 집합\n",
    "- *관찰 영역observation space*: 변수들이 가질 수 있는 모든 값들의 조합\n",
    "- *행동 영역action space*: 모든 상태에서 취할 수 있는 모든 행동에 대한 집합\n",
    "- *전이 함수transition function*: 환경은 에이전트의 행동에 대한 반응으로 상태를 변화할 수 있는데 이와 관련된 함수\n",
    "- *보상 함수reward function*: 행동과 관련된 보상에 대한 함수\n",
    "- *모델mpdel*: 전이와 보상 함수에 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5d7bc-51eb-4b9b-a685-f9868c16da75",
   "metadata": {},
   "source": [
    "보상 신호가 밀집되어 있을수록, 에이전트의 직관성과 학습속도가 높아지지만, 에이전트에게 평견을 주입하게 되어, 결국 에이전트가 예상하지 못한 행동을 할 가능성은 적어지게 된다. 반면, 보상 신호가 적을수록 직관성이 낮아져 에이전트가 새로운 행동을 취할 확률이 높아지지만, 그만큼 에이전트르르 학습시키는데 오래 걸리게 될 것이다.\n",
    "\n",
    "- *타임 스텝time step*: 상호작용이 진행되는 사이클, 시간의 단위\n",
    "- *경험 튜플experience tuple*: 관찰 또는 상태, 행동, 보상 그리고 새로운 관찰\n",
    "- *에피소드형 업무episodic task*: 게임과 같이 자연적으로 끝나는 업무 - *에피소드episode*\n",
    "- *연속형 업무continuing task*: 앞으로 전진하는 동작과 같이 자연적으로 끝나는 업무\n",
    "- *반환값return*: 에피소드 동안 수집된 보상의 총합\n",
    "- *상태state*: 문제에 포함되어 있는 독특하고, 자기 자신만의 설정이 담긴 요소\n",
    "- *상태 영역state square*: 모든 가능한 상태, 집합 S로 표현,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c159436-e5de-44bc-b4af-4f512e0ecff9",
   "metadata": {},
   "source": [
    "MDP\n",
    "- 모든 상태들의 집합S\n",
    "- 모든 *시작 상태*starting state* 혹은 *초기 상태in itial state*라고 부르는 S+의 부분집합이 있음\n",
    "- MDP와의 상호작용 시작하면서 어떤 S에서의 특정 상태에서 어떤 확률 붙포 간의 관계를 그릴 수 있는데 이때 이 확률 분포는 무엇이든 될 수 있지만, 학습이 이뤄지는 동안에는 고정되어 있어야 한다.\n",
    "    - 즉, 이 확률 분포에서 샘플링된 확률은 학습과 에이전트 검증의 처름 에피소드부터 마지막 에피소드까지는 항상 동일해야 한다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1337d7-8b38-4f6f-ba78-d412acbfb97b",
   "metadata": {},
   "source": [
    "p.83"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de381d58-213e-4111-8a94-057819aa3585",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f43dfbff-36d5-46a4-afcb-66c35202093f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6926fbd9-ea72-4dde-b2a4-ff49c9da8b77",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da4aa89a-708a-49fc-b55a-50b0cfcb8d4a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
