{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a327a00e-3d5a-4562-86ad-3bf83eb54fe4",
   "metadata": {
    "id": "d44ba9bc-ebf3-49d8-b2db-9d49bf1bec22",
    "tags": []
   },
   "source": [
    "# Wasserstein GAN\n",
    "> Martin Arjovsky(Courant Institute of Mathematical Sciences), Soumith Chintala(Facebook AI Research), and L´eon Bottou1(Courant Institute of Mathematical Sciences, Facebook AI Research)\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false \n",
    "- author: 최서연\n",
    "- categories: [Wasserstein GAN, GAN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24ed9b-b50c-4ca2-bcae-c57a33e17432",
   "metadata": {},
   "source": [
    "ref: https://arxiv.org/pdf/1701.07875.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ab002-cfde-430c-a34f-f90c7f35cefd",
   "metadata": {},
   "source": [
    "https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f55c08-cda0-4249-abae-e2dc1108e54d",
   "metadata": {},
   "source": [
    "## Different Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426c533-2fc9-4042-a089-4da4bf268d94",
   "metadata": {},
   "source": [
    "- Let $X$ be a compact metric set (such as the space of images $[0, 1]^d$) \n",
    "- and let $Σ$ denote the set of all the Borel subsets of $X$ . \n",
    "- Let $Prob(X )$ denote the space of probability measures defined on $X$ .\n",
    "- We can now define elementary distances and divergences between two distributions $P_r, P_g ∈ Prob(X )$\n",
    "    - The Total Variation (TV) distance, The Kullback-Leibler (KL) divergence, The Jensen-Shannon (JS) divergence, The Earth-Mover (EM) distance or Wasserstein-1이 사용될 것.\n",
    "    - p값들은 절대적으로 연속!\n",
    "    - 그러므로 카이제곱에서 정의되는 같은 측정치 뮤가 나옴.\n",
    "- The following example illustrates how apparently simple sequences of probability distributions converge under the EM distance but do not converge under the other distances and divergences defined above\n",
    "    - 다음 예시에서 보기에 단순한 확률 분포 시퀀스가 EM 거리에서는 수렴하지만 제시된 4개의 거리 및 발산에서는 수렴하지 않음을 보임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45c146-5128-4da5-a07e-7a50de90f9cf",
   "metadata": {},
   "source": [
    "*Example 1 (Learning parallel lines)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecaa6d3-dd0c-40c3-bdf4-ad86c41af11e",
   "metadata": {},
   "source": [
    "- Let $Z ∼ U[0, 1]$ the uniform distribution on the unit interval.\n",
    "- Let $P_0$ be the distribution of $(0, Z) ∈ R^2$ (a 0 on the x-axis and the random variable Z on the y-axis), uniform on a straight vertical line passing through the origin.\n",
    "- Now let $g_θ(z) = (θ, z)$ with $θ$ a single real parameter. \n",
    "- It is easy to see that in this case,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172079f-939c-4817-8eef-93e42bd27d97",
   "metadata": {},
   "source": [
    "- When $θ_t → 0$, the sequence ($P_{θ_t})_{t∈N}$ converges to $P_0$ under the EM distance, but does not converge at all under either the JS, KL, reverse KL, or TV divergences.\n",
    "- Figure 1 illustrates this for the case of the EM and JS distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2121aa2-4004-4fba-ac4b-6d93242ee5d1",
   "metadata": {},
   "source": [
    "- Example 1 gives us a case where we can learn a probability distribution over a low dimensional manifold by doing gradient descent on the EM distance. \n",
    "- This cannot be done with the other distances and divergences because the resulting loss function is not even continuous.\n",
    "    - 결과 손실 함수가 연속적이지 않기 때문에 다른 거리와 발산으로 할 수 없음.\n",
    "- Although this simple example features distributions with disjoint supports, the same conclusion holds when the supports have a non empty intersection contained in a set of measure zero.\n",
    "    - disjoint suppoert를 가진 분포를 특장으로 하는 이 단순한 예시는 supports가 0 집합에 포함된 비어 있지 않은 interaction을 가진 때에도 동일한 결과가 유지된다.\n",
    "- This happens to be the case when two low dimensional manifolds intersect in general position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f17bb5-4179-4a48-81f0-106e4c3f4277",
   "metadata": {},
   "source": [
    "![](https://xiucheng.org/assets/images/wgan-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95f4a5-029f-45f9-88f5-01415ca1bede",
   "metadata": {},
   "source": [
    "Since the Wasserstein distance is much weaker than the JS distance , we can now ask whether $W(P_r, P_θ)$ is a continuous loss function on $θ$ under mild assumptions. This, and more, is true, as we now state and prove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf478f23-911a-440e-8a8a-ccd4d9d6c018",
   "metadata": {},
   "source": [
    "*Theorem 1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce2cf7-3cee-4e97-b194-3f83278dd5f3",
   "metadata": {},
   "source": [
    "- Let $P_r$ be a fixed distribution over $X$ .\n",
    "- Let $Z$ be a random variable(e.g Gaussian) over another space $Z$. \n",
    "- Let $g : Z × R^d → X$ be a function, that will be denoted $g_θ(z)$ with $z$ the first coordinate and $θ$ the second.\n",
    "- Let $P_θ$ denote the distribution of $g_θ(Z)$. Then,\n",
    "1. If $g$ is continuous in $θ$, so is $W(P_r, P_θ)$.\n",
    "2. If $g$ is locally Lipschitz and satisfies regularity assumption 1, then $W(P_r, P_θ)$ is continuous everywhere, and differentiable almost everywhere.\n",
    "3. Statements 1-2 are false for the Jensen-Shannon divergence $JS(P_r, P_θ)$ and all the KLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ec930-f17f-47b1-a9fb-582ab6c526f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
