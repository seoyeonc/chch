{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a731bb-d55c-411d-87f8-f6c782603246",
   "metadata": {
    "id": "cac470df-29e7-4148-9bbd-d8b9a32fa570",
    "tags": []
   },
   "source": [
    "# (not_done_review)그로킹 심층 강화학습\n",
    "> 강찬석\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: 최서연\n",
    "- categories: [Reinforcement Learning]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6bfdb-de3e-4f44-8936-9297c79d6356",
   "metadata": {},
   "source": [
    "# $\\star$ 목표: 일주일에 몇 장씩이라도 보기!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fd0aa-0a6e-4dda-a5ed-bc6c93cda730",
   "metadata": {},
   "source": [
    "ref: https://goodboychan.github.io/book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227222c9-74ff-4207-a917-1216fafbf433",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e663b-943e-4eda-b806-717edf24908e",
   "metadata": {},
   "source": [
    "심층강화학습 deep reinforcement learning DRL 이란 머신 러닝 기법 중 하나.\n",
    "- 지능이 요구되는 문제를 해결할 수 있도록 인공지능 컴퓨터 프로그램을 개발하는데 사용\n",
    "- 시행착오를 통해 얻은 반응을 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a885a96-06a3-4b7b-a777-ad43f8e7c498",
   "metadata": {},
   "source": [
    "심층 강화학습은 문제에 대한 접근법이다.\n",
    "- **에이전트agent**: 의사를 결정하는 객체 자체\n",
    "    - ex) 사물을 집는 로봇 학습시킬때 의사 결정을 좌우하는 코드와 연관\n",
    "- **환경unvironmnet**: 에이전트(의사 결정) 이외의 모든 것\n",
    "    - ex) 의사결정하는 로봇(객체) 제외한 모든 것이 환경의 일부 \n",
    "- **싱태 영역state space**: 변수가 가질 수 있는 모든 값들의 집합\n",
    "- **관찰observation**: 에이전트가 관찰할 수 있는 상태의 일부\n",
    "- **전이 함수transition function** 에이전트와 환경 사이의 관계를 정의한 함수\n",
    "- **보상 함수reward function**: 행동에 대한 반응으로 환경이 제공한 보상 신호와 관련된 함수\n",
    "- **모델model**: 전이와 보상함수를 표현 가능\n",
    "\n",
    "에이전트의 3단계 과정\n",
    "1. 환경과 상호작용을 나누고\n",
    "2. 행동에 대해서 평가를 하며,\n",
    "3. 받은 반응을 개선시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc58edf-5115-4c33-ab82-38db6699e40a",
   "metadata": {},
   "source": [
    "- **에피소드형 업무episodic task**: 게임과 같이 자연적으로 끝나는 업무\n",
    "- **연속형업무continuing task**: 앞으로 가는 동작을 학습하는 경우\n",
    "\n",
    "- 연속형 피드백이 야기하는 문제\n",
    "    - **시긴적 가치 할당 문제tamporal credit assignment problem**:문제에 시간적 개념이 들어가 있고, 행동에도 지연된 속성이 담겨 있으면, 보상에 대한 가치를 부여하기 어렵다. \n",
    "- 평가 가능한 피드백이 야기하는 문제\n",
    "    - **탐험과 착취 간의 트레이드 오프exploration versus explotation trade-off**: 에이전트는 현재 가지고 있는 정보에서 얻을 수 있는 가장 좋은 것과 정보를 새로 얻는 것 간의 균형을 맞출 수 있어야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98653189-f50f-49d4-a114-3410e7627488",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e407f-5d1d-426e-ae60-47f7c6f43fbc",
   "metadata": {},
   "source": [
    "에이전트의 세 단계 과정\n",
    "1. 모든 에이전트는 *상호작용* 요소를 가지고 있고, 학습에 필요한 데이터를 수집한다.\n",
    "2. 모든 에이전트들은 현재 취하고 있는 행동을 *평가*하고, \n",
    "3. 전체적인 성능을 개선하기 위해 만들어진 무언가를 *개선*한다.\n",
    "\n",
    "- *상태 영역state space*: 표현할 수 있는 변수들의 모든 값에 대한 조합\n",
    "- *관찰observation*: 에이전트가 어떤 특정 시간에 얻을 수 있는 변수들의 집합\n",
    "- *관찰 영역observation space*: 변수들이 가질 수 있는 모든 값들의 조합\n",
    "- *행동 영역action space*: 모든 상태에서 취할 수 있는 모든 행동에 대한 집합\n",
    "- *전이 함수transition function*: 환경은 에이전트의 행동에 대한 반응으로 상태를 변화할 수 있는데 이와 관련된 함수\n",
    "- *보상 함수reward function*: 행동과 관련된 보상에 대한 함수\n",
    "- *모델mpdel*: 전이와 보상 함수에 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5d7bc-51eb-4b9b-a685-f9868c16da75",
   "metadata": {},
   "source": [
    "보상 신호가 밀집되어 있을수록, 에이전트의 직관성과 학습속도가 높아지지만, 에이전트에게 평견을 주입하게 되어, 결국 에이전트가 예상하지 못한 행동을 할 가능성은 적어지게 된다. 반면, 보상 신호가 적을수록 직관성이 낮아져 에이전트가 새로운 행동을 취할 확률이 높아지지만, 그만큼 에이전트르르 학습시키는데 오래 걸리게 될 것이다.\n",
    "\n",
    "- *타임 스텝time step*: 상호작용이 진행되는 사이클, 시간의 단위\n",
    "- *경험 튜플experience tuple*: 관찰 또는 상태, 행동, 보상 그리고 새로운 관찰\n",
    "- *에피소드형 업무episodic task*: 게임과 같이 자연적으로 끝나는 업무 - *에피소드episode*\n",
    "- *연속형 업무continuing task*: 앞으로 전진하는 동작과 같이 자연적으로 끝나는 업무\n",
    "- *반환값return*: 에피소드 동안 수집된 보상의 총합\n",
    "- *상태state*: 문제에 포함되어 있는 독특하고, 자기 자신만의 설정이 담긴 요소\n",
    "- *상태 영역state square*: 모든 가능한 상태, 집합 S로 표현,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c159436-e5de-44bc-b4af-4f512e0ecff9",
   "metadata": {},
   "source": [
    "**마르코프 결정 과정Markov decision process MDP**\n",
    "- 수학 프레임워크, 이를 이용해서 강화학습 환경을 연속적인 의사결정 문제로 표현하는 방법 학습.\n",
    "- 일반적인 형태는 불확실성상에 놓여진 상황에서 어떠한 복잡한 연속적 결정을 가상으로 모델링해서, 강화학습 에이전트가 모델과 반응을 주고받고, 경험을 통해서 스스로 학습할 수 있게 해준다.\n",
    "- 모든 상태들의 집합S\n",
    "- 모든 *시작 상태starting state* 혹은 *초기 상태initial state*라고 부르는 S+의 부분집합이 있음\n",
    "- MDP와의 상호작용 시작하면서 어떤 S에서의 특정 상태에서 어떤 확률 붙포 간의 관계를 그릴 수 있는데 이때 이 확률 분포는 무엇이든 될 수 있지만, 학습이 이뤄지는 동안에는 고정되어 있어야 한다.\n",
    "    - 즉, 이 확률 분포에서 샘플링된 확률은 학습과 에이전트 검증의 처름 에피소드부터 마지막 에피소드까지는 항상 동일해야 한다는 것\n",
    "- *흡수absorbing* 또는 *종료 상태terminal state* 라는 특별한 상태도 존재.\n",
    "    - 종료 상태(꼭 하나는 아님) 이외의 모든 상태들을 S라고 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1337d7-8b38-4f6f-ba78-d412acbfb97b",
   "metadata": {},
   "source": [
    "MDP에서는\n",
    "- **A**: 상태에 따라 결정되는 행동의 집합\n",
    "    - $\\therefore$ 특정 상태에서 허용되지 않는 행동도 존재한다는 말.\n",
    "    - 상태(s)를 인자로 받는 함수.\n",
    "    - A(s); 상태(s)에서 취할 수 있는 행동들의 집합 반환, 상수 정의 가능.\n",
    "- 행동 영역은 유한 혹은 무한 가능,\n",
    "    - 단일 행동에 대한 변수들의 집합은 한 개 이상의 료소를 가질 수 있고, 유한해야 함.\n",
    "    - $\\therefore$ 대부분의 환경에서 모든 상태에서 취할 수 있는 행동의 수는 같도록 설계됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de381d58-213e-4111-8a94-057819aa3585",
   "metadata": {},
   "source": [
    "*상태-전이 확률state-transition probability* = *전이 함수*\n",
    "- $T(s,a,s')$\n",
    "- 전이함수$T$는 각 전이 튜플인 $(s,a,s')$을 확률과 연결시켜준다.\n",
    "- 어떤 상태 s에서 행동 a를 취하고 다음 상태가 s'이 되었을 때, 이때의 확률을 반환해준다는 뜻\n",
    "- 이 확률의 합은 1\n",
    "$$p(s'|s,a) = P(S_t = s'|S_{t-1} = s,A_{t-1}=a)$$\n",
    "$$\\sum_{s' \\in S}p(s'|s,a) = a, \\forall s \\in S, \\forall a \\in A(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43dfbff-36d5-46a4-afcb-66c35202093f",
   "metadata": {},
   "source": [
    "*보상 함수$R$*\n",
    "- 전이 튜플 $s,a,s'$을 특정한 스칼라 값으로 매핑,\n",
    "    - 양수 보상 = 수익 또는 이득\n",
    "    - 음수 보상 = 비용, 벌칙, 패널티\n",
    "- $R(s,a,s')$ = $R(s,a)$ = $R(s)$\n",
    "- *보상 함수를 표현하는 가장 명확한 방법은 상태와 행동 그리고 다음 상태, 이 세 가지를 함께 쓰는 것*\n",
    "$$r(s,a) = \\mathbb{E} [R_t|S_{t-1} = s,A_{t-1} = a]$$\n",
    "$$r(s,a,s') = \\mathbb{E} [R_t|S_{t-1} = s, A_{t-1} = a, S_t= s']$$\n",
    "$$R_t \\in \\cal{R} \\subset \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926fbd9-ea72-4dde-b2a4-ff49c9da8b77",
   "metadata": {},
   "source": [
    "*호라이즌horixon*\n",
    "- 계획 호라이즌planning horizon: 에피소드형 업무나 연속적 업무를 에이전트의 관점에서 정의\n",
    "    - 유한 호라이즌finite horizon: 에이전트가 유한한 타임 스템 내에 업무가 종료된다는 것을 알고 있는 계획 호라이즌\n",
    "    - 탐욕 호라이즌greedy horizon: 계획 호라이즌이 1인 경우\n",
    "    - 무한 호라이즌infinite horiaon 에이전트한테 미리 정의된 타임 스텝에 대한 제한이 없어 에이전트가 무한하게 계획할 수 있음\n",
    "        - 특히, 무한한 계획 호라이즌을 가지는 업무는 무기한 호라이즌 업무indefinite horizon task 라고 부름\n",
    "- 에이전트가 타임 스텝 루프에 빠지는 것을 막기 위해 타임 스텝을 제한하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4aa89a-708a-49fc-b55a-50b0cfcb8d4a",
   "metadata": {},
   "source": [
    "*감가율discount factor  = 감마gamma*\n",
    "- 받았던 보상의 시점이 미래로 더 멀어질수록, 현재 시점에서는 이에 대한 가치를 더 낮게 평가\n",
    "\n",
    "- 환경; 자세한 예제; 5개의 미끄러지는 칸을 가지는 환경slippery walk five, SWF\n",
    "$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\dots R_T$$\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots +\\gamma^{T-1} R_T$$\n",
    "$$G_t = \\sum^{\\infty}_{k=0} \\gamma^k R_{t+k+1}$$\n",
    "$$G_t = R_{t+1}+ \\gamma G_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5654d6-d858-4734-a7ad-7b8d3136d85b",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab96a76-43c1-4526-818a-b96dd5873728",
   "metadata": {},
   "source": [
    "에이전트의 목표: 반환값(보상의 총합)을 최대화할 수 있는 행동의 집합을 찾는 것, 정책이 필요하다.\n",
    "- *정책policy*: 가능한 모든 상태를 포괄한 전체적인 계획.\n",
    "- 확률적 혹은 결정적\n",
    "- 정책을 비교하기 위해 시작 상태를 포함한 모든 상태들에 대해 기대 반환값을 계산할 수 있어야 한다.\n",
    "\n",
    "\n",
    "- 정책$\\pi$를 수행할 때, 상태 s에 대한 가치를 정의할 수 있다.\n",
    "    - 에이전트가 정책 $\\pi$를 따르면서 상태 s에서 시작했을 때, 상태 s의 가치는 반환값의 기대치라고 할 수 있다.\n",
    "    - 가치함수는 상태 s에서 정책$\\pi$를 따르면서 시작했을 때의 반환값에 대한 기대치를 나타낸다.\n",
    "$$v_{\\pi} (s) = \\mathbb{E}_{\\pi} [G_t|S_t = s]$$\n",
    "$$v_{\\pi} (s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{+3} + \\dots |S_t = s]$$\n",
    "$$v_{\\pi} (s) = \\sum_a \\pi (a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma v_\\pi (s')] \\forall s \\in S$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994f177-b132-4696-817b-fec3bb5528a0",
   "metadata": {},
   "source": [
    "*행동-가치 함수action-value function* = Q 함수 = $Q^{\\pi}(s,a)$\n",
    "- 상태 s에서 행동 a를 취했을 때, 에이전트가 정책$\\pi$를 수행하면서 얻을 수 있는 기대 반환값\n",
    "- MDP 없이도 정책을 개선시킬 수 있게 해준다.\n",
    "$$q_{\\pi} (s,a) = \\mathbb{E}_{\\pi} [G_t|S_t = s,A_t = a]$$\n",
    "$$q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[R_t + \\gamma G_{t+1} | S_t = s, A_t = a]$$\n",
    "$$q_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a)[r+\\gamma v_{\\pi} (s')], \\forall s \\in S, \\forall a \\in A(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28923dc6-af6b-4a0b-aff6-225aaec7cfab",
   "metadata": {},
   "source": [
    "*행동-이점 함수action-advamtage function* = 이점함수advantage function = $A$\n",
    "- 상태 s에서 행동를 취했을 때의 가치와 정책 $\\pi$에서 상태 s에 대한 상태-가치 함수 간의 차이\n",
    "$$a_{\\pi}(s,a) = q_{\\pi}(s,a) - v_{\\pi}(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9e89f-9920-47d3-8cc7-1ef08e579d80",
   "metadata": {},
   "source": [
    "이상적인 정책optimal policy\n",
    "- 모든 상태에 대해서 다른 정책들보다 기대 반환값이 같거나 더 크게 얻을 수 있는 정책\n",
    "    - 벨만 이상성 공식(아래)\n",
    "$$v_{*}(s) = max_{\\pi} v_{\\pi} (s), \\forall_s \\in S$$\n",
    "$$q_{*}(s,a) = max_{\\pi} q_{\\pi}(s,a), \\forall s \\in S, \\forall a \\in A(s)$$\n",
    "$$v_{*}(s) = max_{a} \\sum_{s',r} p(s',r|s,a)[r+\\gamma v_{*} (s')]$$\n",
    "$$q_{*}(s,a) = \\sum_{s',r}p(s',r|s,a) [r + \\gamma max_{a'} q_{*}(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5942d-d3f1-47df-ad25-8c2f16398178",
   "metadata": {},
   "source": [
    "*반복 정책 평가법iteractive policy evaluation* = *정책평가법policy rvaluation*\n",
    "- 임의의 정책을 평가할 수 있는 알고리즘\n",
    "- 상태 영역을 살펴보면서 반복적으로 에측치를 개선하는 방법 사용\n",
    "- 정책을 입력으로 받고, *예측문제prediction problem*를 풀 수 있는 알고리즘에 대한 가치 함수를 출력으로 내보내는 알고리즘, 이때는 미리 정의한 정책의 가치를 계산..(?)\n",
    "$$v_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s',r|s,a) \\big[ r+\\gamma v_k (s') \\big]$$\n",
    "- 정책 평가 알고리즘을 충분히 반복하면 정책에 대한 가치함수로 수렴시킬 수 있다.\n",
    "\n",
    "- 실제로 적용하는 경우에는 우리가 근사하고자 하는 가치 함수의 변화를 확인하기 위해서 기분보다 작은 임계값을 사용.\n",
    "    - 이런 경우에는 가치 함수의 변화가 우리가 정한 임계값보다 작을경우 반복을 멈추게 된다.\n",
    "    \n",
    "- SWF환경에서 항상 왼쪽 행동을 취하는 정책에 이 알고리즘이 어떻게 동작할까?\n",
    "$$v_{k+1} (s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma v_k (s') \\big] $$\n",
    "    - $\\gamma$는 1이라 가정,\n",
    "    - 항상 왼쪽으로 가는 정책 사용\n",
    "    - $k$: 반목적 정책-쳥가 알고리즘의 수행횟수\n",
    "$$ v^{\\pi}_{1}(5)  = p(s'=4|s=5, a=\\text{왼쪽}) * [R(5,\\text{왼쪽},4) + v^{\\pi}_{0}(4)] + p(s'=5|s=5, a=\\text{왼쪽}) * [R(5,\\text{왼쪽},5) + v^{\\pi}_{0}(5)] + p(s'=6|s=5, a=\\text{왼쪽}) * [R(5,\\text{왼쪽},6) + v^{\\pi}_{0}(6)]$$\n",
    "$$c^{\\pi}_{1} (5) = 0.50 * (0+0) + 0.33 * (0+0) + 0.166 * (1+0) = 0.166 \\dots \\text{   1번 정책 평가법을 사용했을 떄 상태에 대한 가치를 나타냄}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a9ce3-33df-4919-ab82-0e50d4068500",
   "metadata": {},
   "source": [
    "$\\rightarrow$ Bootstraping 붓스트랩 방법: 예측치를 통해서 새로운 예측치를 계산하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6db29-1137-48e7-82d3-ce85d112284e",
   "metadata": {},
   "source": [
    "```python\n",
    "# 정책 평가 알고리즘\n",
    "def policy_evaluation(pi,P, gamma = 1.0, theta = 1e-10):\n",
    "    prev_V = np.zeros(len(P),dtype = np.float64)\n",
    "    while True:\n",
    "        V = np.aeros(len(P),dtype = float64)\n",
    "        for s in range(len(P)):\n",
    "            for prob, next_state, reward, done in P[s][pi(s)]:\n",
    "            V[s] +- prob * (reward + gamma * prev_V[next_state] * (nor done))\n",
    "        if np.max(np.abs(prev_V - V)) < theta:\n",
    "            break\n",
    "        prev_V = V.copy()\n",
    "    return V\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93680e-805d-4550-bf4e-1e02517b50ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cb1fadd-7f59-4226-8cd9-1f16cc75da8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87b69fa6-58a6-4961-b416-581fee6724fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26845576-8467-4cb5-9087-7113965387ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b14ab457-f9d4-45b6-91f2-fd449c8c440b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42d3d7f2-e6d4-4d0c-ba96-ed9fdbf99587",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6509a684-4a29-47f9-af77-e806d055019d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6b2b359-7ea1-4552-a18e-54a2c366d977",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c52b08-2755-46d8-9371-f7dc4d034abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
