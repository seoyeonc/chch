{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TensorFlow 2.0 Category 3 - Cats vs Dogs 분류\n",
    ">  Convolution Neural network 활용한 분류 모델 (Classification), tensorflow-datasets 를 활용한 데이터 전처리\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: 최서연\n",
    "- categories: [TensorFlow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG Net(2014년)\n",
    "실무에서 많이 쓰는 것들\n",
    "- 16layers\n",
    "- 19layers\n",
    "\n",
    "cats vs dogs는 VGG16쓰기!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww0hxcaSGb0m",
    "tags": []
   },
   "source": [
    "## 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI6oK1X-GdJJ"
   },
   "source": [
    "1. GPU 옵션 켜져 있는지 확인할 것!!! (수정 - 노트설정 - 하드웨어설정 (GPU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yoe-K38GQ4b"
   },
   "source": [
    "## 순서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc6iTV8DGPop"
   },
   "source": [
    "1. **import**: 필요한 모듈 import\n",
    "2. **전처리**: 학습에 필요한 데이터 전처리를 수행합니다.\n",
    "3. **모델링(model)**: 모델을 정의합니다.\n",
    "4. **컴파일(compile)**: 모델을 생성합니다.\n",
    "5. **학습 (fit)**: 모델을 학습시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msi1agesayxW"
   },
   "source": [
    "## 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bqqzpm1P8dd9"
   },
   "source": [
    "Computer Vision with CNNs\n",
    "<br>\n",
    "<br>For this exercise you will build a cats v dogs classifier\n",
    "<br>using the Cats v Dogs dataset from TFDS.\n",
    "<br>Be sure to use the final layer as shown \n",
    "<br>    **(Dense, 2 neurons, softmax activation)**\n",
    "<br>\n",
    "<br>The testing infrastructre will **resize all images to 224x224**\n",
    "<br>with **3 bytes of color depth**. Make sure your input layer trains\n",
    "<br>images to that specification, or the tests will fail.\n",
    "<br>\n",
    "<br>Make sure your output layer is exactly as specified here, or the \n",
    "<br>tests will fail.\n",
    "\n",
    "----------------------------------------\n",
    "<br>이 연습에서는 cats v dogs 분류기를 만들 것입니다.\n",
    "TFDS의 Cats v Dogs 데이터 세트 사용.\n",
    "<br> 그림과 같이 최종 레이어를 사용하십시오\n",
    "<br> **(Dense, 뉴런 2 개, activation='softmax')**\n",
    "<br>\n",
    "<br> 테스트 인프라는 **모든 이미지의 크기를 224x224로 조정합니다(컬러사진)**. 입력 레이어를 확인하십시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VXJfrSfG8dd5"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIXfMfFq8dd9"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg0sXFW2nil7"
   },
   "source": [
    "**tensorflow-datasets**를 활용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTjsZFWGn117"
   },
   "source": [
    "* [Cats vs Dogs 데이터셋 문서 보기](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs?hl=ko)\n",
    "\n",
    "* [tensorflow-datasets 다루기](https://www.tensorflow.org/datasets/splits?hl=ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTCJ8FxgogB_"
   },
   "source": [
    "**시험에서 주어지는 데이터셋 로드 형태**\n",
    "\n",
    "* 예전 방식이므로 아래처러 주어지는 코드를 과감히 삭제 후, 아래 제공되는 방식으로 변경합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jEE244gh2mA"
   },
   "source": [
    "```python\n",
    "setattr(tfds.image_classification.cats_vs_dogs, '_URL',\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e1VLqKMO8deA"
   },
   "outputs": [],
   "source": [
    "dataset_name = 'cats_vs_dogs'\n",
    "\n",
    "# 처음 80%의 데이터만 사용\n",
    "train_dataset = tfds.load(name=dataset_name, split='train[:80%]')\n",
    "\n",
    "# 최근 20%의 데이터만 사용\n",
    "valid_dataset = tfds.load(name=dataset_name, split='train[80%:]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "739L8AKh-OZG"
   },
   "source": [
    "시험에서 요구하는 **전처리 요구 조건**은 다음과 같습니다.\n",
    "\n",
    "1. 이미지 정규화 (Normalization)\n",
    "2. 이미지 사이즈 맞추기: (224 X 224) \n",
    "3. image(x), label(y)를 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7fLV4E6_8uD"
   },
   "source": [
    "**[실습코드]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fV1LU9W1-Swt"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    # x, y 데이터를 정의합니다.\n",
    "    x = data['image']\n",
    "    y = data['label']\n",
    "    # image 정규화(Normalization)\n",
    "    x = x / 255\n",
    "    # 사이즈를 (224, 224)로 변환합니다.\n",
    "    x = tf.image.resize(x, size=(224, 224))\n",
    "    # x, y  데이터를 return 합니다.\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPQ9hOrnpHKj"
   },
   "source": [
    "만든 전처리 함수(preprocessing)를 **dataset에 mapping**하고, **batch_size도 지정**합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wq-NyPMI_oZM"
   },
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JRM0LRxt-Sru"
   },
   "outputs": [],
   "source": [
    "train_data = train_dataset.map(preprocess).batch(batch_size)\n",
    "valid_data = valid_dataset.map(preprocess).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvvarzxw8ded"
   },
   "source": [
    "## 모델 정의 (Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azRvYVbQnim5"
   },
   "source": [
    "이제 Modeling을 할 차례입니다.\n",
    "\n",
    "`Sequential` 모델 안에서 층을 깊게 쌓아 올려 주면 됩니다.\n",
    "\n",
    "1. `input_shape`는 224 X 224 컬러사진인 **(224, 224, 3)**으로 지정합니다.\n",
    "2. transfer learning 기법을 통해 VGG16 모델을 활용한 전이학습 모델을 완성합니다.\n",
    "3. 출력층은 class 갯수 2개의 뉴런이 요구됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dcJtZOg0EqA7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "transfer_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "transfer_model.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "weights='imagenet', include_top=False, input_shape=(224, 224, 3)\n",
    "```\n",
    "-weights: image분류기의 가중치 가져오기\n",
    "- include_top layer안 들고 오게 false 입력\n",
    "- input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google erver에서 학습된 모델 가져오는 과정임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rpWdaUGX8dee"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    transfer_model,\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iQRWnG9A8def"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               12845568  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,626,178\n",
      "Trainable params: 12,911,490\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 마지막 scale512임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnmQV-fDnim8"
   },
   "source": [
    "## 컴파일 (compile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHQ1abHXK8e9"
   },
   "source": [
    "1. `optimizer`는 가장 최적화가 잘되는 알고리즘인 'adam'을 사용합니다.\n",
    "2. `loss`설정\n",
    "  * 출력층 activation이 `sigmoid` 인 경우: `binary_crossentropy`\n",
    "  * 출력층 activation이 `softmax` 인 경우: \n",
    "    * 원핫인코딩(O): `categorical_crossentropy`\n",
    "    * 원핫인코딩(X): `sparse_categorical_crossentropy`)\n",
    "3. `metrics`를 'acc' 혹은 'accuracy'로 지정하면, 학습시 정확도를 모니터링 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNurLvnisN-t"
   },
   "source": [
    "전처리 단계에서 **one-hot encoding** 을 해주었습니다. 따라서, `categorical_crossentropy`를 지정해주면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXGMljJQhmp_"
   },
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QCLw6RMZnim-"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyLUPgGCninB"
   },
   "source": [
    "## ModelCheckpoint: 체크포인트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46Oi04ZMLtEB"
   },
   "source": [
    "`val_loss` 기준으로 epoch 마다 최적의 모델을 저장하기 위하여, ModelCheckpoint를 만듭니다.\n",
    "* `checkpoint_path`는 모델이 저장될 파일 명을 설정합니다.\n",
    "* `ModelCheckpoint`을 선언하고, 적절한 옵션 값을 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qJwGq3PoninB"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"my_checkpoint.ckpt\"\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
    "                             save_weights_only=True, \n",
    "                             save_best_only=True, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3mjb5EAninE"
   },
   "source": [
    "## 학습 (fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-X6hK_DMYZH"
   },
   "source": [
    "1. `validation_data`를 반드시 지정합니다.\n",
    "2. `epochs`을 적절하게 지정합니다.\n",
    "3. `callbacks`에 바로 위에서 만든 checkpoint를 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2uHXDA_vninF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.2787 - acc: 0.8851\n",
      "Epoch 1: val_loss improved from inf to 0.18210, saving model to my_checkpoint.ckpt\n",
      "582/582 [==============================] - 766s 1s/step - loss: 0.2787 - acc: 0.8851 - val_loss: 0.1821 - val_acc: 0.9207\n",
      "Epoch 2/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.1807 - acc: 0.9227\n",
      "Epoch 2: val_loss did not improve from 0.18210\n",
      "582/582 [==============================] - 767s 1s/step - loss: 0.1807 - acc: 0.9227 - val_loss: 0.2050 - val_acc: 0.9129\n",
      "Epoch 3/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.1534 - acc: 0.9368\n",
      "Epoch 3: val_loss did not improve from 0.18210\n",
      "582/582 [==============================] - 766s 1s/step - loss: 0.1534 - acc: 0.9368 - val_loss: 0.1912 - val_acc: 0.9226\n",
      "Epoch 4/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.1357 - acc: 0.9445\n",
      "Epoch 4: val_loss improved from 0.18210 to 0.17464, saving model to my_checkpoint.ckpt\n",
      "582/582 [==============================] - 717s 1s/step - loss: 0.1357 - acc: 0.9445 - val_loss: 0.1746 - val_acc: 0.9314\n",
      "Epoch 5/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.1276 - acc: 0.9477\n",
      "Epoch 5: val_loss did not improve from 0.17464\n",
      "582/582 [==============================] - 587s 1s/step - loss: 0.1276 - acc: 0.9477 - val_loss: 0.1936 - val_acc: 0.9273\n",
      "Epoch 6/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.1155 - acc: 0.9529\n",
      "Epoch 6: val_loss did not improve from 0.17464\n",
      "582/582 [==============================] - 595s 1s/step - loss: 0.1155 - acc: 0.9529 - val_loss: 0.2203 - val_acc: 0.9134\n",
      "Epoch 7/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.1091 - acc: 0.9552\n",
      "Epoch 7: val_loss improved from 0.17464 to 0.16164, saving model to my_checkpoint.ckpt\n",
      "582/582 [==============================] - 600s 1s/step - loss: 0.1091 - acc: 0.9552 - val_loss: 0.1616 - val_acc: 0.9336\n",
      "Epoch 8/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.0937 - acc: 0.9632\n",
      "Epoch 8: val_loss did not improve from 0.16164\n",
      "582/582 [==============================] - 600s 1s/step - loss: 0.0937 - acc: 0.9632 - val_loss: 0.1686 - val_acc: 0.9319\n",
      "Epoch 9/20\n",
      "510/582 [=========================>....] - ETA: 59s - loss: 0.0911 - acc: 0.9636 \n",
      "Epoch 9: val_loss did not improve from 0.16164\n",
      "582/582 [==============================] - 599s 1s/step - loss: 0.0889 - acc: 0.9646 - val_loss: 0.2089 - val_acc: 0.9293\n",
      "Epoch 10/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.0889 - acc: 0.9644\n",
      "Epoch 10: val_loss did not improve from 0.16164\n",
      "582/582 [==============================] - 599s 1s/step - loss: 0.0889 - acc: 0.9644 - val_loss: 0.2816 - val_acc: 0.9043\n",
      "Epoch 11/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.0802 - acc: 0.9673\n",
      "Epoch 11: val_loss did not improve from 0.16164\n",
      "582/582 [==============================] - 599s 1s/step - loss: 0.0802 - acc: 0.9673 - val_loss: 0.2202 - val_acc: 0.9233\n",
      "Epoch 12/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.0697 - acc: 0.9739\n",
      "Epoch 12: val_loss did not improve from 0.16164\n",
      "582/582 [==============================] - 600s 1s/step - loss: 0.0697 - acc: 0.9739 - val_loss: 0.2284 - val_acc: 0.9185\n",
      "Epoch 13/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.0665 - acc: 0.9734\n",
      "Epoch 13: val_loss did not improve from 0.16164\n",
      "582/582 [==============================] - 601s 1s/step - loss: 0.0665 - acc: 0.9734 - val_loss: 0.3095 - val_acc: 0.9117\n",
      "Epoch 14/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.0665 - acc: 0.9745\n",
      "Epoch 14: val_loss did not improve from 0.16164\n",
      "582/582 [==============================] - 601s 1s/step - loss: 0.0665 - acc: 0.9745 - val_loss: 0.2381 - val_acc: 0.9155\n",
      "Epoch 15/20\n",
      " 72/582 [==>...........................] - ETA: 7:00 - loss: 0.0774 - acc: 0.9696"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data,\n",
    "          validation_data=(valid_data),\n",
    "          epochs=20,\n",
    "          callbacks=[checkpoint],\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch 하나만 돌려도 accuracy90 이상 나오는 구글 본딴 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwnduSgRiBw8"
   },
   "source": [
    "## 학습 완료 후 Load Weights (ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLqb_6XrMvdq"
   },
   "source": [
    "학습이 완료된 후에는 반드시 `load_weights`를 해주어야 합니다.\n",
    "\n",
    "그렇지 않으면, 열심히 ModelCheckpoint를 만든 의미가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4jO1ucZ9ninH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fab9a3189d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint 를 저장한 파일명을 입력합니다.\n",
    "model.load_weights(checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
