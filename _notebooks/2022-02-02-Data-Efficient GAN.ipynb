{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95858dd-aa66-4f48-8319-437d61916c21",
   "metadata": {
    "id": "d44ba9bc-ebf3-49d8-b2db-9d49bf1bec22",
    "tags": []
   },
   "source": [
    "# Differentiable Augmentation for Data-Efficient GAN Training\n",
    "> Shengyu Zhao(IIIS, Tsinghua University and MIT), Zhijian Liu(MIT), Ji Lin(MIT), Jun-Yan Zhu(Adobe and CMU), Song Han(MIT)\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false \n",
    "- author: 최서연\n",
    "- categories: [GAN, 논문리뷰]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5eba40-6316-4e34-ad6b-be8aad278164",
   "metadata": {},
   "source": [
    "ref: https://proceedings.neurips.cc/paper/2020/file/55479c55ebd1efd3ff125f1337100388-Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefeb243-1316-4102-9e2b-2f11df53f7b2",
   "metadata": {},
   "source": [
    "ref: https://github.com/mit-han-lab/data-efficient-gans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e857bc2-13af-4c1b-9f7e-09f08bda6208",
   "metadata": {},
   "source": [
    "> youtube: https://youtu.be/SsqcjS6SVM0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf777f61-a1f9-4dff-aeb0-78a8c1061c5c",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ddb62-a8e7-4398-9cae-b26f837b2118",
   "metadata": {},
   "source": [
    "- The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. \n",
    "    - 제한된 양의 학습 데이터가 주어지면 성능 나빠짐\n",
    "    - discriminator가 정확한 training set 기억하는 주된 이유\n",
    "- To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. \n",
    "    - 실제/가짜 sample에 differentiable augmentations의 다양한 type 부과해서 GAN의 data 효율성을 증진하는 DiffAugment(Differentiable Augmentation) 제시\n",
    "    - 일반화된 sample에서 differentiable augmentation 차별화된 확대 수용하면 training이 효율적으로 안정화하고 더 나은 convergence를 이끔\n",
    "    - only 20% training data, we can match the top performance on CIFAR-10 and CIFAR-100\n",
    "    - our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa030bd-5ddb-4beb-ba39-451d4e4cea61",
   "metadata": {},
   "source": [
    "##  Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ebf48-0dca-4347-a1f5-114682fc817b",
   "metadata": {},
   "source": [
    "GANs(Generative Adversarial Networks)는 generator G와 discriminator D로 target dataset의 분포를 모델링하는 것이 목적\n",
    "\n",
    "genorator G는 typically 가우시안 분포에서 나온 입력 latent 벡터 z에서 출력 G(z)에 mapping한다.\n",
    "\n",
    "discriminator D는 실제 관측치 x에서 generated sample G(z)를 구별해내는 것을 배움\n",
    "\n",
    "The standard GANs training algorithm alternately optimizes the discriminator’s loss $L_D$ and the generator’s loss $L_G$ given loss functions $f_D$ and $f_G$:\n",
    "$$L_D = E_{x~p_{data}(x)}|f_D (-D(x))| + E_{x~p(x)}|f_D (D(G(z)))|, \\dots(1)$$\n",
    "$$L_G = E_{x~p(x)} |f_G (-D(G(z)))|, \\dots(2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42122120-ae0c-4def-8441-36430df43e61",
   "metadata": {},
   "source": [
    "![](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628d6cb-7f36-428d-8d2e-b0395b07baa8",
   "metadata": {},
   "source": [
    "Figure 4: Overview of DiffAugment for updating D (left) and G (right)\n",
    "- DiffAugment는 실제 smaple x와 generated 출력 G(z)에 augmentation T를 적용\n",
    "- G를 업데이트하면 기울기는 T로 역전파되어야 하며 T는 입력으로 미분할 수 있어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b583f-9cc7-4243-9d02-edfcf6eabf42",
   "metadata": {},
   "source": [
    "Here, different loss functions can be used, such as the non-saturating loss, where $f_D(x) = f_G(x) = log (1 + e^x)$, and the hinge loss, where $f_D(x) = max(0, 1 + x)$ and $f_G(x) = x$.\n",
    "\n",
    "the discriminator tends to memorize the observations as the training progresses.\n",
    "- discrminator는 training 한 관찰을 기억하는 경향이 있음\n",
    "\n",
    "An overfitted discriminator penalizes any generated samples other than the exact training data points, provides uninformative gradients due to poor generalization, and usually leads to training instability.\n",
    "- 과적합 discriminator는 정확한 training data point 외의 generated sample에 penalize하고, poor generation 따문에 정보 없는 gradient를 제공하지 않으며 training 불안정성을 이끈다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce2204-be27-4621-88f8-e34edbf52851",
   "metadata": {},
   "source": [
    "**Challenge: Discriminator Overfitting**\n",
    "- CIFAR-10 사용하여 BigGAN 성능 분석\n",
    "- figure 1 미루어보아 데이터가 100% 주어져도 genarator과 discriminator 사이의 격차 계속 증가\n",
    "    - discriminator가 단순하게 training image를 기억하고 있다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e97a0-26a2-4360-96ce-de5374fd7229",
   "metadata": {},
   "source": [
    "## Revisiting Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3a1ac-a78f-4bd5-b516-74637f6c0cb7",
   "metadata": {},
   "source": [
    "Data augmentation is a commonly-used strategy to reduce overfitting in many recognition tasks.\n",
    "- it has an irreplaceable role and can also be applied in conjunction with other regularization techniques: e.g., weight decay.\n",
    "\n",
    "the discriminator suffers from a similar overfitting problem as the binary classifier.\n",
    "- data augmentation is seldom used in the GAN literature compared to the explicit regularizations on the discriminator\n",
    "-  In fact, a recent work [50] observes that directly applying data augmentation to GANs does not improve the baseline. \n",
    "\n",
    "ask the questions:\n",
    "- what prevents us from simply applying data augmentation to GANs? \n",
    "- Why is augmenting GANs not as effective as augmenting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c95576-b82d-4c15-85e9-610fb1180567",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Augment reals only**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5af87f-12a9-445f-923b-127cd3b5b22e",
   "metadata": {},
   "source": [
    "The most straightforward way of augmenting GANs would be directly applying augmentation T to the real observations x, which we call “Augment reals only”: GAN 확대 혹은 증가를 위한 간단한 방법, Augment reals only라 부르는 실제 관측치 x를 T에 직접 적용시키기\n",
    "$$L_D = E_{x~p_{data}(x)}|f_D (-D(T(x))))| + E_{x~p(x)}|f_D(D(G((z))))|, \\dots (3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2405db-c11a-4335-980f-670b97339653",
   "metadata": {},
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQrxZ_1odR9kZuceLJYnEX3HRBwC5GY7R_H9Q&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbe722-4dc5-45cf-a727-b18bcbab6642",
   "metadata": {},
   "source": [
    "Figure 5. Understanding why vanilla augmentation strategies fail: \n",
    "- (a) “Augment reals only” 는 확대augmentation와 동일한 데이터 왜곡 의미, \n",
    "- (b)“Augment D only” 는 augment된 T(x)와 T(G(z))를 완벽하게 분리하지만 G(z)(Augmentation없는 가짜 이미지)를 거의 인지하지 못함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccadf93-26cc-4fbd-9c91-47ba6391583c",
   "metadata": {},
   "source": [
    "**“Augment reals only”**는 generative modeling의 원래 목적에서 벗어남.\n",
    "- 모델이 x대신에 T(x)의 다른 data distribution에서 learning하고 있기 때문이다.\n",
    "- 이는 실제 이미지의 분포가 크게 변화하여 어느 augmentation이든 적용하는 것으로부터 막기 때문.\n",
    "    - This prevents us from applying any augmentation that significantly alters the distribution of the real images.\n",
    "- 비록 강하게 특정 dataset에 의존하지만, 이 요구 사항을 만족하는 선택은 대부분 case에서 수평 flip이 될 수 있다.\n",
    "    - The choices that meet this requirement, although strongly dependent on the specific dataset, can only be horizontal flips in most cases. \n",
    "- 임의의 수평 flip을 적용하는 것은 성능이 보통 상승하는 것을 알 수 있어 baseline 을 더 stronger하게 만들기 위해 모든 experiment에서 임의의 수평 플립 사용함\n",
    "    - We find that applying random horizontal flips does increase the performance moderately, and we use it in all our experiments to make our baselines stronger.\n",
    "- 모델은 augmentation으로 도입된 원치 않는 색과 기하학적 왜곡을 생성하는 방법을 학습하여 성능이 크게 저하됨.\n",
    "    - As expected, the model learns to produce unwanted color and geometric distortion (e.g., unnatural color, cutout holes) as introduced by these augmentations, resulting in a significantly worse performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d20e59-2355-4dc7-8e3c-be559d0555ff",
   "metadata": {},
   "source": [
    "**Augment D only**\n",
    "- Augment reals only 은 실제 sample에 one-sided augmentation을 적용했던 것은 generated 분포가 manupulated 실제 분포와 일치한는 경우에만 convergence수렴이 achive될 수 있었다.\n",
    "    - Previously, “Augment reals only” applies one-sided augmentation to the real samples, and hence the convergence can be achieved only if the generated distribution matches the manipulated real distribution.\n",
    "- discriminator의 관점에서 D를 업데이트할 때 진짜와 가짜 sample 모두 augment 하는 것이 temp할 수 있음.\n",
    "    - From the discriminator’s perspective, it may be tempting to augment both real and fake samples when we update D:\n",
    "$$L_D = E_{x~p_{data}(z)}|f_D(-D(T(x)))| + E_{x~p(x)}|f_D(D(T(G(z))))|, \\dots (5)$$\n",
    "$$L_G = E_{x~p(x)}|f_G (-D(G(z)))|, \\dots (6)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d425d-2e34-48d7-a9a7-763c6a5889c3",
   "metadata": {},
   "source": [
    "- 실제 sample x와 가짜 sample G(z)가 같은 함수 T에 적용됨.\n",
    "- generator가 x의 분포를 성공적을 모델링한다면, G(z)와 x  뿐만 아니라 T(G(z)와 T(x)도 discriminator에 의해 판별될 수 없어야 한다.\n",
    "    - If the generator successfully models the distribution of x, T(G(z)) and T(x) should be indistinguishable to the discriminator as well as G(z) and x.\n",
    "- 하지만 이 strategy는 worse result를 이끔(Table 1에서 “Augment D only” 부분)\n",
    "- Figure 5 (b)“Augment D only” 에서는 Translatopnn이 적용된 training dynamics plot임\n",
    "    -  Figure 5 b plots the training dynamics of “Augment D only” with Translation applied. \n",
    "- 비록 D는 90% 이상의 정확도로 완벽하게 augmenteg image들인 T(G(z))와 T(x)를 분류해내지만, 10% 이하의 정확도로 augmentation 없이 generated image인 G(z)를 인식하는 데 실패함\n",
    "    - Although D classifies the augmented images (both T(G(z)) and T(x)) perfectly with an accuracy of above 90%, it fails to recognize G(z), the generated images without augmentation, with an accuracy of lower than 10%.\n",
    "- 결과적으로, generator는 G(z)에 의해 discriminator를 완전히 fool하고 discriminator로부터 유용한 정보를 얻릉 수 없다.\n",
    "    -  As a result, the generator completely fools the discriminator by G(z) and cannot obtain useful information from the discriminator.\n",
    "- generator G와 discriminator D 사이의 delicate한 균형을 깨는 시도가 실패하기 쉽다는 것을 나타냄."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e233f7b-9c41-4302-a1e2-1bbdb4e8dba6",
   "metadata": {},
   "source": [
    "##   Differentiable Augmentation for GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4425cab4-51d9-40f7-b277-23309641ee87",
   "metadata": {},
   "source": [
    "Augment reals only의 실패는 실제와 가짜 sample을 augment하도록 동기부여 하지만, Augment D only의 실패는 generator가 augmented sample을 놔둬서는 안 된다고 경고한다.\n",
    "- The failure of “Augment reals only” motivates us to augment both real and fake samples, while the failure of “Augment D only” warns us that the generator should not neglect the augmented samples.\n",
    "- augmented sample 을 통해 G로 gradient를 전파하기 위해 augmentation T 는 구별될 수 있어야함.\n",
    "    - 이게 바로 Differentiable Augmentation (DiffAugment)\n",
    "$$L_D = E_{x~p_{data}}(x)[f_D (-D(T(x)))] + E_{x~p(x)}[f_D(D(T(G(z))))], \\dots (7)$$\n",
    "$$L_g = E_{z~p(z)}[f_G(-D(T(G(z))))], \\dots (8)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7943e-bf60-4a3d-8600-2bac19efece8",
   "metadata": {},
   "source": [
    "- T는 동일한 임의의 함수, 하지만 세 위치에 거쳐 꼭 동일한 임의의 seed를 가질 필요는 없음\n",
    "    -  Note that T is required to be the same (random) function but not necessarily the same random seed across the three places illustrated in Figure 4.\n",
    "-  As shown in Table 1, BigGAN can be improved using the simple Translation policy and further boosted using a composition of Cutout and Translation; it is also robust to the strongest policy when Color is used in combined.\n",
    "- Figue 6에서 stronger한 DiffAugment policy가 일반적으로 더 낮은 training 정확도의 cost애서 더 높은 discriminator의 검증 정확도를 유지하고, overfitting을 완화하며, 결과적으로 더 나은 수렴convergence를 성취achieve했다고 분석한다.\n",
    "    - Figure 6 analyzes that stronger DiffAugment policies generally maintain a higher discriminator’s validation accuracy at the cost of a lower training accuracy, alleviate the overfitting problem, and eventually achieve better convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1117f-09bd-4f6a-ac08-d0fc6b823d13",
   "metadata": {},
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSyl7x7gcXynJl30VgC2_x7cIS-EXMZ-dfioZCGfvsY1sxVVja47qBjZUFCdHWhgY2oNYo&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f20b4e-7b22-4392-a49a-fbb8bf6c3037",
   "metadata": {},
   "source": [
    "Figure 6: Analysis of different types of DiffAugment on CIFAR-10 with 100% training data. \n",
    "- stronger한 DiffAugment가 극적으로 discriminator의 training 정확도(중간)와 validation 정확도(오른쪽) 차이를 감소할 수 있어 더 나은 수렴convergence에(왼쪽) 이르게 할 수 있다.\n",
    "    - A stronger DiffAugment can dramatically reduce the gap between the discriminator’s training accuracy (middle) and validation accuracy (right), leading to a better convergence (left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746411f-cc3a-4a66-9019-97f0bc98f8d2",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107f814a-2345-4c71-a0ed-5965691096c1",
   "metadata": {},
   "source": [
    "### ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387c893-d41f-406b-8f2d-1c4dc25abbb0",
   "metadata": {},
   "source": [
    "- We follow the top-performing model BigGAN [2] on ImageNet dataset at 128×128 resolution. \n",
    "- Additionally, we augment real images with random horizontal flips, yielding the best reimplementation of BigGAN to our knowledge (FID: ours 7.6 vs. 8.7 in the original paper [2]). \n",
    "- We use the simple Translation DiffAugment for all the data percentage settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f78c30-22f3-408a-af94-04abf53bfb71",
   "metadata": {},
   "source": [
    "###  FFHQ and LSUN-Cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabeb7b-d088-40d4-bf11-e73e72ee903d",
   "metadata": {},
   "source": [
    "- We further experiment with StyleGAN2 [18] on the FFHQ portrait dataset [17] and the LSUN-Cat dataset [46] at 256×256 resolution.\n",
    "- We investigate different limited data settings, with 1k, 5k, 10k, and 30k training images available.\n",
    "- We apply the strongest Color + Translation + Cutout DiffAugment to all the StyleGAN2 baselines without any hyperparameter changes. \n",
    "- The real images are also augmented with random horizontal flips as commonly applied in StyleGAN2 [18].\n",
    "- 성능이 모든 데이터 백분율 설정에서 고려할만했음!\n",
    "- 게다가 DiffAugment에 사용되는 고정 정책으로 성능이 adoptive augmentation strategy에 기반한 동시작업인 ADA와 동등함.\n",
    "    - Moreover, with the fixed policies used in DiffAugment, our performance is on par with ADA [16], a concurrent work based on the adaptive augmentation strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dad8e7-de08-45fc-8efa-9c52e8f108fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CIFAR-10 and CIFAR-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656c060-a9a6-4cb6-bfcb-3476998df59d",
   "metadata": {},
   "source": [
    "- experiment on the class-conditional BigGAN [2] and CR-BigGAN [50] and unconditional StyleGAN2 [18] models.\n",
    "- For a fair comparison, augment real images with random horizontal flips for all the baselines. \n",
    "\n",
    "For DiffAugment, we adopt **Translation + Cutout** for the BigGAN models, **Color + Cutout** for StyleGAN2 with 100% data, and **Color + Translation + Cutout** for StyleGAN2 with 10% or 20% data.\n",
    "-  Table 4로 미루어 보아 our method improves all the baselines independently of the baseline architectures, regularizations, and loss functions (hinge loss in BigGAN and non-saturating loss in StyleGAN2) without any hyperparameter changes.\n",
    "    - The improvements are considerable especially when limited data is available. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e3e59-a427-4c73-9a65-862d7e13dbdd",
   "metadata": {},
   "source": [
    "### Low-Shot Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56eb976-7fda-4d0e-93e8-d6720bc61eb8",
   "metadata": {},
   "source": [
    "image data가 제대로 수집이 되지 않았을때.\n",
    "- fine-tunre을 통해 사전 훈련된 모델을 적용 시키기도 하고\n",
    "    -  Wang et al. [45] use fine-tuning to transfer the knowledge of models pre-trained on external large-scale datasets.\n",
    "- 모델의 일부만을 fine-tune하기도 함\n",
    "    - Several works propose to fine-tune only part of the model [30,31,44]. \n",
    "\n",
    "논문에서 제안하는 것은 image data가 제대로 수집되지 않았을 때 외부 데이터셋이나 모델을 사용하지 않고 결쟁력있는 결과를 제공할 수 있을 뿐만 아니라 기존 전송 학습 방법과 직교한다는 것을 보여준다.\n",
    "- Below, we show that our method not only produces competitive results without using external datasets or models but also is orthogonal to the existing transfer learning methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53b039a-3f27-418b-a342-e0a697039818",
   "metadata": {},
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRFlqOukMAm5H9aJxZJ3M0DRN-kST79NJeffw&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e66976-c83c-464d-855c-4bb5055ce151",
   "metadata": {},
   "source": [
    "Figure 7: Style space interpolation of our method for low-shot generation without pre-training. \n",
    "- The smooth interpolation results suggest little overfitting of our method even given small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7dc524-c16c-4d23-86ea-669564905c96",
   "metadata": {},
   "source": [
    "For DiffAugment, we adopt Color + Translation + Cutout for StyleGAN2, Color + Cutout for both the vanilla fine-tuning algorithm TransferGAN [45] and FreezeD [30] that freezes the first several layers of the discriminator. \n",
    "\n",
    "Without any pre-training, we still achieve results on par with the existing transfer learning algorithms that require tens of thousands of images, with an exception on the 100-shot Obama dataset where pre-training with human faces clearly leads to better generalization. \n",
    "\n",
    "tiny dataset에서는 과적합의 문제가 있을 수 있지만 figure7은 style space의 선형 보간법을 통한 방법의 과적합이 적다는 것을 보여줌. \n",
    "- While there might be a concern that the generator is likely to overfit the tiny datasets (i.e., generating identical training images), Figure 7 suggests little overfitting of our method via linear interpolation in the style space [17]; please refer to the supplementary material for the nearest neighbor tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d9a8c-497d-4595-9962-22b5d446e2b0",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f0114-aba6-4644-9813-bdcfe6e1c20e",
   "metadata": {},
   "source": [
    "smaller model 이나 stronger regularization이 비슷하게 과접합을 줄일 수 있는지.또 DiffAugment 가 여전히 도움이 되는지 보기\n",
    "- Below, we investigate whether smaller model or stronger regularization would similarly reduce overfitting and whether DiffAugment still helps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928c970-a654-4410-af64-8eec2e2b3512",
   "metadata": {},
   "source": [
    "![](https://d3i71xaburhd42.cloudfront.net/dac89790fb29d5ddac521927f2fdbabc6d6cf3ab/12-Figure10-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae5605-611f-4858-9541-e5f1faae5b5b",
   "metadata": {},
   "source": [
    "Figure 8: Analysis of smaller models or stronger regularization on CIFAR-10 with 10% training data.\n",
    "- (a) Smaller models reduce overfitting for the BigGAN baseline, while our method dominates its performance at all model capacities. 작은 모델은 BigGAN 기준선에 대한 과적합을 줄이지만 모든 model capacities에서 성능이 지배적이다.\n",
    "- (b) Over a wide sweep of the R1 regularization γ for the baseline StyleGAN2, its best FID (26.87) is still much worse than ours (14.50). 기준선 StyleGAN2에 대해 R1 정규화의 wide sweep에 걸쳐, best FID는 논문에서 제시한 것보다 여전히 별로다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45de6ab-1690-4272-9ba6-a4d4a298835d",
   "metadata": {},
   "source": [
    "**Model Size Matters**\n",
    "- G,D의 채널 수를 점진적으로 절반으로 즐임으로서 model capacity 줄임\n",
    "    - We reduce the model capacity of BigGAN by progressively halving the number of channels for both G and D.\n",
    "- As plotted in Figure 8a, the baseline heavily overfits on CIFAR-10 with 10% training data when using the full model and achieves a minimum FID of 29.02 at 1/4 channels.\n",
    "- However, it is surpassed by our method over all model capacities. \n",
    "- 4분의 1 채널로 논문의 모델은 21.57의 훨씬 더 나은 FID를 성취했지만, 그 차이는 모델이 커지게 될 수록 단조롭게 증가한다.\n",
    "    - With 1/4 channels, our model achieves a significantly better FID of 21.57, while the gap is monotonically increasing as the model becomes larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f4113-715a-4246-afe8-8f798eee63df",
   "metadata": {},
   "source": [
    "**Stronger Regularization Matters**\n",
    "- StyleGAN2는 training을 안정화하기 위해 R1 정규화를 선택하고, 강도를 10에서 10의 4제곱까지 증가시킨다.\n",
    "    - As StyleGAN2 adopts the R1 regularization [27] to stabilize training, we increase its strength from γ = 0.1 to up to 104 and plot the FID curves in Figure 8b. \n",
    "- While we initially find that γ = 0.1 works best under the 100% data setting, the choice of γ = 103 boosts its performance from 34.05 to 26.87 under the 10% data setting. \n",
    "- When γ = 104 , within 750k iterations, we only observe a minimum FID of 29.14 at 440k iteration and the performance deteriorates after that. \n",
    "- Figure 8 (b)로 미루어보아 논문에서 제시한 DiffAugment가 정규화하는 것에 비해 효과적이라는 것을 알 수 있음\n",
    "    - However, its best FID is still 1.8× worse than ours (with the default γ = 0.1). This shows that DiffAugment is more effective compared to explicitly regularizing the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b12472-7f96-4ed6-9ba0-99340cf95e98",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/2596d75bcb594621a8fd45f36e8f0996.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzk1ODI3Mg==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fc6ea-5c7d-4e46-9add-5056825df369",
   "metadata": {},
   "source": [
    "Figure 9: Various types of DiffAugment consis\u0002tently outperform the baseline. We report Style\u0002GAN2’s FID on CIFAR-10 with 10% training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78cb52f-85d2-47ba-a1cf-ffbd47a3b2f5",
   "metadata": {},
   "source": [
    "**Choice of DiffAugment Matters**\n",
    "\n",
    "We investigate additional choices of DiffAugment in Figure 9, including random 90◦ rotations ({−90◦ , 0 ◦ , 90◦} each with 1/3 probability), Gaussian noise (with a standard deviation of 0.1), and general geometry transformations that involve bilinear interpolation, such as bilinear translation (within [−0.25, 0.25]), bilinear scaling (within [0.75, 1.25]), bilinear rotation (within [−30◦ , 30◦]), and bilinear shearing (within [−0.25, 0.25]). While all these policies consistently outperform the baseline, **we find that the Color + Translation + Cutout DiffAugment is especially effective**. The simplicity also makes it easier to deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e4cae-625a-447e-bdcf-d5fec7208c23",
   "metadata": {},
   "source": [
    "##  Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b174026-ccf0-4d30-a6f8-79cbbac5f449",
   "metadata": {},
   "source": [
    "- present DiffAugment for data-efficient GAN training. \n",
    "- DiffAugment reveals valuable observations that augmenting both real and fake samples effectively prevents the discriminator from over-fitting, and that the augmentation must be differentiable to enable both generator and discriminator training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e841be8-cc42-4f26-a062-c17eecbb0e78",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516343be-7f31-4126-a463-e7e40c5b71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def DiffAugment(x, policy='', channels_first=False):\n",
    "    if policy:\n",
    "        if channels_first:\n",
    "            x = tf.transpose(x, [0, 2, 3, 1])\n",
    "        for p in policy.split(','):\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x)\n",
    "        if channels_first:\n",
    "            x = tf.transpose(x, [0, 3, 1, 2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) - 0.5\n",
    "    x = x + magnitude\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) * 2\n",
    "    x_mean = tf.reduce_mean(x, axis=3, keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x):\n",
    "    magnitude = tf.random.uniform([tf.shape(x)[0], 1, 1, 1]) + 0.5\n",
    "    x_mean = tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True)\n",
    "    x = (x - x_mean) * magnitude + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    image_size = tf.shape(x)[1:3]\n",
    "    shift = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "    translation_x = tf.random.uniform([batch_size, 1], -shift[0], shift[0] + 1, dtype=tf.int32)\n",
    "    translation_y = tf.random.uniform([batch_size, 1], -shift[1], shift[1] + 1, dtype=tf.int32)\n",
    "    grid_x = tf.clip_by_value(tf.expand_dims(tf.range(image_size[0], dtype=tf.int32), 0) + translation_x + 1, 0, image_size[0] + 1)\n",
    "    grid_y = tf.clip_by_value(tf.expand_dims(tf.range(image_size[1], dtype=tf.int32), 0) + translation_y + 1, 0, image_size[1] + 1)\n",
    "    x = tf.gather_nd(tf.pad(x, [[0, 0], [1, 1], [0, 0], [0, 0]]), tf.expand_dims(grid_x, -1), batch_dims=1)\n",
    "    x = tf.transpose(tf.gather_nd(tf.pad(tf.transpose(x, [0, 2, 1, 3]), [[0, 0], [1, 1], [0, 0], [0, 0]]), tf.expand_dims(grid_y, -1), batch_dims=1), [0, 2, 1, 3])\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    image_size = tf.shape(x)[1:3]\n",
    "    cutout_size = tf.cast(tf.cast(image_size, tf.float32) * ratio + 0.5, tf.int32)\n",
    "    offset_x = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[0] + (1 - cutout_size[0] % 2), dtype=tf.int32)\n",
    "    offset_y = tf.random.uniform([tf.shape(x)[0], 1, 1], maxval=image_size[1] + (1 - cutout_size[1] % 2), dtype=tf.int32)\n",
    "    grid_batch, grid_x, grid_y = tf.meshgrid(tf.range(batch_size, dtype=tf.int32), tf.range(cutout_size[0], dtype=tf.int32), tf.range(cutout_size[1], dtype=tf.int32), indexing='ij')\n",
    "    cutout_grid = tf.stack([grid_batch, grid_x + offset_x - cutout_size[0] // 2, grid_y + offset_y - cutout_size[1] // 2], axis=-1)\n",
    "    mask_shape = tf.stack([batch_size, image_size[0], image_size[1]])\n",
    "    cutout_grid = tf.maximum(cutout_grid, 0)\n",
    "    cutout_grid = tf.minimum(cutout_grid, tf.reshape(mask_shape - 1, [1, 1, 1, 3]))\n",
    "    mask = tf.maximum(1 - tf.scatter_nd(cutout_grid, tf.ones([batch_size, cutout_size[0], cutout_size[1]], dtype=tf.float32), mask_shape), 0)\n",
    "    x = x * tf.expand_dims(mask, axis=3)\n",
    "    return x\n",
    "\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a49e8d-ca49-4a56-b604-a2ca31887667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def DiffAugment(x, policy='', channels_first=True):\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        for p in policy.split(','):\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x)\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x):\n",
    "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x):\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x):\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2).contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
    "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
    "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
    "    mask[grid_batch, grid_x, grid_y] = 0\n",
    "    x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e89048-d559-48ed-8c31-a52a97f5f72a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from DiffAugment_pytorch import DiffAugment\n",
    "from DiffAugment_tf import DiffAugment\n",
    "policy = 'color,translation,cutout' # If your dataset is as small as ours (e.g.,\n",
    "# hundreds of images), we recommend using the strongest Color + Translation + Cutout.\n",
    "# For large datasets, try using a subset of transformations in ['color', 'translation', 'cutout'].\n",
    "# Welcome to discover more DiffAugment transformations!\n",
    "\n",
    "...\n",
    "# Training loop: update D\n",
    "reals = sample_real_images() # a batch of real images\n",
    "z = sample_latent_vectors()\n",
    "fakes = Generator(z) # a batch of fake images\n",
    "real_scores = Discriminator(DiffAugment(reals, policy=policy))\n",
    "fake_scores = Discriminator(DiffAugment(fakes, policy=policy))\n",
    "# Calculating D's loss based on real_scores and fake_scores...\n",
    "...\n",
    "\n",
    "...\n",
    "# Training loop: update G\n",
    "z = sample_latent_vectors()\n",
    "fakes = Generator(z) # a batch of fake images\n",
    "fake_scores = Discriminator(DiffAugment(fakes, policy=policy))\n",
    "# Calculating G's loss based on fake_scores...\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
