{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TensorFlow Category 5\n",
    "> Sequence (시계열) 데이터 다루기\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: 최서연\n",
    "- categories: [TensorFlow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 5. Sequences(sunspots)\n",
    "\n",
    "type A\n",
    "```python\n",
    "tf.keras.layers.Lambda(lambda x: x + 400)\n",
    "```\n",
    "lambda를 사용하여 sequence에 사용하여 문제가 제시됨(건들지 말라고 문제에서 언급될 것)\n",
    "\n",
    "type B\n",
    "```python\n",
    "min = np.min(series)\n",
    "max = np.max(series)\n",
    "seroes -= min\n",
    "seroes /= max\n",
    "time = np.array(time_step)\n",
    "```\n",
    "\n",
    "[0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "window_size = 3, shift = 1\n",
    "- 3일의 데이터(0,1,2)를 가지고 타겟(예측값인 3) 단일 예측할거야\n",
    "- 3일의 데이터(1,2,3)를 가지고 타겟(예측값인 4) 단일 예측할거야\n",
    "- ...\n",
    "\n",
    "window_size = 5, shift = 2\n",
    "- 5일의 데이터(0,1,2,3,4)를 가지고 타겟(예측값인 5) 단일 예측할거야\n",
    "- 5일의 데이터(2,3,4,5,6)를 가지고 타겟(예측값인 7) 단일 예측할거야\n",
    "- ...\n",
    "\n",
    "5번 문제는 many to many문제 \n",
    "- 학습 데이터(0,1,2,3,4)가지고 타겟(예측값1,2,3,4,5) sequence 예측,\n",
    "- window_size = 5\n",
    "\n",
    "5번 문제는 many to many문제 가능_하지만 시험에서는 위에 문제 나옴\n",
    "- 학습 데이터(0,1,2,3,4)가지고 타겟(예측값5,6,7,8,9) sequence 예측,\n",
    "- window_size = 5\n",
    "\n",
    "$\\star$ many to many 기법$\\star$ \n",
    "\n",
    "drop_remainder = False\n",
    "- False로 줘야 window size에 안맞는 예들이 사라짐\n",
    "- [0,1,2,3,4,5]있을때 0,1,2,가지고 shift 1일때 3,4,5 예측 후 (4,5) 혹은 (5) 이렇게 입력값과 출력값이 many to many의 수가 같지 않은 것 사라짐\n",
    "\n",
    "buffer_size = 30,000(full_shuffle)\n",
    "- 0~29,999 셔플\n",
    "\n",
    "buffer_size = 1,000\n",
    "- 0~999 셔플\n",
    "-999~1,999 셔플\n",
    "- 1,999~2,999 셔플\n",
    "- ...\n",
    "- 28,999~29,999 셔플\n",
    "\n",
    "```python\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "\tseries = tf.expand_dims(series, axis=-1) # 차원을 1차원에서 2차원으로 늘려줌\n",
    "\tds = tf.data.Dataset.from_tensor_slices(series) # dataset으로 변환 아래 옵션 사용위해 데이터셋으로서의 변환이 필요\n",
    "\tds = ds.window(window_size + 1, shift = 1, drop_remainder=True) # 1을 더해주는 이유는 window 크기가 y의 끝까지 다 포함되도록 하기 위해서, 만약 (1,2,3)으로 (4,5,6) 예측하면 window_size + 3이 되겠지. 지금은 (1,2,3)으로 (2,3,4)예측하니까 window_size+1임\n",
    "\tds = flat_map(lambda w: w.batch(sindow_size + 1)) # 데이터가 [[1],[2],[3],[4],[5]]이렇게 묶이는데 flat_map쓰면 [1,2,3,4,5] 펼쳐준다. window_size는 위와 같게함 여기서는 +1으로서 같게 함\n",
    "\tds = ds.shuffle(shuffle_buffer)\n",
    "\tds = ds.map(lambda w : (w[:-1],w[1:])) # x,y데이터 분할 지금 window_size+1놓았을때 window_size가 5여서 총 6이 입력된다면, 예를들어 [0,1,2,3,4,5]이렇게 6개 있다치면 x는 -1번째까지, [0,1,2,3,4]가 되고, y는 [1,2,3,4,5]가 된다. \n",
    "\t# 만약 [0,1,2,3,4,5]에서 x가[0,1,2,3,4]이고 y로서 5만 예측하고 싶다면 (2[:-1].w[-1])해주면 된다.(many to one) 보토 주가예측 모델 만들때\n",
    "\treturn ds.batch(batch_size).prefetch(1) # 하나의 배치를 미리 더 만들어라, 미리 만들어 놓기 때문에 배치가 실행되는 속도가 빨라진다.\n",
    "```\n",
    "\n",
    "Conc1D\n",
    "- 1차원 합성곱\n",
    "- kernal_size는 단일 데이터\n",
    "- strides는 몇 칸씩 이동하며 학습할 건지 정하는 역할\n",
    "- padding = 'causal' 원인을 남겨둔다 이해하면 될 듯 앞의 결과를 남겨두기 위한 causal옵션\n",
    "\n",
    "Stochastic Gradient Descent(SGD)\n",
    "- 경사하강법 : 경사를 타고 내려와 최저점(loss가 가장 낮은 지점)에 도달하는 최적화 알고리즘\n",
    "- 학습율이 너무 작은 경우 학습이 진행되지 않는다.\n",
    "- 학습율이 너무 큰 경우 minimum에 도달하지 않는다.\n",
    "\n",
    "Momentum(관성)\n",
    "- momentum = 0.0 마찰력이 높다는 뜻. local minimum value에 수렴하는 오류가 생긴다. global minimun value에 수렴해야 하는데!\n",
    "- momentum = 0.9로 보통 놓음 마찰력을 줄이는 역할을 한다.\n",
    "\t- adam은 기본적으로 마찰력 낮춰져 있음\n",
    "\n",
    "Optimizer\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.SGD(lr = 0.0001,momentum=0.9)\n",
    "model.compile(loss = loss, optimizer, metrics[\"mae\"]\n",
    "```\n",
    "\n",
    "Huber Loss\n",
    "- 어떤 특정 임계값 기준으로\n",
    "- 오차가 0에 가까울수록 MSE(제곱평균오차)\n",
    "- 오차가 0에서 멀어질수록 MAE(절대값평균오차)\n",
    "- 두 loss가 섞여 있음!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRSKbgK8PRs5"
   },
   "source": [
    "## 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc4QcKvRPSj-"
   },
   "source": [
    "1. GPU 옵션 켜져 있는지 확인할 것!!! (수정 - 노트설정 - 하드웨어설정 (GPU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNPjnA62PXVn"
   },
   "source": [
    "## 순서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T463L0aPPX_n"
   },
   "source": [
    "1. **import**: 필요한 모듈 import\n",
    "2. **전처리**: 학습에 필요한 데이터 전처리를 수행합니다.\n",
    "3. **모델링(model)**: 모델을 정의합니다.\n",
    "4. **컴파일(compile)**: 모델을 생성합니다.\n",
    "5. **학습 (fit)**: 모델을 학습시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1Hj9c1NPbPu"
   },
   "source": [
    "## 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcvEYUuhPb3f"
   },
   "source": [
    "For this task you will need to train a neural network\n",
    "to predict sunspot activity using the Sunspots.csv\n",
    "provided. \n",
    "\n",
    "Your neural network is expected to have an MAE\n",
    "of at least 20, with top marks going to one with an MAE\n",
    "of around 15. \n",
    "\n",
    "At the bottom is provided some testing\n",
    "code should you want to check before uploading which measures\n",
    "the MAE for you. \n",
    "\n",
    "Strongly recommend you test your model with\n",
    "this to be able to see how it performs.\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------\n",
    "**Sequence(시퀀스)**\n",
    "\n",
    "Sunspots.csv를 사용하여 **태양 흑점 활동(sunspot)**을 예측하는 인공신경망을 만듭니다.\n",
    "\n",
    "MAE 오차 기준으로 최소 20이하로 예측할 것을 권장하며, 탑 랭킹에 들려면 MAE 15 근처에 도달해야합니다.\n",
    "\n",
    "아래 주어진 샘플코드는 당신의 모델을 테스트 하는 용도로 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C3ewm9XQHgr"
   },
   "source": [
    "-----------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTUdLmS7TE40"
   },
   "source": [
    "## 필요한 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqt0NBn4MZl5"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixYVm16KMZl7"
   },
   "outputs": [],
   "source": [
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
    "urllib.request.urlretrieve(url, 'sunspots.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LPl4l2AMZl9"
   },
   "source": [
    "## csv 파일로부터 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpW4dyWaMZl-"
   },
   "source": [
    "csv.reader() 함수를 활용합니다.\n",
    "\n",
    "* 첫번째 파라미터에는 file을 , delimiter에는 구분자를 넣어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BamvZ6AvMZl-"
   },
   "outputs": [],
   "source": [
    "with open('sunspots.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        print(row)\n",
    "        i+=1\n",
    "        if i > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxeMEobmNUdo"
   },
   "source": [
    "빈 list를 만들어 줍니다. (sunspots, time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dar7eoERNZvF"
   },
   "outputs": [],
   "source": [
    "sunspots = []\n",
    "time_step = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neOkzKg4MZmC"
   },
   "source": [
    "`time_step`에는 **index** 값을, `sunspots`에는 sunspots의 정보를 넣어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0o2dSwVMZmC"
   },
   "outputs": [],
   "source": [
    "with open('sunspots.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    # 첫 줄은 header이므로 skip 합니다.\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        sunspots.append(float(row[2]))\n",
    "        time_step.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKgsyCLRTNgS"
   },
   "source": [
    "sunspots, time_step 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eECxANX-TKaj"
   },
   "outputs": [],
   "source": [
    "sunspots[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npwlA3tSTLgF"
   },
   "outputs": [],
   "source": [
    "time_step[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UKs4AbeNfqu"
   },
   "source": [
    "sunspots와 time_step을 `numpy array`로 변환합니다.\n",
    "\n",
    "* 참고: 모델은 list 타입을 받아들이지 못합니다. 따라서, numpy array 로 변환해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8MrSCu3MZmE"
   },
   "outputs": [],
   "source": [
    "series = np.array(sunspots)\n",
    "time = np.array(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fky6hg0yTkDk"
   },
   "outputs": [],
   "source": [
    "series.shape, time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TF0xdyV7Tap3"
   },
   "source": [
    "## 태양의 흑점 활동 (sunspots) 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnTzXWAFMZmG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(time, series)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-YdTCYeMZmc"
   },
   "source": [
    "## Train Set, Validation Set 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRfGcPJET7Dq"
   },
   "source": [
    "3000 인덱스를 기준으로 Train / Validation Set를 분할 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9TsEUONT6nF"
   },
   "outputs": [],
   "source": [
    "split_time = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QXuDIkTMZmd"
   },
   "outputs": [],
   "source": [
    "time_train = time[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "\n",
    "x_train = series[:split_time]\n",
    "x_valid = series[split_time:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amk_ksmDMZmb"
   },
   "source": [
    "## Window Dataset Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9VGhKTIMZmQ"
   },
   "source": [
    "자세한 Dataset 활용법은 [블로그 링크](https://teddylee777.github.io/tensorflow/dataset-batch-window)를 참고해 보시고, 연습해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k161f1YQMZmR"
   },
   "outputs": [],
   "source": [
    "# 윈도우 사이즈\n",
    "window_size=30\n",
    "# 배치 사이즈\n",
    "batch_size = 32\n",
    "# 셔플 사이즈\n",
    "shuffle_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfyNoztDMZmb"
   },
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPxvjABJWDro"
   },
   "source": [
    "`train_set`와 `validation_set`를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mLFgMMBMZmg"
   },
   "outputs": [],
   "source": [
    "train_set = windowed_dataset(x_train, \n",
    "                             window_size=window_size, \n",
    "                             batch_size=batch_size,\n",
    "                             shuffle_buffer=shuffle_size)\n",
    "\n",
    "validation_set = windowed_dataset(x_valid, \n",
    "                                  window_size=window_size,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle_buffer=shuffle_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2k1dYQcMZmj"
   },
   "source": [
    "## 모델 정의 (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntF-9mrFMZmn"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('https://i.stack.imgur.com/NmYZJ.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLJ21qRtMZmp"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    tf.keras.layers.Conv1D(60, kernel_size=5,\n",
    "                         padding=\"causal\",\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=[None, 1]),\n",
    "    tf.keras.layers.LSTM(60, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(60, return_sequences=True),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    tf.keras.layers.Lambda(lambda x: x * 400)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1bif1TuUl1m"
   },
   "source": [
    "모델의 구조 요약을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgET749LMZmt"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXKGQ3rmMZmv"
   },
   "source": [
    "## 컴파일 (compile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ1ETr6mUuqB"
   },
   "source": [
    "**Optimizer**는 SGD(Stochastic Gradient Descent) 를 사용합니다.\n",
    "\n",
    "* lr(learning_rate): 학습률입니다.\n",
    "* momentum: 모멘텀 (가중치) 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjBesK-SMZmv"
   },
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=1e-5, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYmPEGtZMZmx"
   },
   "source": [
    "**Huber Loss**: MSE와 MAE를 절충한 후버 손실(Huber loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eVR4UNKMZmx"
   },
   "source": [
    "\\begin{split}L_{\\delta}=\\left\\{\\begin{matrix}\n",
    "\\frac{1}{2}(y - \\hat{y})^{2} & if \\left | (y - \\hat{y})  \\right | < \\delta\\\\\n",
    "\\delta ((y - \\hat{y}) - \\frac1 2 \\delta) & otherwise\n",
    "\\end{matrix}\\right.\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqIDvyQZMZmy"
   },
   "outputs": [],
   "source": [
    "loss= Huber()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICeNa8kQVDxz"
   },
   "source": [
    "model.compile()시 우리가 튜닝한 **optimizer**와 **loss**를 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NrbT-cAMZm2"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJ0gsuoqSv7z"
   },
   "source": [
    "## ModelCheckpoint: 체크포인트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXHmDZ2aSx4O"
   },
   "source": [
    "`val_loss` 기준으로 epoch 마다 최적의 모델을 저장하기 위하여, ModelCheckpoint를 만듭니다.\n",
    "* `checkpoint_path`는 모델이 저장될 파일 명을 설정합니다.\n",
    "* `ModelCheckpoint`을 선언하고, 적절한 옵션 값을 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1JJZyg0MZm5"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = 'tmp_checkpoint.ckpt'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             save_weights_only=True, \n",
    "                             save_best_only=True, \n",
    "                             monitor='val_mae',\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1leCk2zKMZm7"
   },
   "source": [
    "## 학습 (fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frdpb5SEMZm7"
   },
   "outputs": [],
   "source": [
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrDA4DzbMZm8"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_set, \n",
    "                    validation_data=(validation_set), \n",
    "                    epochs=epochs, \n",
    "                    callbacks=[checkpoint],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shzhTOjAninH"
   },
   "source": [
    "## 학습 완료 후 Load Weights (ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLqb_6XrMvdq"
   },
   "source": [
    "학습이 완료된 후에는 반드시 `load_weights`를 해주어야 합니다.\n",
    "\n",
    "그렇지 않으면, 열심히 ModelCheckpoint를 만든 의미가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jO1ucZ9ninH"
   },
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1t0xRupR1LmK"
   },
   "source": [
    "## 학습 오차에 대한 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwus5OLdFg2t"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8lQCKvaMZnC"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(np.arange(1, epochs+1), history.history['loss'])\n",
    "plt.plot(np.arange(1, epochs+1), history.history['val_loss'])\n",
    "plt.title('Loss / Val Loss', fontsize=20)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['loss', 'val_loss'], fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzL9Jv_SMZnE"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(np.arange(1, epochs+1), history.history['mae'])\n",
    "plt.plot(np.arange(1, epochs+1), history.history['val_mae'])\n",
    "plt.title('MAE / Val MAE', fontsize=20)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(['mae', 'val_mae'], fontsize=15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
