{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c558f67-3541-4564-8b06-db80ad9d20f7",
   "metadata": {},
   "source": [
    "# Instance Selection for GANs\n",
    "> Terrance DeVries, Michal Drozdzal, Graham W. Taylor\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: 최서연"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3046e-528c-4fc0-bd0a-9402a3d8c795",
   "metadata": {},
   "source": [
    "ref: https://arxiv.org/pdf/2007.15255.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daff7a9-3f65-4fd1-a1f2-2e976c2fc749",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580aeef-81d7-47d8-9137-186b61d15999",
   "metadata": {},
   "source": [
    "Several recently proposed techniques attempt to avoid spurious samples, either by rejecting them after generation, or by truncating the model’s latent space.\n",
    "- 최근 제안된 기술들은 가짜 샘플들을 피하려는 시도는 생성 후 거절하거니 모델의 잠재 공간을 잘라내는 것임.\n",
    "- 효과적이긴 한데 모델의 대부분이 사용되지 않는 샘플에 할당"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade8892-e27d-4b35-a42f-67c58a4841f3",
   "metadata": {},
   "source": [
    "altering the training dataset via instance selection before model training has taken place. \n",
    "- 그래서 모델 학습이 일어나기 전에 인스턴스 선택을 통해 학습셋을 변경하는 것을 제안할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e9a15-3372-44a3-aafe-991f6dc19ca4",
   "metadata": {},
   "source": [
    "## Instance Selection for GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91deac67-1475-415c-a272-8cfce16467ab",
   "metadata": {},
   "source": [
    "- to automatically remove the sparsest regions of the data manifold, specifically those parts that GANs struggle to capture. \n",
    "- define an image embedding function F and a scoring function H."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b3e81-80c8-4d59-909e-e494b0797721",
   "metadata": {},
   "source": [
    "**Embedding function**\n",
    "\n",
    "- F projects images into an embedding space\n",
    "    - image z data set이 주어지면, $z = F(x)$를 data point $x ∈ X $에 적용하여 embeded image Z 가 주어진다.\n",
    "    - image  generation을 위해 사전 학습된 image classifier의 feature space와 같은 aligned embedding function을 제안하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcfab9f-3288-4e87-a1de-235538abc292",
   "metadata": {},
   "source": [
    "**Scoring function**\n",
    "- H is used to to assess the manifold density in a neighbourhood around each embedded data point z.\n",
    "    - 논문에서 비교할 세 가지 scoring function selection\n",
    "        - *log likelihood under a standard Gaussian model,\n",
    "        - log likelihood under a Probabilistic Principal Component Analysis (PPCA) model,\n",
    "        - distance to the Kth nearest neighbour (KNN Distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b79e6-a168-4c47-83d5-77740a064019",
   "metadata": {},
   "source": [
    "The Gaussian model is fit to the *embedded dataset by computing the empirical mean $µ$ and the sample covariance $Σ$ of $Z$.*\n",
    "- d는 z의 demension\n",
    "\n",
    "$$H_{Gaussian}(z) = −\\frac{1}{2}[ln(|Σ|) + (z − µ)^{T} Σ^{−1}(z − µ) + d ln(2π)], (1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e3caa-094f-46aa-bf5b-03401cc8fa8b",
   "metadata": {},
   "source": [
    "- 논문 설정: set the number of principal components such that 95% of the variance in the data is preserved.\n",
    "\n",
    "$$H_{PPCA}(z) = −\\frac{1}{2}[ln(|C|) + Tr((z − µ)^{T} C^{−1}(z − µ)) + d ln(2π)], C = WW^T + σ^2 I, (2)$$\n",
    "- $W$ is the fit model weight matrix,\n",
    "- $µ$ is the empirical mean of $Z$, \n",
    "- $σ$ is the residual variance, \n",
    "- $I$ is the identity matrix, \n",
    "- $d$ is the dimension of $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3cb269-5312-42cb-bc34-7b365efcc9d3",
   "metadata": {},
   "source": [
    "**KNN**\n",
    "- $z$와 $Z \\ {z}$의 유클리드 거리 계산 후 가장 가까운 k번째 원소까지 거리 반환해 data point 얻는데 사용한다.\n",
    "- To convert to a score, we make the resulting distance negative, such that smaller distances return larger values. \n",
    "\n",
    "$$H_{KNN}(z, K, Z) = − min\\underset{K}  \\{||z − z_i ||_2 : z_i ∈ Z \\ {z} \\}, (3)$$\n",
    "\n",
    "- 집합에서 k번째 가장 작은 값.$\\leftarrow$논문에서는 k=5로 정함\n",
    "- To perform *instance selection*, we compute scores $H(F(x))$ for each data point and keep all data points with scores above some threshold $ψ$.\n",
    "- For convenience, *we often set $ψ$ to be equal to some percentile of the scores, such that we preserve the top N% of the best scoring data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4d822-f1c8-42d7-9781-eec3e17ac352",
   "metadata": {},
   "source": [
    "Figure 1에서 High likelihood images share a similar visual structure, while low likelihood samples are more varied 였음!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1247c213-4cca-42cd-b4e3-dfed39b71aad",
   "metadata": {},
   "source": [
    "$$X' = {x ∈ X s.t. H(F(x)) > ψ}$$\n",
    "- data points $x ∈ X$ 의 초기 학습 set을 구성함으로써 reduced training set $X'$를 구성함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8913aab-34e5-4a7a-b4e6-a7084931f9eb",
   "metadata": {},
   "source": [
    "Figure 1에서 ImageNet의 Red Fox class 에서 most and least likely imgaed를 보면 training set으로부터 data points를 제거하는 것이 좋은 이유가 설명된다.\n",
    "\n",
    "Likelihood는 pretrain된 Inceptionv3 classifier에서 feature embedding에 적합한 가우시안모델에 의해 결정된다.\n",
    "\n",
    "- The most likely images (a) are similarly cropped around the fox’s face, while the least likely images (b) have many odd viewpoints and often suffer from occlusion. It is logical to imagine how a generative model trained on these unusual instances may try to generate samples that mimic such conditions, resulting in undesirable outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201ddad-07ad-44b8-acb3-82080abcd844",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15ebb3-f71c-4cd4-8bbc-b3d6e781c985",
   "metadata": {},
   "source": [
    "- review evaluation metrics,\n",
    "- motivate selecting instances based on manifold density, \n",
    "- analyze the impact of applying instance selection to GAN training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa776b0-9fed-43dc-95ef-c0633932d0a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870ac72-9a6a-42fa-9dec-f209b77ca2c6",
   "metadata": {},
   "source": [
    "- When calculating FID we follow Brock et al. [2] in using all images in the training set *to estimate the reference distribution*, and *sampling 50 k images* to make up the generated distribution.\n",
    "- For P&R and D&C we use an Inceptionv3 embedding.\n",
    "- 1 N and M are **set to 10 k samples** for both the reference and generated distributions, and K is **set equal to 5** as recommended by Naeem et al. [19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ef1c3-9af2-4383-9b9b-938b9024f979",
   "metadata": {},
   "source": [
    "### Relationship Between Dataset Manifold Density and GAN Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89786861-a9d5-4bb9-ac4d-7220745381b4",
   "metadata": {},
   "source": [
    "- image manifold는 많은 data point들이 서로 가까이에 있는 영역에서보다 정확히 정의된다.\n",
    "- GAN은 주어진 dataset의 data point를 기반으로 image manifold를 재현하려고 시도하기 떄문에 잘 정의된 manifold(no sparse manifold regions)가 있는 dataset에서 더 나은 성능을 발휘해야 한다고 suspect한다.\n",
    "    - 그래서 use the ImageNet2 dataset [7] and treat each of the 1000 classes as a separate dataset 할거다\n",
    "    - use a single class-conditional BigGAN from [2] that has been pretrained on ImageNet at 128 × 128 resolution. \n",
    "    - For each class, we sample 700 real images from the dataset, and generate 700 class-conditioned samples with the BigGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e8903-9284-485f-b3cf-d60144d2f782",
   "metadata": {},
   "source": [
    "To measure the density for each class manifold we compare three different methods: \n",
    "- Gaussian likelihood, \n",
    "- Probabilistic Principal Component Analysis (PPCA) likelihood,\n",
    "- and distance to the Kth neighbour (KNN Distance) (§3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b52b8f-c4ce-4b7d-ac20-ec33999fc225",
   "metadata": {},
   "source": [
    "![](https://d3i71xaburhd42.cloudfront.net/d534182c1a64143e74e9f00fd7394b9223fe62a0/5-Figure2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3100d-2825-4b66-9a10-3ff48baa73d8",
   "metadata": {},
   "source": [
    "Figure 2. image data set의 각 class에 대한 manifold 밀도 추정치와 FID 사이의 상관관계. x측 값이 낮을수록 dataset manifold의 밀도가 높다는 것을 나타냄. y축 값이 낮을수록 sample의 품질이 우수함을 나타냄."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0dacb-9d8d-4a04-b2ce-7458e5a3036d",
   "metadata": {},
   "source": [
    "### Embedding and Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508b57b-de78-4bbf-94f0-667b5f5014a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
