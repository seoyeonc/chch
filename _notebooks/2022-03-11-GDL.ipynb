{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54cf825-af06-41ed-ad14-5103a7b9548f",
   "metadata": {
    "id": "cac470df-29e7-4148-9bbd-d8b9a32fa570",
    "tags": []
   },
   "source": [
    "# Geometric deep learning: going beyond Euclidean data\n",
    "> Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: 최서연\n",
    "- categories: [논문리뷰]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4832237e-092a-4071-ae2a-d890cf339138",
   "metadata": {},
   "source": [
    "- Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds.\n",
    "    - 기하학적 딥러닝: 그래프나 매니폴드같은 비유클리디언 도메인에서 구조화된 깊은 신경망 모델을 일반화하는 등장하는 기술 시도를 포괄하는 용어\n",
    "- The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f171a-2cf7-4fbe-b9f7-bfe4a6d652f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### I. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1811c9e0-0d90-4602-b4e3-c7ec6276ee95",
   "metadata": {},
   "source": [
    "- The use of convolutions has a two-fold effect. \n",
    "    - First, it allows extracting local features that are shared across the image domain and greatly reduces the number of parameters in the network with respect to generic deep architectures (and thus also the risk of overfitting), without sacrificing the expressive capacity of the network.\n",
    "    - 적은 수의 파라메터, 로컬 특징 추출\n",
    "    - Second, the convolutional architecture itself imposes some priors about the data, which appear very suitable especially for natural images\n",
    "    - 자연 이미지에 적합한 우선순위 부과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b219a2-060f-43f3-9cd5-04e2d85fbfe0",
   "metadata": {},
   "source": [
    "- The non-Euclidean nature of such data implies that there are no such familiar properties as global parameterization, common system of coordinates, vector space structure, or shift-invariance. \n",
    "- Consequently, basic operations like convolution that are taken for granted in the Euclidean case are even not well defined on non-Euclidean domains. \n",
    "    - 유클리드레서 당연한 컨벌루션같은 기본 연산은 비유클리드 도메인에서 잘 정의되지 않음!\n",
    "- The purpose of our paper is to show different methods of translating the key ingredients of successful deep learning methods such as convolutional neural networks to non-Euclidean data.\n",
    "    - 목적: 비유클리디안 데이터에 컨벌루션 신경망같은 성공적인 딥러닝 방법의 주요 구성을 변환하는 다양한 방법을 보여주기~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e1fdfc-93c4-4c6a-ae78-e440b332cd4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### II. GEOMETRIC LEARNING PROBLEMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ce927-f38c-4fee-969a-848f26797143",
   "metadata": {},
   "source": [
    "- Broadly speaking, we can distinguish between two classes of geometric learning problems.\n",
    "- In the first class of problems, the goal is to characterize the structure of the data.\n",
    "    - 목적은 데이터 구조를 특성화하는 것\n",
    "- The second class of problems deals with analyzing functions defined on a given non-Euclidean domain. \n",
    "- These two classes are related, since understanding the properties of functions defined on a domain conveys certain information about the domain, and vice-versa, the structure of the domain imposes certain properties on the functions on it.\n",
    "    - 도메인에 정의된 함수의 특성을 이해하는 것은 도메인에 대한 특정 정보를 전달하고, 그 반대로 도매안의 구조는 도메인에서 함수에 특성 속성을 부과하기 때문에 구 클래스는 연관이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba72cf2-ad6b-4283-a856-b8d13e21f9fc",
   "metadata": {},
   "source": [
    "**Structure of the domain:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a16be-9474-4aa5-8f30-7b7d84da397c",
   "metadata": {},
   "source": [
    "- Many methods for nonlinear dimensionality reduction consist of two steps: \n",
    "    - first, they start with constructing a representation of local affinity of the data points (typically, a sparsely connected graph). \n",
    "        - 데이터 포인트의 로컬 관련성 구성\n",
    "    - Second, the data points are embedded into a low-dimensional space trying to preserve some criterion of the original affinity.\n",
    "        - 데이터 포인트는 저차원 공간에 임베드됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72984008-385b-44ae-8ae2-da6b37731dc7",
   "metadata": {},
   "source": [
    "Instead of embedding the vertices, the graph structure can be processed by decomposing it into small sub-graphs called motifs [36] or graphlets [37].\n",
    "- 정점 임베딩하는 대신 그래프 구조는 모티브나 그래프릿같은 작은 서브 그래프에서 분해함으로써 처리 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4ab0a5-d898-4cf4-9289-2de077ebbc97",
   "metadata": {},
   "source": [
    "- In network analysis applications such as computational sociology, the topological structure of the social graph representing the social relations between people carries important insights allowing, for example, to classify the vertices and detect communities [40].\n",
    "    - 네트워크 분석 어플리케이션은 정점을 분류하고 커뮤니티 탐지\n",
    "- In natural language processing, words in a corpus can be represented by the co-occurrence graph, where two words are connected if they often appear near each other [41].\n",
    "    - 자연어 처리에서는 두 단어가 서로 근처에 나타나면 연결된 동시 발생 그래프로 표현 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e484f7f-20ba-4205-a21b-888659e6735e",
   "metadata": {},
   "source": [
    "**Data on a domain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a5b06-647b-430e-ba72-017ff1b37b63",
   "metadata": {},
   "source": [
    "- Our second class of problems deals with analyzing functions defined on a given non-Euclidean domain.\n",
    "- We can further break down such problems into two subclasses: problems where the domain is fixed and those where multiple domains are given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdae23a-2680-4390-be05-d0be0a2a914c",
   "metadata": {},
   "source": [
    "- In computer graphics and vision applications, finding similarity and correspondence between shapes are examples of the second sub-class of problems: each shape is modeled as a manifold, and one has to work with multiple such domains.\n",
    "    - 각 모양은 매니폴드로 모델와되고, 하나는 여러 도메인에서 작업해야 한다.\n",
    "- In this setting, a generalization of convolution in the spatial domain using local charting [46], [47], [48] appears to be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df59d2-d1e6-4738-9763-188fca4e5093",
   "metadata": {},
   "source": [
    "**Brief history**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918792d-ed01-45bf-976c-fd877ac9a22d",
   "metadata": {},
   "source": [
    "- The main focus of this review is on this second class of problems, namely learning functions on non-Euclidean structured domains, and in particular, attempts to generalize the popular CNNs to such settings.\n",
    "- First attempts to generalize neural networks to graphs we are aware of are due to Scarselli et al. [49], who proposed a scheme combining recurrent neural networks and random walk models.\n",
    "    - 그래프에서 신경망을 일반화하려는 첫번째 시도는 Scarselli이 함~ 임의보행 모델과 반복 신경망 모델의 결합을 제안함\n",
    "- The first formulation of CNNs on graphs is due to Bruna et al. [52], who used the definition of convolutions in the spectral domain.\n",
    "    - 그레프에서 cnn을 첫번째 공식화한 건 Bruna, 스펙트럼 도메인에서 컨벌루션 정의를 사용!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83eb14-e137-46fb-81c3-2e56739d2a54",
   "metadata": {},
   "source": [
    "- Their paper, while being of conceptual importance, came with significant computational drawbacks that fell short of a truly useful method.\n",
    "    - 유용한데 계산상의 결점이 존재.\n",
    "- These drawbacks were subsequently addressed in the followup works of Henaff et al.[44] and Defferrard et al. [45].\n",
    "- In the latter paper, graph CNNs allowed achieving some state-of-the-art results.\n",
    "    - 후에 해결책 제시하는듯?!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c534c-4700-499b-8ac1-ceaae18ebedc",
   "metadata": {},
   "source": [
    "- In a parallel effort in the computer vision and graphics community, Masci et al. [47] showed the first CNN model on meshed surfaces, resorting to a spatial definition of the convolution operation based on local intrinsic patches.\n",
    "    - 평행적 노력: 로컬 고유 패치를 기반으로 한 컨벌루션 연산의 공간적 정의에 의존한 매쉬드 표면에서 첫번째 CNN 모델을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cf3176-20f7-4ef3-9b79-f354ec78e9fc",
   "metadata": {},
   "source": [
    "- The interest in deep learning on graphs or manifolds has exploded in the past year, resulting in numerous attempts to apply these methods in a broad spectrum of problems ranging from biochemistry [55] to recommender systems [56]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c53c1c-69f2-47c0-8d6c-8a65e2e5120c",
   "metadata": {},
   "source": [
    "**Structure of the paper**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf01866-4135-4277-a467-7a4364517c34",
   "metadata": {},
   "source": [
    "- Going to the non-Euclidean world in Section IV, we then define basic notions in differential geometry and graph theory.\n",
    "- These topics are insufficiently known in the signal processing community, and to our knowledge, there is no introductorylevel reference treating these so different structures in a common way.\n",
    "- One of our goals is to provide an accessible overview of these models resorting as much as possible to the intuition of traditional signal processing.\n",
    "- In Sections V–VIII, we overview the main geometric deep learning paradigms, emphasizing the similarities and the differences between Euclidean and non-Euclidean learning methods.\n",
    "- In Section IX, we show examples of selected problems from the fields of network analysis, particle physics, recommender systems, computer vision, and graphics. \n",
    "- In Section X, we draw conclusions and outline current main challenges and potential future research directions in geometric deep learning. \n",
    "- To make the paper more readable, we use inserts to illustrate important concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e7024-56f0-45ed-9205-c82a4de21c0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### III. DEEP LEARNING ON EUCLIDEAN DOMAINS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761b710-54c1-4dad-a64f-8b039fb8ed26",
   "metadata": {},
   "source": [
    "**Geometric priors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc2e0a-e08e-4dc4-9202-805f2c50995d",
   "metadata": {},
   "source": [
    "- Consider a compact d-dimensional Euclidean domain $\\Omega = [0; 1]^d \\subset \\mathbb{R}^d $ on which squareintegrable functions $f \\in L2(\\Omega)$ are defined\n",
    "    - 제곱할 수 있는 함수가 정의된 컴책트한 d차원의 유클리디안 도메인을 고려해보자.\n",
    "- We consider a generic supervised learning setting, in which an unknown8 function $y : L^2(\\Omega) \\rightarrow \\cal{Y}$ is observed on a training set\n",
    "    - $\\{ f_i \\in L^2 (\\Omega) , y_i = y(f_i )  \\}_{i \\in \\cal{I}}$..(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae58745-b510-4751-b2bf-9cd90820806d",
   "metadata": {},
   "source": [
    "- In a supervised classification setting, the target space $\\cal{Y}$ can be thought discrete with $| \\cal{Y} |$ being the number of classes. \n",
    "- In a multiple object recognition setting, we can replace $\\cal{Y}$ by the $K$-dimensional simplex, which represents the posterior class probabilities $p(y|x)$. In regression tasks, we may consider $\\cal{Y} = \\mathbb{R}^m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c11d5-8095-4986-8acb-89068b5001c8",
   "metadata": {},
   "source": [
    "**Stationarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce98a9-d967-4903-839b-06ccb0057fb3",
   "metadata": {},
   "source": [
    "- Let $\\cal{T}vf(x) = f(x - v),$  $x; v \\in \\Omega$ (2) be a translation operator acting on functions $f \\in L^2 (\\Omega)$\n",
    "- Our first assumption is that the function y is either invariant or equivariant with respect to translations, depending on the task.\n",
    "- In the former case, we have $y(\\cal{T}_v f) = y(f)$ for any $f \\in L^2(\\Omega)$ and $v \\in \\Omega$. \n",
    "- This is typically the case in object classification tasks. \n",
    "- In the latter, we have $y(\\cal{T}_v f) = \\cal{T}_v y(f)$, which is welldefined when the output of the model is a space in which translations can act upon (for example, in problems of object8 localization, semantic segmentation, or motion estimation).\n",
    "- *Our definition of invariance should not be confused with the traditional notion of translation invariant systems in signal processing, which corresponds to translation equivariance in our language (since the output translates whenever the input translates).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269516a-a280-4cf6-8eda-a615d7653e78",
   "metadata": {},
   "source": [
    "**Local deformations and scale separation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072f9d1-f9db-457f-ab7a-784e0152e338",
   "metadata": {},
   "source": [
    "- Similarly, a deformation $\\cal{L}_{\\cal{T}}$, where $\\tau : \\Omega \\rightarrow \\Omega$ is a smooth vector field, acts on $L^2 (\\Omega)$ as $\\cal{L}_{\\cal{T}} f(x) = f(x - \\tau(x))$.\n",
    "- deformation can model local translations, changes in point of view, rotations and frequency transpositions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d4e7b-3047-4497-a6dc-9fd910868af0",
   "metadata": {},
   "source": [
    "- Most tasks studied in computer vision are not only translation invariant/equivariant, but also stable with respect to local deformations [57], [18]. \n",
    "- In tasks that are translation invariant we have\n",
    "    - $| y(\\cal{L}_{\\cal{T}} f) - y(f)| \\approx \\| \\bigtriangledown \\tau \\|$, (3)\n",
    "    - for all $f$, $\\tau$ .\n",
    "- Here, $\\| \\bigtriangledown \\tau \\|$ measures the smoothness of a given deformation field.\n",
    "- In other words, the quantity to be predicted does not change much if the input image is slightly deformed.\n",
    "- In tasks that are translation equivariant, we have\n",
    "    - $| y(\\cal{L}_{\\cal{T}} f) - \\cal{L}_{\\cal{T}}y(f)| \\approx \\| \\bigtriangledown \\tau \\|$, (4)\n",
    "- This property is much **stronger** than the previous one, since the space of local deformations has a high dimensionality, as opposed to the d-dimensional translation group.\n",
    "- It follows from (3) that we can extract sufficient statistics at a lower spatial resolution by downsampling demodulated localized filter responses without losing approximation power.\n",
    "- An important consequence of this is that long-range dependencies can be broken into multi-scale local interaction terms, leading to hierarchical models in which spatial resolution is progressively reduced. \n",
    "- To illustrate this principle, denote by\n",
    "    - $Y (x_1; x2; v) = Prob(f(u) = x1 \\text{ and } f(u + v) = x2)$ (5)\n",
    "    - the joint distribution of two image pixels at an offset $v$ from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6afb75-e3e6-4d8f-b7b6-570c16be6eb8",
   "metadata": {},
   "source": [
    "- In the presence of long-range dependencies, this joint distribution will not be separable for any $v$.\n",
    "- However, the deformation stability prior states that $Y (x_1, x_2; v) \\approx Y (x_1, x_2; v(1 + \\epsilon ))$ for small $\\epsilon$. \n",
    "- In other words, whereas long-range dependencies indeed exist in natural images and are critical to object recognition, they can be captured and down-sampled at different scales. \n",
    "- This principle of stability to local deformations has been exploited in the computer vision community in models other than CNNs,\n",
    "- for instance, deformable parts models [58]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e642b7d-d21a-4a6b-ba15-04dcda94b657",
   "metadata": {},
   "source": [
    "**Convolutional neural networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fbb917-13bd-42c2-9393-d01a7d570c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
