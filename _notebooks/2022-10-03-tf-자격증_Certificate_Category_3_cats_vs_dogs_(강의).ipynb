{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TensorFlow 2.0 Category 3 - Cats vs Dogs 분류\n",
    ">  Convolution Neural network 활용한 분류 모델 (Classification), tensorflow-datasets 를 활용한 데이터 전처리\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: 최서연\n",
    "- categories: [TensorFlow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type b (cats vs dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww0hxcaSGb0m"
   },
   "source": [
    "## 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI6oK1X-GdJJ"
   },
   "source": [
    "1. GPU 옵션 켜져 있는지 확인할 것!!! (수정 - 노트설정 - 하드웨어설정 (GPU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yoe-K38GQ4b"
   },
   "source": [
    "## 순서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc6iTV8DGPop"
   },
   "source": [
    "1. **import**: 필요한 모듈 import\n",
    "2. **전처리**: 학습에 필요한 데이터 전처리를 수행합니다.\n",
    "3. **모델링(model)**: 모델을 정의합니다.\n",
    "4. **컴파일(compile)**: 모델을 생성합니다.\n",
    "5. **학습 (fit)**: 모델을 학습시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msi1agesayxW"
   },
   "source": [
    "## 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bqqzpm1P8dd9"
   },
   "source": [
    "Computer Vision with CNNs\n",
    "<br>\n",
    "<br>For this exercise you will build a cats v dogs classifier\n",
    "<br>using the Cats v Dogs dataset from TFDS.\n",
    "<br>Be sure to use the final layer as shown \n",
    "<br>    **(Dense, 2 neurons, softmax activation)**\n",
    "<br>\n",
    "<br>The testing infrastructre will **resize all images to 224x224**\n",
    "<br>with **3 bytes of color depth**. Make sure your input layer trains\n",
    "<br>images to that specification, or the tests will fail.\n",
    "<br>\n",
    "<br>Make sure your output layer is exactly as specified here, or the \n",
    "<br>tests will fail.\n",
    "\n",
    "----------------------------------------\n",
    "<br>이 연습에서는 cats v dogs 분류기를 만들 것입니다.\n",
    "TFDS의 Cats v Dogs 데이터 세트 사용.\n",
    "<br> 그림과 같이 최종 레이어를 사용하십시오\n",
    "<br> **(Dense, 뉴런 2 개, activation='softmax')**\n",
    "<br>\n",
    "<br> 테스트 인프라는 **모든 이미지의 크기를 224x224로 조정합니다(컬러사진)**. 입력 레이어를 확인하십시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VXJfrSfG8dd5"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIXfMfFq8dd9"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg0sXFW2nil7"
   },
   "source": [
    "**tensorflow-datasets**를 활용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTjsZFWGn117"
   },
   "source": [
    "* [Cats vs Dogs 데이터셋 문서 보기](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs?hl=ko)\n",
    "\n",
    "* [tensorflow-datasets 다루기](https://www.tensorflow.org/datasets/splits?hl=ko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTCJ8FxgogB_"
   },
   "source": [
    "**시험에서 주어지는 데이터셋 로드 형태**\n",
    "\n",
    "* 예전 방식이므로 아래처러 주어지는 코드를 과감히 삭제 후, 아래 제공되는 방식으로 변경합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v70IGh4o8dd-"
   },
   "source": [
    "```python\n",
    "dataset_name = 'cats_vs_dogs'\n",
    "train_dataset = tfds.load(name=dataset_name, split='train')\n",
    "```\n",
    "시험에서 주어지는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "e1VLqKMO8deA"
   },
   "outputs": [],
   "source": [
    "dataset_name = 'cats_vs_dogs'\n",
    "\n",
    "# 처음 80%의 데이터만 사용\n",
    "train_dataset = tfds.load(name=dataset_name, split='train[:80%]')\n",
    "\n",
    "# 최근 20%의 데이터만 사용\n",
    "valid_dataset = tfds.load(name=dataset_name, split='train[80%:]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "739L8AKh-OZG"
   },
   "source": [
    "시험에서 요구하는 **전처리 요구 조건**은 다음과 같습니다.\n",
    "\n",
    "1. 이미지 정규화 (Normalization)\n",
    "2. 이미지 사이즈 맞추기: (224 X 224) \n",
    "3. image(x), label(y)를 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7fLV4E6_8uD"
   },
   "source": [
    "**[실습코드]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zVGrlzVd_8T9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec={'image': TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), 'image/filename': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "87jVAM8Tsp-e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 224, 3)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "(22, 224, 3)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "(22, 224, 3)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataset.take(3):\n",
    "    x = data['image']/255\n",
    "    y = data['label']\n",
    "    x = tf.image.resize(x,size=(22,224,))\n",
    "    print(x.shape)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fV1LU9W1-Swt"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    # x, y 데이터를 정의합니다.\n",
    "    x = data['image']\n",
    "    y = data['label']\n",
    "    # image 정규화(Normalization)\n",
    "    x = x / 255\n",
    "    # 사이즈를 (224, 224)로 변환합니다.\n",
    "    x = tf.image.resize(x, size=(224, 224))\n",
    "    # x, y  데이터를 return 합니다.\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPQ9hOrnpHKj"
   },
   "source": [
    "만든 전처리 함수(preprocessing)를 **dataset에 mapping**하고, **batch_size도 지정**합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Wq-NyPMI_oZM"
   },
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JRM0LRxt-Sru"
   },
   "outputs": [],
   "source": [
    "train_data = train_dataset.map(preprocess).batch(batch_size)\n",
    "valid_data = valid_dataset.map(preprocess).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvvarzxw8ded"
   },
   "source": [
    "## 모델 정의 (Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azRvYVbQnim5"
   },
   "source": [
    "이제 Modeling을 할 차례입니다.\n",
    "\n",
    "`Sequential` 모델 안에서 층을 깊게 쌓아 올려 주면 됩니다.\n",
    "\n",
    "1. `input_shape`는 (height, width, color_channel)입니다. cats vs dogs 문제에서는 (224, 224, 3) 이 됩니다.\n",
    "2. 깊은 출력층과 더 많은 Layer를 쌓습니다.\n",
    "3. Dense Layer에 `activation='relu'`를 적용합니다.\n",
    "4. 분류(Classification)의 마지막 층의 출력 숫자는 분류하고자 하는 클래스 갯수와 **같아야** 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rpWdaUGX8dee"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), input_shape=(224, 224, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iQRWnG9A8def"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 10, 10, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 5, 5, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               3277312   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,898,562\n",
      "Trainable params: 3,898,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnmQV-fDnim8"
   },
   "source": [
    "## 컴파일 (compile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHQ1abHXK8e9"
   },
   "source": [
    "1. `optimizer`는 가장 최적화가 잘되는 알고리즘인 'adam'을 사용합니다.\n",
    "2. `loss`설정\n",
    "  * 출력층 activation이 `sigmoid` 인 경우: `binary_crossentropy`\n",
    "  * 출력층 activation이 `softmax` 인 경우: \n",
    "    * 원핫인코딩(O): `categorical_crossentropy`\n",
    "    * 원핫인코딩(X): `sparse_categorical_crossentropy`)\n",
    "3. `metrics`를 'acc' 혹은 'accuracy'로 지정하면, 학습시 정확도를 모니터링 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXGMljJQhmp_"
   },
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "QCLw6RMZnim-"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyLUPgGCninB"
   },
   "source": [
    "## ModelCheckpoint: 체크포인트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46Oi04ZMLtEB"
   },
   "source": [
    "`val_loss` 기준으로 epoch 마다 최적의 모델을 저장하기 위하여, ModelCheckpoint를 만듭니다.\n",
    "* `checkpoint_path`는 모델이 저장될 파일 명을 설정합니다.\n",
    "* `ModelCheckpoint`을 선언하고, 적절한 옵션 값을 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qJwGq3PoninB"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"my_checkpoint.ckpt\"\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
    "                             save_weights_only=True, \n",
    "                             save_best_only=True, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3mjb5EAninE"
   },
   "source": [
    "## 학습 (fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-X6hK_DMYZH"
   },
   "source": [
    "1. `validation_data`를 반드시 지정합니다.\n",
    "2. `epochs`을 적절하게 지정합니다.\n",
    "3. `callbacks`에 바로 위에서 만든 checkpoint를 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2uHXDA_vninF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6941 - acc: 0.5068\n",
      "Epoch 1: val_loss improved from inf to 0.69348, saving model to my_checkpoint.ckpt\n",
      "582/582 [==============================] - 158s 271ms/step - loss: 0.6941 - acc: 0.5068 - val_loss: 0.6935 - val_acc: 0.4901\n",
      "Epoch 2/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5012\n",
      "Epoch 2: val_loss improved from 0.69348 to 0.69342, saving model to my_checkpoint.ckpt\n",
      "582/582 [==============================] - 157s 270ms/step - loss: 0.6932 - acc: 0.5012 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 3/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5036\n",
      "Epoch 3: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 158s 271ms/step - loss: 0.6932 - acc: 0.5036 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 4/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5032\n",
      "Epoch 4: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 157s 270ms/step - loss: 0.6932 - acc: 0.5032 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 5/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5032\n",
      "Epoch 5: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 157s 270ms/step - loss: 0.6932 - acc: 0.5032 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 6/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5034\n",
      "Epoch 6: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 157s 270ms/step - loss: 0.6932 - acc: 0.5034 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 7/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5032\n",
      "Epoch 7: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 187s 321ms/step - loss: 0.6932 - acc: 0.5032 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 8/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5033\n",
      "Epoch 8: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 206s 354ms/step - loss: 0.6932 - acc: 0.5033 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 9/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5028\n",
      "Epoch 9: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 206s 355ms/step - loss: 0.6932 - acc: 0.5028 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 10/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 10: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 207s 356ms/step - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 11/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 11: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 208s 358ms/step - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 12/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 12: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 208s 357ms/step - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 13/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 13: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 208s 357ms/step - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 14/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 14: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 208s 356ms/step - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 15/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 15: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 208s 357ms/step - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 16/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 16: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 208s 358ms/step - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 17/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 17: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 208s 357ms/step - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 18/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 18: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 208s 357ms/step - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 19/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 19: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 207s 356ms/step - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6934 - val_acc: 0.4901\n",
      "Epoch 20/20\n",
      "582/582 [==============================] - ETA: 0s - loss: 0.6932 - acc: 0.5027\n",
      "Epoch 20: val_loss did not improve from 0.69342\n",
      "582/582 [==============================] - 208s 358ms/step - loss: 0.6932 - acc: 0.5027 - val_loss: 0.6934 - val_acc: 0.4901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fca9c382620>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,\n",
    "          validation_data=(valid_data),\n",
    "          epochs=20,\n",
    "          callbacks=[checkpoint],\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwnduSgRiBw8"
   },
   "source": [
    "## 학습 완료 후 Load Weights (ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLqb_6XrMvdq"
   },
   "source": [
    "학습이 완료된 후에는 반드시 `load_weights`를 해주어야 합니다.\n",
    "\n",
    "그렇지 않으면, 열심히 ModelCheckpoint를 만든 의미가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4jO1ucZ9ninH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.conv2d.Conv2D object at 0x7fca9d23e680> and <keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7fca9d2a1990>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.conv2d.Conv2D object at 0x7fca9d23e680> and <keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7fca9d2a1990>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.conv2d.Conv2D object at 0x7fca9c26ece0> and <keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7fca9d23e860>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.convolutional.conv2d.Conv2D object at 0x7fca9c26ece0> and <keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x7fca9d23e860>).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received incompatible tensor with shape (512,) when attempting to restore variable with shape (64,) and name layer_with_weights-1/bias/.ATTRIBUTES/VARIABLE_VALUE.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# checkpoint 를 저장한 파일명을 입력합니다.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py:135\u001b[0m, in \u001b[0;36mResourceVariableSaveable.restore\u001b[0;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[1;32m    132\u001b[0m   assigned_variable \u001b[38;5;241m=\u001b[39m resource_variable_ops\u001b[38;5;241m.\u001b[39mshape_safe_assign_variable_handle(\n\u001b[1;32m    133\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_op, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_shape, restored_tensor)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 135\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    136\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived incompatible tensor with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrestored_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen attempting to restore variable with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m assigned_variable\n",
      "\u001b[0;31mValueError\u001b[0m: Received incompatible tensor with shape (512,) when attempting to restore variable with shape (64,) and name layer_with_weights-1/bias/.ATTRIBUTES/VARIABLE_VALUE."
     ]
    }
   ],
   "source": [
    "# checkpoint 를 저장한 파일명을 입력합니다.\n",
    "model.load_weights(checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
