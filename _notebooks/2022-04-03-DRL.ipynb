{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a731bb-d55c-411d-87f8-f6c782603246",
   "metadata": {
    "id": "cac470df-29e7-4148-9bbd-d8b9a32fa570",
    "tags": []
   },
   "source": [
    "# (not_done_review)그로킹 심층 강화학습\n",
    "> 강찬석\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: 최서연\n",
    "- categories: [Reinforcement Learning]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6bfdb-de3e-4f44-8936-9297c79d6356",
   "metadata": {},
   "source": [
    "# $\\star$ 목표: 일주일에 몇 장씩이라도 보기!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fd0aa-0a6e-4dda-a5ed-bc6c93cda730",
   "metadata": {},
   "source": [
    "ref: https://goodboychan.github.io/book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227222c9-74ff-4207-a917-1216fafbf433",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e663b-943e-4eda-b806-717edf24908e",
   "metadata": {},
   "source": [
    "심층강화학습 deep reinforcement learning DRL 이란 머신 러닝 기법 중 하나.\n",
    "- 지능이 요구되는 문제를 해결할 수 있도록 인공지능 컴퓨터 프로그램을 개발하는데 사용\n",
    "- 시행착오를 통해 얻은 반응을 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a885a96-06a3-4b7b-a777-ad43f8e7c498",
   "metadata": {},
   "source": [
    "심층 강화학습은 문제에 대한 접근법이다.\n",
    "- **에이전트agent**: 의사를 결정하는 객체 자체\n",
    "    - ex) 사물을 집는 로봇 학습시킬때 의사 결정을 좌우하는 코드와 연관\n",
    "- **환경unvironmnet**: 에이전트(의사 결정) 이외의 모든 것\n",
    "    - ex) 의사결정하는 로봇(객체) 제외한 모든 것이 환경의 일부 \n",
    "- **상태 영역state space**: 변수가 가질 수 있는 모든 값들의 집합\n",
    "- **관찰observation**: 에이전트가 관찰할 수 있는 상태의 일부\n",
    "- **전이 함수transition function** 에이전트와 환경 사이의 관계를 정의한 함수\n",
    "- **보상 함수reward function**: 행동에 대한 반응으로 환경이 제공한 보상 신호와 관련된 함수\n",
    "- **모델model**: 전이와 보상함수를 표현 가능\n",
    "\n",
    "에이전트의 3단계 과정\n",
    "1. 환경과 상호작용을 나누고\n",
    "2. 행동에 대해서 평가를 하며,\n",
    "3. 받은 반응을 개선시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc58edf-5115-4c33-ab82-38db6699e40a",
   "metadata": {},
   "source": [
    "- **에피소드형 업무episodic task**: 게임과 같이 자연적으로 끝나는 업무\n",
    "- **연속형업무continuing task**: 앞으로 가는 동작을 학습하는 경우\n",
    "\n",
    "- 연속형 피드백이 야기하는 문제\n",
    "    - **시간적 가치 할당 문제tamporal credit assignment problem**:문제에 시간적 개념이 들어가 있고, 행동에도 지연된 속성이 담겨 있으면, 보상에 대한 가치를 부여하기 어렵다. \n",
    "- 평가 가능한 피드백이 야기하는 문제\n",
    "    - **탐험과 착취 간의 트레이드 오프exploration versus explotation trade-off**: 에이전트는 현재 가지고 있는 정보에서 얻을 수 있는 가장 좋은 것과 정보를 새로 얻는 것 간의 균형을 맞출 수 있어야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98653189-f50f-49d4-a114-3410e7627488",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e407f-5d1d-426e-ae60-47f7c6f43fbc",
   "metadata": {},
   "source": [
    "에이전트의 세 단계 과정\n",
    "1. 모든 에이전트는 *상호작용* 요소를 가지고 있고, 학습에 필요한 데이터를 수집한다.\n",
    "2. 모든 에이전트들은 현재 취하고 있는 행동을 *평가*하고, \n",
    "3. 전체적인 성능을 개선하기 위해 만들어진 무언가를 *개선*한다.\n",
    "\n",
    "- *상태 영역state space*: 표현할 수 있는 변수들의 모든 값에 대한 조합\n",
    "- *관찰observation*: 에이전트가 어떤 특정 시간에 얻을 수 있는 변수들의 집합\n",
    "- *관찰 영역observation space*: 변수들이 가질 수 있는 모든 값들의 조합\n",
    "- *행동 영역action space*: 모든 상태에서 취할 수 있는 모든 행동에 대한 집합\n",
    "- *전이 함수transition function*: 환경은 에이전트의 행동에 대한 반응으로 상태를 변화할 수 있는데 이와 관련된 함수\n",
    "- *보상 함수reward function*: 행동과 관련된 보상에 대한 함수\n",
    "- *모델mpdel*: 전이와 보상 함수의 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5d7bc-51eb-4b9b-a685-f9868c16da75",
   "metadata": {},
   "source": [
    "보상 신호가 밀집되어 있을수록, 에이전트의 직관성과 학습속도가 높아지지만, 에이전트에게 편견을 주입하게 되어, 결국 에이전트가 예상하지 못한 행동을 할 가능성은 적어지게 된다. 반면, 보상 신호가 적을수록 직관성이 낮아져 에이전트가 새로운 행동을 취할 확률이 높아지지만, 그만큼 에이전트르르 학습시키는데 오래 걸리게 될 것이다.\n",
    "\n",
    "- *타임 스텝time step*: 상호작용이 진행되는 사이클, 시간의 단위\n",
    "- *경험 튜플experience tuple*: 관찰 또는 상태, 행동, 보상 그리고 새로운 관찰\n",
    "- *에피소드형 업무episodic task*: 게임과 같이 자연적으로 끝나는 업무 - *에피소드episode*\n",
    "- *연속형 업무continuing task*: 앞으로 전진하는 동작과 같이 자연적으로 끝나는 업무\n",
    "- *반환값return*: 에피소드 동안 수집된 보상의 총합\n",
    "- *상태state*: 문제에 포함되어 있는 독특하고, 자기 자신만의 설정이 담긴 요소\n",
    "- *상태 영역state square*: 모든 가능한 상태, 집합 S로 표현,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c159436-e5de-44bc-b4af-4f512e0ecff9",
   "metadata": {},
   "source": [
    "**마르코프 결정 과정Markov decision process MDP**\n",
    "- 수학 프레임워크, 이를 이용해서 강화학습 환경을 연속적인 의사결정 문제로 표현하는 방법 학습.\n",
    "- 일반적인 형태는 불확실성상에 놓여진 상황에서 어떠한 복잡한 연속적 결정을 가상으로 모델링해서, 강화학습 에이전트가 모델과 반응을 주고받고, 경험을 통해서 스스로 학습할 수 있게 해준다.\n",
    "- 모든 상태들의 집합S\n",
    "- 모든 *시작 상태starting state* 혹은 *초기 상태initial state*라고 부르는 S+의 부분집합이 있음\n",
    "- MDP와의 상호작용 시작하면서 어떤 S에서의 특정 상태에서 어떤 확률 붙포 간의 관계를 그릴 수 있는데 이때 이 확률 분포는 무엇이든 될 수 있지만, 학습이 이뤄지는 동안에는 고정되어 있어야 한다.\n",
    "    - 즉, 이 확률 분포에서 샘플링된 확률은 학습과 에이전트 검증의 처름 에피소드부터 마지막 에피소드까지는 항상 동일해야 한다는 것\n",
    "- *흡수absorbing* 또는 *종료 상태terminal state* 라는 특별한 상태도 존재.\n",
    "    - 종료 상태(꼭 하나는 아님) 이외의 모든 상태들을 S라고 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1337d7-8b38-4f6f-ba78-d412acbfb97b",
   "metadata": {},
   "source": [
    "MDP에서는\n",
    "- **A**: 상태에 따라 결정되는 행동의 집합\n",
    "    - $\\therefore$ 특정 상태에서 허용되지 않는 행동도 존재한다는 말.\n",
    "    - 상태(s)를 인자로 받는 함수.\n",
    "    - A(s); 상태(s)에서 취할 수 있는 행동들의 집합 반환, 상수 정의 가능.\n",
    "- 행동 영역은 유한 혹은 무한 가능,\n",
    "    - 단일 행동에 대한 변수들의 집합은 한 개 이상의 료소를 가질 수 있고, 유한해야 함.\n",
    "    - $\\therefore$ 대부분의 환경에서 모든 상태에서 취할 수 있는 행동의 수는 같도록 설계됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de381d58-213e-4111-8a94-057819aa3585",
   "metadata": {},
   "source": [
    "*상태-전이 확률state-transition probability* = *전이 함수*\n",
    "- $T(s,a,s')$\n",
    "- 전이함수$T$는 각 전이 튜플인 $(s,a,s')$을 확률과 연결시켜준다.\n",
    "- 어떤 상태 s에서 행동 a를 취하고 다음 상태가 s'이 되었을 때, 이때의 확률을 반환해준다는 뜻\n",
    "- 이 확률의 합은 1\n",
    "$$p(s'|s,a) = P(S_t = s'|S_{t-1} = s,A_{t-1}=a)$$\n",
    "$$\\sum_{s' \\in S}p(s'|s,a) = a, \\forall s \\in S, \\forall a \\in A(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43dfbff-36d5-46a4-afcb-66c35202093f",
   "metadata": {},
   "source": [
    "*보상 함수$R$*\n",
    "- 전이 튜플 $s,a,s'$을 특정한 스칼라 값으로 매핑,\n",
    "    - 양수 보상 = 수익 또는 이득\n",
    "    - 음수 보상 = 비용, 벌칙, 패널티\n",
    "- $R(s,a,s')$ = $R(s,a)$ = $R(s)$\n",
    "- *보상 함수를 표현하는 가장 명확한 방법은 상태와 행동 그리고 다음 상태, 이 세 가지를 함께 쓰는 것*\n",
    "$$r(s,a) = \\mathbb{E} [R_t|S_{t-1} = s,A_{t-1} = a]$$\n",
    "$$r(s,a,s') = \\mathbb{E} [R_t|S_{t-1} = s, A_{t-1} = a, S_t= s']$$\n",
    "$$R_t \\in \\cal{R} \\subset \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926fbd9-ea72-4dde-b2a4-ff49c9da8b77",
   "metadata": {},
   "source": [
    "*호라이즌horixon*\n",
    "- 계획 호라이즌planning horizon: 에피소드형 업무나 연속적 업무를 에이전트의 관점에서 정의\n",
    "    - 유한 호라이즌finite horizon: 에이전트가 유한한 타임 스템 내에 업무가 종료된다는 것을 알고 있는 계획 호라이즌\n",
    "    - 탐욕 호라이즌greedy horizon: 계획 호라이즌이 1인 경우\n",
    "    - 무한 호라이즌infinite horiaon 에이전트한테 미리 정의된 타임 스텝에 대한 제한이 없어 에이전트가 무한하게 계획할 수 있음\n",
    "        - 특히, 무한한 계획 호라이즌을 가지는 업무는 무기한 호라이즌 업무indefinite horizon task 라고 부름\n",
    "- 에이전트가 타임 스텝 루프에 빠지는 것을 막기 위해 타임 스텝을 제한하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4aa89a-708a-49fc-b55a-50b0cfcb8d4a",
   "metadata": {},
   "source": [
    "*감가율discount factor  = 감마gamma*\n",
    "- 받았던 보상의 시점이 미래로 더 멀어질수록, 현재 시점에서는 이에 대한 가치를 더 낮게 평가\n",
    "\n",
    "- 환경; 자세한 예제; 5개의 미끄러지는 칸을 가지는 환경slippery walk five, SWF\n",
    "$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\dots R_T$$\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots +\\gamma^{T-1} R_T$$\n",
    "$$G_t = \\sum^{\\infty}_{k=0} \\gamma^k R_{t+k+1}$$\n",
    "$$G_t = R_{t+1}+ \\gamma G_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5654d6-d858-4734-a7ad-7b8d3136d85b",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab96a76-43c1-4526-818a-b96dd5873728",
   "metadata": {},
   "source": [
    "에이전트의 목표: 반환값(보상의 총합)을 최대화할 수 있는 행동의 집합을 찾는 것, 정책이 필요하다.\n",
    "- *정책policy*: 가능한 모든 상태를 포괄한 전체적인 계획.\n",
    "- 확률적 혹은 결정적\n",
    "- 정책을 비교하기 위해 시작 상태를 포함한 모든 상태들에 대해 기대 반환값을 계산할 수 있어야 한다.\n",
    "\n",
    "\n",
    "- 정책$\\pi$를 수행할 때, 상태 s에 대한 가치를 정의할 수 있다.\n",
    "    - 에이전트가 정책 $\\pi$를 따르면서 상태 s에서 시작했을 때, 상태 s의 가치는 반환값의 기대치라고 할 수 있다.\n",
    "    - 가치함수는 상태 s에서 정책$\\pi$를 따르면서 시작했을 때의 반환값에 대한 기대치를 나타낸다.\n",
    "$$v_{\\pi} (s) = \\mathbb{E}_{\\pi} [G_t|S_t = s]$$\n",
    "$$v_{\\pi} (s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{+3} + \\dots |S_t = s]$$\n",
    "$$v_{\\pi} (s) = \\sum_a \\pi (a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma v_\\pi (s')] \\forall s \\in S$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994f177-b132-4696-817b-fec3bb5528a0",
   "metadata": {},
   "source": [
    "*행동-가치 함수action-value function* = Q 함수 = $Q^{\\pi}(s,a)$\n",
    "- 상태 s에서 행동 a를 취했을 때, 에이전트가 정책$\\pi$를 수행하면서 얻을 수 있는 기대 반환값\n",
    "- MDP 없이도 정책을 개선시킬 수 있게 해준다.\n",
    "$$q_{\\pi} (s,a) = \\mathbb{E}_{\\pi} [G_t|S_t = s,A_t = a]$$\n",
    "$$q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[R_t + \\gamma G_{t+1} | S_t = s, A_t = a]$$\n",
    "$$q_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a)[r+\\gamma v_{\\pi} (s')], \\forall s \\in S, \\forall a \\in A(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28923dc6-af6b-4a0b-aff6-225aaec7cfab",
   "metadata": {},
   "source": [
    "*행동-이점 함수action-advamtage function* = 이점함수advantage function = $A$\n",
    "- 상태 s에서 행동를 취했을 때의 가치와 정책 $\\pi$에서 상태 s에 대한 상태-가치 함수 간의 차이\n",
    "$$a_{\\pi}(s,a) = q_{\\pi}(s,a) - v_{\\pi}(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9e89f-9920-47d3-8cc7-1ef08e579d80",
   "metadata": {},
   "source": [
    "이상적인 정책optimal policy\n",
    "- 모든 상태에 대해서 다른 정책들보다 기대 반환값이 같거나 더 크게 얻을 수 있는 정책\n",
    "    - 벨만 이상성 공식(아래)\n",
    "$$v_{*}(s) = max_{\\pi} v_{\\pi} (s), \\forall_s \\in S$$\n",
    "$$q_{*}(s,a) = max_{\\pi} q_{\\pi}(s,a), \\forall s \\in S, \\forall a \\in A(s)$$\n",
    "$$v_{*}(s) = max_{a} \\sum_{s',r} p(s',r|s,a)[r+\\gamma v_{*} (s')]$$\n",
    "$$q_{*}(s,a) = \\sum_{s',r}p(s',r|s,a) [r + \\gamma max_{a'} q_{*}(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5942d-d3f1-47df-ad25-8c2f16398178",
   "metadata": {},
   "source": [
    "*반복 정책 평가법iteractive policy evaluation* = *정책평가법policy rvaluation*\n",
    "- 임의의 정책을 평가할 수 있는 알고리즘\n",
    "- 상태 영역을 살펴보면서 반복적으로 에측치를 개선하는 방법 사용\n",
    "- 정책을 입력으로 받고, *예측문제prediction problem*를 풀 수 있는 알고리즘에 대한 가치 함수를 출력으로 내보내는 알고리즘, 이때는 미리 정의한 정책의 가치를 계산..(?)\n",
    "$$v_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s',r|s,a) \\big[ r+\\gamma v_k (s') \\big]$$\n",
    "- 정책 평가 알고리즘을 충분히 반복하면 정책에 대한 가치함수로 수렴시킬 수 있다.\n",
    "\n",
    "- 실제로 적용하는 경우에는 우리가 근사하고자 하는 가치 함수의 변화를 확인하기 위해서 기분보다 작은 임계값을 사용.\n",
    "    - 이런 경우에는 가치 함수의 변화가 우리가 정한 임계값보다 작을경우 반복을 멈추게 된다.\n",
    "    \n",
    "- SWF환경에서 항상 왼쪽 행동을 취하는 정책에 이 알고리즘이 어떻게 동작할까?\n",
    "$$v_{k+1} (s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma v_k (s') \\big] $$\n",
    "    - $\\gamma$는 1이라 가정,\n",
    "    - 항상 왼쪽으로 가는 정책 사용\n",
    "    - $k$: 반복적 정책-평가 알고리즘의 수행횟수\n",
    "$$ v^{\\pi}_{1}(5)  = p(s'=4|s=5, a=\\text{왼쪽}) * [R(5,\\text{왼쪽},4) + v^{\\pi}_{0}(4)] +$$\n",
    "$$p(s'=5|s=5, a=\\text{왼쪽}) * [R(5,\\text{왼쪽},5) + v^{\\pi}_{0}(5)] + $$\n",
    "$$p(s'=6|s=5, a=\\text{왼쪽}) * [R(5,\\text{왼쪽},6) + v^{\\pi}_{0}(6)]$$\n",
    "$$c^{\\pi}_{1} (5) = 0.50 * (0+0) + 0.33 * (0+0) + 0.166 * (1+0) = 0.166 \\dots \\text{   1번 정책 평가법을 사용했을 떄 상태에 대한 가치를 나타냄}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a9ce3-33df-4919-ab82-0e50d4068500",
   "metadata": {},
   "source": [
    "$\\rightarrow$ Bootstraping 붓스트랩 방법: 예측치를 통해서 새로운 예측치를 계산하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6db29-1137-48e7-82d3-ce85d112284e",
   "metadata": {},
   "source": [
    "```python\n",
    "# 정책(pi) 평가 알고리즘\n",
    "def policy_evaluation(pi,P, gamma = 1.0, theta = 1e-10):\n",
    "    prev_V = np.zeros(len(P),dtype = np.float64)\n",
    "    while True:\n",
    "        V = np.aeros(len(P),dtype = float64)\n",
    "        for s in range(len(P)):\n",
    "            for prob, next_state, reward, done in P[s][pi(s)]:\n",
    "            V[s] +- prob * (reward + gamma * prev_V[next_state] * (nor done))\n",
    "        if np.max(np.abs(prev_V - V)) < theta:\n",
    "            break\n",
    "        prev_V = V.copy()\n",
    "    return V\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93680e-805d-4550-bf4e-1e02517b50ef",
   "metadata": {},
   "source": [
    "> Note: 정책-개선 알고리즘이 동작하는 방법, 상태-가치 함수와 MDP를 사용해서 행동-가치 함수를 계산하고, 원래 정책의 행동-가치 함수에 대한 탐용 정책을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1fadd-7f59-4226-8cd9-1f16cc75da8c",
   "metadata": {},
   "source": [
    "정책개선 공식$$\\pi ' (s) argmax_a \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma v_{\\pi} (s') \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b69fa6-58a6-4961-b416-581fee6724fe",
   "metadata": {},
   "source": [
    "```python\n",
    "# 정책 개선(new_pi) 알고리즘\n",
    "def policy_improvement(V,P, gamma+1.0):\n",
    "    Q = np.zeros((len{P}, len(P[0])), dtype = np.float64)\n",
    "    for s in range(len(P)):\n",
    "        for a in rnage(len(P[s])):\n",
    "            for prob, next_state, reward, done in P[s][a]: # 여기서 done의 의미는 다음 상태가 종료 상태인지의 여부를 나타냄\n",
    "                A[s][a] += prob * (reward + gamma * V[next_state] * nor done)\n",
    "    new_pi = lambda s: {s: a for s, a in enumerate(np.argmax(np.argmax(Q, axis=1)))}[s]\n",
    "    return new_pi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74baed8c-db48-4eaf-ba4b-5a73efebcb4c",
   "metadata": {},
   "source": [
    "*잃반화된 정책 순환generalized policy iteration, GPI* - 정책 순환, 가치 순환\n",
    "- 예측된 가치 함수를 사용해서 정책을 개선시키고, 예측된 가치 함수도 현대 정책의 실제 가치 함수를 바탕으로 개선시키는 강화학습의 일반적인 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26845576-8467-4cb5-9087-7113965387ea",
   "metadata": {},
   "source": [
    "*정책 순환policy iteration*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ab457-f9d4-45b6-91f2-fd449c8c440b",
   "metadata": {},
   "source": [
    "```python\n",
    "# 정책 순환 알고리즘\n",
    "def policy _iteration (P, gamma = 1.0, theta = 1e-10):\n",
    "    random_actions = np.random.choice(tuple(P[0].keys()),len(P))\n",
    "    pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s] # 임의 행동집합 만들고 행동에 대한 상태 매핑\n",
    "    while True:\n",
    "        old_pi = {s: pi(s) for s in range(len(P))} # 이전 정책에 대한 복사본 만들기\n",
    "        V = policy_evaluation(pi, P, gamma, theta)\n",
    "        pi = policy_improvement(V, P, gamma)\n",
    "        if old_pi = {s:pi(s) for s in range(len(P))}: # 새로운 정책이 달라진 점이 있나? 있으면 앞의 과정 반복적 시행\n",
    "            break\n",
    "    return V, pi # 없다면 루프 중단시키고 이상적인 정책과 이상적인 상태-가치 함수 반환\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3d7f2-e6d4-4d0c-ba96-ed9fdbf99587",
   "metadata": {},
   "source": [
    "*가치 순환value iteration, VI*\n",
    "- 단일 반복 이후에 부분적인 정책 평가를 하더라도, 정책 평가시 한번 상태-영역을 훑은 이후에 대한 예측된 Q-함수에 탐용 정책을 취하면, 초기의 정책을 개선시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509a684-4a29-47f9-af77-e806d055019d",
   "metadata": {},
   "source": [
    "가치 순환 공식\n",
    "$$v_{k+1} (s) = max_a \\sum_{s',r} p(s',r|s,a) \\big[ r + \\gamma v_k (s') \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30f91d-ca3a-4e8e-8449-0356c83c6ceb",
   "metadata": {},
   "source": [
    "```python\n",
    "# 가치 순환 알고리즘\n",
    "def value_iteration(P, gamma = 1.0, theta = 1e-10): # tehta는 수렴에 대한 임계값\n",
    "    V = np.zeros(len(P),dtype = np.float64)\n",
    "    while True:\n",
    "        Q = np.zeros((len(P), len(P[0])), dtype = np.float64) # Q 함수가 0이어야 예측치가 정확해진다.\n",
    "        for s in range(len(P)):\n",
    "            for a in range(len(P[s])):\n",
    "                for prob, next_state, reward, done in P[s][a]:\n",
    "                    Q[s][a] += prob * (reward + gamma * V[next_state] * (not done)) # 행동-가치 함수 계산\n",
    "        if np.max(np.abs(V - np.max(Q, axis = 1))) < theta: # 상태-가치 함수가 변하지 않으면 이상적인 V함수를 찾은 것이고, 루프가 멈춤\n",
    "            break\n",
    "        V = np.max(Q, axis=1) # 개선과 평가 단계의 혼합\n",
    "    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]                     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2b359-7ea1-4552-a18e-54a2c366d977",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d391d-bea5-4a4c-aacb-315fbb91fb11",
   "metadata": {},
   "source": [
    "*멀티 암드 밴딧Multi-armed bandit, MAB*\n",
    "- 상태 영역과 이에 따른 호라이즌이 1인 독특한 강화학습 문제\n",
    "- 여러 개의 선택지 중 하나를 선택하는 환경\n",
    "$$G_0 = 1* 0 + 0.99 * 0 0.9801 * 0 + 0.9702 * 0 + 0.9605 * 0.9509 * 1$$\n",
    "$$q(a) = \\mathbb{E} \\big[ R_t | A_t = a \\big]$$\n",
    "- 행동 A에 대한 Q 함수는 A 가 샘플링 되었을 때의 기대 보상치를 나타낸다.\n",
    "$$v_* = q(a_*) = max_{a \\in A} q(a)$$\n",
    "$$a_* = argmax_{a \\in A} q(a)$$\n",
    "$$q(a_*) = v_*$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c06e65-faa9-423c-acf5-b8345ad7f172",
   "metadata": {},
   "source": [
    "*전체 후회값total regret*\n",
    "- 전체 에피소드를 통해서 얻은 보상의 총합과 전체 보상치 간의 오차를 줄이면서 에피소드 당 기대 보상치를 최대화하는 것\n",
    "- 에피소드별로 이상적인 행동을 취했을 때의 실제 기대 보상치와 현재 정책이 선택한 행동을 취했을 때의 기대 보상치 간의 차이를 존부 더한다.\n",
    "- 낮을수록 더 좋은 성능\n",
    "$$T = \\sum^{E}_{e = 1} \\mathbb{E} \\big[ v_* - q_* (A_e) \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b63c4a-5e4c-4b39-8b0f-567d9a68bbe2",
   "metadata": {},
   "source": [
    "MAB 환경을 풀기 위한 방법들\n",
    "1. 임의 탐색 전략random exploration strategy\n",
    "    - 대부분 탐욕적으로 행동을 취하다가 입실론이라는 임계값의 확률로 행동을 임의로 선택\n",
    "2. 낙관적인 탐색 전략optimistic exploration strategy\n",
    "    - 의사 결정 문제에서 불확실성을 수치적으로 표현하고, 높은 불확실성 상에서도 상태에 대한 선호도를 높이는 조금 더 체계적인 방법\n",
    "3. 정보 상태 영역 탐색 전략information state-space exploration strategy\n",
    "    - 환경의 일부로써 에이전트의 정보 상태를 모델링, 상태 영역의 일부로 불확실성을 해석\n",
    "        - 환경의 상태가 탐색되지 않은 미지의 경우와 이미 탐색된 경우일 때 다르게 보일 수 있다는 것을 의미\n",
    "    - 상태 영역을 늘림으로써 복잡성이 증가하는 단점 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640b9765-43b9-43fa-b9b5-9a60b7fdf00b",
   "metadata": {},
   "source": [
    "*그리디 전략greedy strategy* = *순수 착취 전략pure exploitation strategy*\n",
    "- 탐욕적으로 행동을 취하는 전략은 항상 가장 높은 추정 가치를 얻는 행동을 취한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8b660-bc34-439a-ae4d-f57f5fb9032d",
   "metadata": {},
   "source": [
    "```python\n",
    "# 순수 착취 전략\n",
    "def pure_exploitation(enc, n_episodes=5000):\n",
    "    Q = np.zeros((env.action_space.n),dtype=np.float64)\n",
    "    N = np.zeros((env.action_space.n), dtype=np.int)\n",
    "    Qe = np.empty((n_episodes, env.action_space.n),dtype=np.float64)\n",
    "    returns = np.empty(n_episodes, dtype=np.float64)\n",
    "    actions = np.empty(n_episodes, dtype=np.int)\n",
    "    name = 'Pure exploitation'\n",
    "    for e in tqdm(range(n_episodes), # 메인 루프로 들어가면서 환경과 상호작용이 일어나는 구간\n",
    "                  desc = 'Episodes for: ' + name,\n",
    "                  leave=False):\n",
    "        action = np.argmax(Q) # Q 값을 최대화할 수 있는 행동 선택\n",
    "        _, reward, _, _ = env.step(action) # 환경에 해당 행동 적용 후 새로운 보상\n",
    "        B[action] += 1\n",
    "        Q[action] = Q[action] + (reward - Q[action])/N[action]\n",
    "        Qe[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4971045f-7b80-4d4a-80ed-74d654c22fd5",
   "metadata": {},
   "source": [
    "*임의 전략random strategy* = *순수 탐색 전략 pure exploration strategy*\n",
    "- 착취하지 않고 탐색만 하는 전략, \n",
    "- 에이전트의 유일한 목표는 정보를 얻는 것\n",
    "- 착취없이 행동을 결정하는 간단한 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d016e7b-916e-489c-a45f-cc1ad920dcff",
   "metadata": {},
   "source": [
    "```python\n",
    "# 순수 탄색 전략\n",
    "def pure_exploration(env, n_episodes=5000):\n",
    "    Q = np.zeros((env.action_space.n),dtype=np.float64)\n",
    "    N = np.zeros((env.action_space.n), dtype=np.int)\n",
    "    Qe = np.empty((n_episodes, env.action_space.n),dtype=np.float64)\n",
    "    returns = np.empty(n_episodes, dtype=np.float64)\n",
    "    actions = np.empty(n_episodes, dtype=np.int)\n",
    "    name = 'Pure exploration'\n",
    "    for e in tqdm(range(n_episodes),\n",
    "                  desc = 'Episodes for: ' + name,\n",
    "                  leave=False):\n",
    "        action=np.random.rsndit(len(Q))\n",
    "        _, reward, _, _ = env.step(action) # 환경에 해당 행동 적용 후 새로운 보상\n",
    "        B[action] += 1\n",
    "        Q[action] = Q[action] + (reward - Q[action])/N[action]\n",
    "        Qe[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce68a6d-35cc-42b2-acdd-e4c32d706f7a",
   "metadata": {},
   "source": [
    "착취는 목표이며, 탐색은 이 목표를 달성하기 위한 정보를 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8c795-ef89-4267-8dee-9dc6fd4ca680",
   "metadata": {},
   "source": [
    "**입실론 그리디 전략과 입실론 그리디 감가전략이 가장 많이 쓰이는 탐색 전략, 잘 동작하면서도 내부 구조가 단순하다는 이유로 선호받는다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d999511-3cbe-42d9-be3c-1e6835bebdcd",
   "metadata": {},
   "source": [
    "*입실론-그리디 전략 epsilon-greedy strategy*\n",
    "- 행동을 임의로 선택,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c6615-1952-4971-8c30-59542da1c371",
   "metadata": {},
   "source": [
    "```python\n",
    "def epsilon_greedy(env,epsilon=0.01, n_episodes=5000):\n",
    "    Q = np.zeros((env.action_space.n),dtype=np.float64)\n",
    "    N = np.zeros((env.action_space.n), dtype=np.int)\n",
    "    Qe = np.empty((n_episodes, env.action_space.n),dtype=np.float64)\n",
    "    returns = np.empty(n_episodes, dtype=np.float64)\n",
    "    actions = np.empty(n_episodes, dtype=np.int)\n",
    "    name = 'Epsilon-Greedy {}'.format(epsilon)\n",
    "    for e in tqdm(range(n_episodes),\n",
    "                  desc='Episodes for: ' + name,\n",
    "                  leave=False):\n",
    "        if np.random.uniform() >epsilon: # 우선 임의로 숫자를 선택하고 이 값을 하이퍼파라미터인 epsilon과 비교\n",
    "            action = np.argmax(Q)\n",
    "        else:\n",
    "            action = np.random.randit(len(Q))\n",
    "            _, reward, _, _ = env.step(action) # 환경에 해당 행동 적용 후 새로운 보상\n",
    "            B[action] += 1\n",
    "            Q[action] = Q[action] + (reward - Q[action])/N[action]\n",
    "            Qe[e] = Q\n",
    "            returns[e] = reward\n",
    "            actions[e] = action\n",
    "    return name, returns, Qe, actions\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a441d9-f7bb-4031-92e8-dc349af0895f",
   "metadata": {},
   "source": [
    "*입실론-그리디 감가 전략 decaying epsilon-greedy strategy*\n",
    "- 처음에는 입실론을 1보다 작거나 같은 매우 큰 값으로 시작하고, 매 타임 스텝마다 그 값을 감가시키는 것\n",
    "    - 선형적으로 감가\n",
    "    - 기하급수적으로 감가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc417af7-9f57-450c-baa6-44c3acacb153",
   "metadata": {},
   "source": [
    "*낙관적 초기화 전략optimistic initialization* = 불확실성에 맞닿은 낙관성optimosm in the face of uncertainty\n",
    "- Q함수를 낙관적인 값인 높은 값으로 초기화함으로써 탐색되지 않은 행동에 대한 탐색을 할 수 있게 도와줘서\n",
    "- 에이전트는 환경과 상호작용 하면서 추정치는 낮은 값으로 수렴하기 시작하고\n",
    "- 정확해진 추정치는 에이전트가 실제로 높은 보상을 받을 수 있는 행동을 찾고 수렴할 수 있게 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b849895-edbc-48da-948b-a1b69fd20b30",
   "metadata": {},
   "source": [
    "*소프트맥스 전략 softmax strategy*\n",
    "- 행동-가치 함수에 기반한 확률 분포로부터 행동을 샘플링하는데, 이 때 행동을 선택하는 확률은 행동-가치 추정에 비례하도록 한다.\n",
    "- Q 함수에 대한 선호도를 매기고, 이 선호도를 기반한 확률 분포에서 행동을 샘플링하면 된다.\n",
    "- Q값의 추정치 차이는 높은 추정치를 가지는 행동은 자주 선택하고, 낮은 추정치를 가지는 행동을 덜 선택하는 경향을 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb4c98-8f80-48b4-ab36-aa5308b3a563",
   "metadata": {},
   "source": [
    "$$\\pi(a) = \\frac{exp\\big( \\frac{Q(a)}{\\tau} \\big) }{ \\sum^{B}_{b=0} exp \\big( \\frac{Q(b)}{\\tau} \\big) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc5ce7-1d9f-481a-9834-01516246aa05",
   "metadata": {},
   "source": [
    "- $\\tau$는 온도 계수,\n",
    "- $Q$값을 $\\tau$로 나눠즈면 행동을 선택하는 선호도가 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e1b7d-decb-4298-b927-3e92198d9933",
   "metadata": {},
   "source": [
    "*신뢰 상한 전략upper confidence bound. UCB*\n",
    "- 낙관적 초기화 원칙은 그대로 유지하되, 불확실성을 추정하는 값을 계산하는데 통계적인 기법을 사용하고, 이를 탐색할 떄 일종의 보너스로 사용하늗 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab28b6d-4dc8-41af-931e-5cfb7352c068",
   "metadata": {},
   "source": [
    "$$A_e = argmax_a \\big[ Q_e(a) + c\\sqrt{\\frac{\\ln e}{N_e(a)}} \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd9a36b-9092-4fcb-973b-a7378e76d2de",
   "metadata": {},
   "source": [
    "*톰슨 샘플링 전략 thormpson sampling*\n",
    "- 탐색과 착위 사이에서 균형을 잡는데, 베이지안 기법을 사용한 샘플링 기반 확류탐색 전략\n",
    "- 책의 예시에서는 Q 값을 하나의 가우시안 분포로 고려하여 구현한다.\n",
    "    - 평균을 기준으로 대칭을 이루고, 가르치는 목적에 적합할만큼 간단하기 때문\n",
    "    - 다른 분포도 사용될 수 있다.\n",
    "    - 가우시안 분포를 사용하는 이유\n",
    "        - 가우시안 평균이 Q값의 추정치이고, 가우시안 표준편차는 추정치에 대한 불확실성을 나타내는데 이 값은 매 에피소드마다 업데이트 된다.\n",
    "- 하지만 베타 분포를 더 많이 쓰이는 것처럼 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc23bc-1e88-4fbe-ac98-0a110c53d273",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ddfd95-76cf-4712-be7f-b5f3e8f52d16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fe53ad1-2416-4874-8266-dabf81855471",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "161b3bb7-33d5-40a8-861c-bb44e32caa88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9b160a7-885c-443f-8c89-65662412c393",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5276fa57-38ab-444b-ac27-0cca09295ccd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5601f7a-5409-4ecf-bbbb-5612b1d61225",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10699b07-e934-4e7b-8dc4-034ca42fff0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "956ae105-fb66-4654-8c20-501c038df99b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2182737b-7601-4403-b33f-3a29caf31921",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fcb2fb5-868c-4b42-9b8c-fb1eb783c071",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bb5a245-fad9-42d1-95a1-2eb850902776",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffc0e5bd-1d78-4d24-85b1-12035fc7e4be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0adee5bc-a22b-438f-88af-f432245bbb10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "913e112e-4a1c-4ab7-8faa-93a5a40417a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "792ffb44-4cd4-4aaa-8404-13ca56bc7e14",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e351b31-f811-4b03-abe1-7ade03f9582a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790bca37-dfdf-4174-a1ee-86f5e457ad7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8597c-0a3d-4d49-a192-b768fd18f514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6503e-0c74-4aff-82cb-3874e2076a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67033ca-61e4-4b1a-b39e-540981ebba69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7c2d6-569c-496b-a5e4-174fff2e402f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c3b16-b941-43c0-af9a-215022229daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692b5de-083e-4c05-84e8-018eba55f8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6db7592e-a2d5-4c0b-be41-94ebf29daefa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82c8ea7b-0103-4a4a-9d40-fb2c1517b434",
   "metadata": {},
   "source": [
    "$52.$ 어느 전자제품의 수명이 확률변수 $X$ ~ $EXP(100)$라고 할 때,\n",
    "\n",
    "(1) $P(X>30)$을 구하라.\n",
    "\n",
    "(2) $P(X>110)$을 구하라.\n",
    "\n",
    "(3) $P(X>110|X>80)$를 구하고 (1)의 결과와 비교하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6fb48-b80b-4687-a30b-b315e615328e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
