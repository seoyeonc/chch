{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a327a00e-3d5a-4562-86ad-3bf83eb54fe4",
   "metadata": {
    "id": "d44ba9bc-ebf3-49d8-b2db-9d49bf1bec22",
    "tags": []
   },
   "source": [
    "# Wasserstein GAN\n",
    "> Martin Arjovsky(Courant Institute of Mathematical Sciences), Soumith Chintala(Facebook AI Research), and L´eon Bottou1(Courant Institute of Mathematical Sciences, Facebook AI Research)\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: false \n",
    "- author: 최서연\n",
    "- categories: [Wasserstein GAN, GAN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24ed9b-b50c-4ca2-bcae-c57a33e17432",
   "metadata": {},
   "source": [
    "ref: https://arxiv.org/pdf/1701.07875.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ab002-cfde-430c-a34f-f90c7f35cefd",
   "metadata": {},
   "source": [
    "https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb682f-d23b-4aae-a009-79297596217d",
   "metadata": {},
   "source": [
    "https://ahjeong.tistory.com/7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f55c08-cda0-4249-abae-e2dc1108e54d",
   "metadata": {},
   "source": [
    "## Different Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426c533-2fc9-4042-a089-4da4bf268d94",
   "metadata": {},
   "source": [
    "- Let $X$ be a compact metric set (such as the space of images $[0, 1]^d$) \n",
    "- and let $Σ$ denote the set of all the Borel subsets of $X$ . \n",
    "- Let $Prob(X )$ denote the space of probability measures defined on $X$ .\n",
    "- We can now define elementary distances and divergences between two distributions $P_r, P_g ∈ Prob(X )$\n",
    "    - The Total Variation (TV) distance, The Kullback-Leibler (KL) divergence, The Jensen-Shannon (JS) divergence, The Earth-Mover (EM) distance or Wasserstein-1이 사용될 것.\n",
    "    - p값들은 절대적으로 연속!\n",
    "    - 그러므로 카이제곱에서 정의되는 같은 측정치 뮤가 나옴.\n",
    "- The following example illustrates how apparently simple sequences of probability distributions converge under the EM distance but do not converge under the other distances and divergences defined above\n",
    "    - 다음 예시에서 보기에 단순한 확률 분포 시퀀스가 EM 거리에서는 수렴하지만 제시된 4개의 거리 및 발산에서는 수렴하지 않음을 보임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45c146-5128-4da5-a07e-7a50de90f9cf",
   "metadata": {},
   "source": [
    "*Example 1 (Learning parallel lines)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecaa6d3-dd0c-40c3-bdf4-ad86c41af11e",
   "metadata": {},
   "source": [
    "- Let $Z ∼ U[0, 1]$ the uniform distribution on the unit interval.\n",
    "- Let $P_0$ be the distribution of $(0, Z) ∈ R^2$ (a 0 on the x-axis and the random variable Z on the y-axis), uniform on a straight vertical line passing through the origin.\n",
    "- Now let $g_θ(z) = (θ, z)$ with $θ$ a single real parameter. \n",
    "- It is easy to see that in this case,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172079f-939c-4817-8eef-93e42bd27d97",
   "metadata": {},
   "source": [
    "- When $θ_t → 0$, the sequence ($P_{θ_t})_{t∈N}$ converges to $P_0$ under the EM distance, but does not converge at all under either the JS, KL, reverse KL, or TV divergences.\n",
    "- Figure 1 illustrates this for the case of the EM and JS distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2121aa2-4004-4fba-ac4b-6d93242ee5d1",
   "metadata": {},
   "source": [
    "- Example 1 gives us a case where we can learn a probability distribution over a low dimensional manifold by doing gradient descent on the EM distance. \n",
    "- This cannot be done with the other distances and divergences because the resulting loss function is not even continuous.\n",
    "    - 결과 손실 함수가 연속적이지 않기 때문에 다른 거리와 발산으로 할 수 없음.\n",
    "- Although this simple example features distributions with disjoint supports, the same conclusion holds when the supports have a non empty intersection contained in a set of measure zero.\n",
    "    - disjoint suppoert를 가진 분포를 특장으로 하는 이 단순한 예시는 supports가 0 집합에 포함된 비어 있지 않은 interaction을 가진 때에도 동일한 결과가 유지된다.\n",
    "- This happens to be the case when two low dimensional manifolds intersect in general position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f17bb5-4179-4a48-81f0-106e4c3f4277",
   "metadata": {},
   "source": [
    "![](https://xiucheng.org/assets/images/wgan-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95f4a5-029f-45f9-88f5-01415ca1bede",
   "metadata": {},
   "source": [
    "Since the Wasserstein distance is much weaker than the JS distance , we can now ask whether $W(P_r, P_θ)$ is a continuous loss function on $θ$ under mild assumptions. This, and more, is true, as we now state and prove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf478f23-911a-440e-8a8a-ccd4d9d6c018",
   "metadata": {},
   "source": [
    "*Theorem 1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce2cf7-3cee-4e97-b194-3f83278dd5f3",
   "metadata": {},
   "source": [
    "- Let $P_r$ be a fixed distribution over $X$ .\n",
    "- Let $Z$ be a random variable(e.g Gaussian) over another space $Z$. \n",
    "- Let $g : Z × R^d → X$ be a function, that will be denoted $g_θ(z)$ with $z$ the first coordinate and $θ$ the second.\n",
    "- Let $P_θ$ denote the distribution of $g_θ(Z)$. Then,\n",
    "1. If $g$ is continuous in $θ$, so is $W(P_r, P_θ)$.\n",
    "    - g가 세타에 대해 연속이면 EM 거리도 연속!\n",
    "2. If $g$ is locally Lipschitz and satisfies regularity assumption 1, then $W(P_r, P_θ)$ is continuous everywhere, and differentiable almost everywhere.\n",
    "    - g가 립시츠 조건 만족한다면  EM 거리도 연속!\n",
    "3. Statements 1-2 are false for the Jensen-Shannon divergence $JS(P_r, P_θ)$ and all the KLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2444c2-4cf2-480a-9d82-54fae7cc8b67",
   "metadata": {},
   "source": [
    "The following corollary tells us that learning by minimizing the EM distance makes sense (at least in theory) with neural networks.\n",
    "- 다음 결과는 EM 거리를 최소화하여 학습하는 것이 신경망에 타당하다는 것을 말해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f80594-916e-4e8f-a114-b35f1b2669d0",
   "metadata": {},
   "source": [
    "*Corollary 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf21b6-37bf-471b-834e-1c8a04c829b4",
   "metadata": {},
   "source": [
    "- Let $g_θ$ be any feedforward neural network parameterized by $θ$, and $p(z)$ a prior over $z$ such that $E_{z∼p(z)}(||z||) < ∞$ (e.g. Gaussian, uniform, etc.)\n",
    "- Then assumption 1 is satisfied and therefore $W(P_r, P_θ)$ is continuous everywhere and differentiable almost everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406d029-db18-4e10-9cb2-4e03e6e0ed85",
   "metadata": {},
   "source": [
    "All this shows that EM is a much more sensible cost function for our problem than at least the Jensen-Shannon divergence. The following theorem describes the relative strength of the topologies induced by these distances and divergences, with KL the strongest, followed by JS and TV, and EM the weakest.\n",
    "- EM이 최소 JS 발산보다 제시한 문제에 보다 더 합리적인 cost function이라는 것을 보임.\n",
    "- 상대적인 강도는 KL > JS > TV > EM 의 순을 이룸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d51d9a6-9b87-4f73-88f5-c11f018dd5e0",
   "metadata": {},
   "source": [
    "*Theorem 2.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e25c557-2891-4424-9a51-329899d78c39",
   "metadata": {},
   "source": [
    "- Let $P$ be a distribution on a compact space $X$ and $(P_n)_{n∈N}$ be a sequence of distributions on $X$ .\n",
    "- Then, considering all limits as $n → ∞$, \n",
    "1. The following statements are equivalent \n",
    "    - $δ(P_n, P) → 0$ with δ the total variation distance \n",
    "    - $JS(P_n, P) → 0$ with JS the Jensen-Shannon divergence.\n",
    "2. The following statements are equivalent\n",
    "    - $W(P_n, P) → 0.$\n",
    "    - $P_n D→ P$ where $D→$ represents convergence in distribution for random variables.\n",
    "3. KL($P_n|P) → 0$ or KL($P|P_n) → 0$ imply the statements in (1).\n",
    "4. The statements in (1) imply the statements in (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65baa771-1d89-488f-b878-9c425eedc495",
   "metadata": {},
   "source": [
    "This highlights the fact that the KL, JS, and TV distances are not sensible cost functions when learning distributions supported by low dimensional manifolds. However the EM distance is sensible in that setup. This obviously leads us to the next section where we introduce a practical approximation of optimizing the EM distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85278230-d405-4402-aee1-59bf430a364d",
   "metadata": {},
   "source": [
    "## Wasserstein GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79e87bc-c72b-4f60-9560-203dd80e456e",
   "metadata": {},
   "source": [
    "- Again, Theorem 2 points to the fact that $W(P_r, P_θ)$ might have nicer properties when optimized than $JS(P_r, P_θ)$. \n",
    "- However, the infimum in (1) is highly intractable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c389727-702a-46df-87b4-0d728619333c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
