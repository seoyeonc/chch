{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c558f67-3541-4564-8b06-db80ad9d20f7",
   "metadata": {},
   "source": [
    "# Instance Selection for GANs\n",
    "> Terrance DeVries, Michal Drozdzal, Graham W. Taylor\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- author: 최서연\n",
    "- categories: [논문리뷰]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3046e-528c-4fc0-bd0a-9402a3d8c795",
   "metadata": {},
   "source": [
    "ref: https://arxiv.org/pdf/2007.15255.pdf\n",
    "\n",
    "git: https://github.com/uoguelph-mlrg/instance_selection_for_gans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852928d2-26f1-41f6-9d91-17fda6d23b78",
   "metadata": {},
   "source": [
    "instance, GAN에서 쉽게 말하자면 만들어내는 결과(?) 의미하는 듯,\n",
    "- 위조지폐로 보면 지폐가 instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daff7a9-3f65-4fd1-a1f2-2e976c2fc749",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580aeef-81d7-47d8-9137-186b61d15999",
   "metadata": {},
   "source": [
    "Several recently proposed techniques attempt to avoid spurious samples, either by rejecting them after generation, or by truncating the model’s latent space.\n",
    "- 최근 제안된 기술들은 가짜 샘플들을 피하려는 시도는 생성 후 거절하거나 모델의 잠재 공간을 잘라내는 것임.\n",
    "- 효과적이긴 한데 모델의 대부분이 사용되지 않는 샘플에 할당"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade8892-e27d-4b35-a42f-67c58a4841f3",
   "metadata": {},
   "source": [
    "altering the training dataset via instance selection before model training has taken place. \n",
    "- 그래서 모델 학습이 일어나기 전에 인스턴스 선택을 통해 학습셋을 변경하는 것을 제안할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e9a15-3372-44a3-aafe-991f6dc19ca4",
   "metadata": {},
   "source": [
    "## Instance Selection for GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91deac67-1475-415c-a272-8cfce16467ab",
   "metadata": {},
   "source": [
    "- to automatically remove the sparsest regions of the data manifold, specifically those parts that GANs struggle to capture. \n",
    "- define an image embedding function F and a scoring function H."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b3e81-80c8-4d59-909e-e494b0797721",
   "metadata": {},
   "source": [
    "**Embedding function**\n",
    "\n",
    "- F projects images into an embedding space\n",
    "    - image z data set이 주어지면, $z = F(x)$를 data point $x ∈ X $에 적용하여 embeded image Z 가 주어진다.\n",
    "    - image  generation을 위해 사전 학습된 image classifier의 feature space와 같은 aligned embedding function을 제안하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcfab9f-3288-4e87-a1de-235538abc292",
   "metadata": {},
   "source": [
    "**Scoring function**\n",
    "- H is used to to assess the manifold density in a neighbourhood around each embedded data point z.\n",
    "    - 논문에서 비교할 세 가지 scoring function selection\n",
    "        - *log likelihood under a standard Gaussian model,\n",
    "        - log likelihood under a Probabilistic Principal Component Analysis (PPCA) model,\n",
    "        - distance to the Kth nearest neighbour (KNN Distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b79e6-a168-4c47-83d5-77740a064019",
   "metadata": {},
   "source": [
    "The Gaussian model is fit to the *embedded dataset by computing the empirical mean $µ$ and the sample covariance $Σ$ of $Z$.*\n",
    "- d는 z의 demension\n",
    "\n",
    "$$H_{Gaussian}(z) = −\\frac{1}{2}[ln(|Σ|) + (z − µ)^{T} Σ^{−1}(z − µ) + d ln(2π)], (1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e3caa-094f-46aa-bf5b-03401cc8fa8b",
   "metadata": {},
   "source": [
    "- 논문 설정: set the number of principal components such that 95% of the variance in the data is preserved.\n",
    "\n",
    "$$H_{PPCA}(z) = −\\frac{1}{2}[ln(|C|) + Tr((z − µ)^{T} C^{−1}(z − µ)) + d ln(2π)], C = WW^T + σ^2 I, (2)$$\n",
    "- $W$ is the fit model weight matrix,\n",
    "- $µ$ is the empirical mean of $Z$, \n",
    "- $σ$ is the residual variance, \n",
    "- $I$ is the identity matrix, \n",
    "- $d$ is the dimension of $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3cb269-5312-42cb-bc34-7b365efcc9d3",
   "metadata": {},
   "source": [
    "**KNN**\n",
    "- $z$와 $Z \\ {z}$의 유클리드 거리 계산 후 가장 가까운 k번째 원소까지 거리 반환해 data point 얻는데 사용한다.\n",
    "- To convert to a score, we make the resulting distance negative, such that smaller distances return larger values. \n",
    "\n",
    "$$H_{KNN}(z, K, Z) = − min\\underset{K}  \\{||z − z_i ||_2 : z_i ∈ Z \\ {z} \\}, (3)$$\n",
    "\n",
    "- 집합에서 k번째 가장 작은 값.$\\leftarrow$논문에서는 k=5로 정함\n",
    "- To perform *instance selection*, we compute scores $H(F(x))$ for each data point and keep all data points with scores above some threshold $ψ$.\n",
    "- For convenience, *we often set $ψ$ to be equal to some percentile of the scores, such that we preserve the top N% of the best scoring data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4d822-f1c8-42d7-9781-eec3e17ac352",
   "metadata": {},
   "source": [
    "Figure 1에서 High likelihood images share a similar visual structure, while low likelihood samples are more varied 였음!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1247c213-4cca-42cd-b4e3-dfed39b71aad",
   "metadata": {},
   "source": [
    "$$X' = {x ∈ X s.t. H(F(x)) > ψ}$$\n",
    "- data points $x ∈ X$ 의 초기 학습 set을 구성함으로써 reduced training set $X'$를 구성함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8913aab-34e5-4a7a-b4e6-a7084931f9eb",
   "metadata": {},
   "source": [
    "Figure 1에서 ImageNet의 Red Fox class 에서 most and least likely imgaed를 보면 training set으로부터 data points를 제거하는 것이 좋은 이유가 설명된다.\n",
    "\n",
    "Likelihood는 pretrain된 Inceptionv3 classifier에서 feature embedding에 적합한 가우시안모델에 의해 결정된다.\n",
    "\n",
    "- The most likely images (a) are similarly cropped around the fox’s face, while the least likely images (b) have many odd viewpoints and often suffer from occlusion. It is logical to imagine how a generative model trained on these unusual instances may try to generate samples that mimic such conditions, resulting in undesirable outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201ddad-07ad-44b8-acb3-82080abcd844",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15ebb3-f71c-4cd4-8bbc-b3d6e781c985",
   "metadata": {},
   "source": [
    "- review evaluation metrics,\n",
    "- motivate selecting instances based on manifold density, \n",
    "- analyze the impact of applying instance selection to GAN training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa776b0-9fed-43dc-95ef-c0633932d0a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870ac72-9a6a-42fa-9dec-f209b77ca2c6",
   "metadata": {},
   "source": [
    "- When calculating FID we follow Brock et al. [2] in using all images in the training set *to estimate the reference distribution*, and *sampling 50 k images* to make up the generated distribution.\n",
    "- For P&R and D&C we use an Inceptionv3 embedding.\n",
    "- 1 N and M are **set to 10 k samples** for both the reference and generated distributions, and K is **set equal to 5** as recommended by Naeem et al. [19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ef1c3-9af2-4383-9b9b-938b9024f979",
   "metadata": {},
   "source": [
    "### Relationship Between Dataset Manifold Density and GAN Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89786861-a9d5-4bb9-ac4d-7220745381b4",
   "metadata": {},
   "source": [
    "- image manifold는 많은 data point들이 서로 가까이에 있는 영역에서보다 정확히 정의된다.\n",
    "- GAN은 주어진 dataset의 data point를 기반으로 image manifold를 재현하려고 시도하기 떄문에 잘 정의된 manifold(no sparse manifold regions)가 있는 dataset에서 더 나은 성능을 발휘해야 한다고 suspect한다.\n",
    "    - 그래서 use the ImageNet2 dataset [7] and treat each of the 1000 classes as a separate dataset 할거다\n",
    "    - use a single class-conditional BigGAN from [2] that has been pretrained on ImageNet at 128 × 128 resolution. \n",
    "    - For each class, we sample 700 real images from the dataset, and generate 700 class-conditioned samples with the BigGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e8903-9284-485f-b3cf-d60144d2f782",
   "metadata": {},
   "source": [
    "To measure the density for each class manifold we compare three different methods: \n",
    "- Gaussian likelihood, \n",
    "- Probabilistic Principal Component Analysis (PPCA) likelihood,\n",
    "- and distance to the Kth neighbour (KNN Distance) (§3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b52b8f-c4ce-4b7d-ac20-ec33999fc225",
   "metadata": {},
   "source": [
    "![](https://d3i71xaburhd42.cloudfront.net/d534182c1a64143e74e9f00fd7394b9223fe62a0/5-Figure2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3100d-2825-4b66-9a10-3ff48baa73d8",
   "metadata": {},
   "source": [
    "Figure 2. image data set의 각 class에 대한 manifold 밀도 추정치와 FID 사이의 상관관계. x측 값이 낮을수록 dataset manifold의 밀도가 높다는 것을 나타냄. y축 값이 낮을수록 sample의 품질이 우수함을 나타냄."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0dacb-9d8d-4a04-b2ce-7458e5a3036d",
   "metadata": {},
   "source": [
    "### Embedding and Scoring Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b39755-042a-41dc-93f2-9f44f597512a",
   "metadata": {},
   "source": [
    "**dataset msnifold가 GAN 성능과 상관있다는 것을 확인하면 data manifold의 저밀도 영역에 놓여진 data point를 제거하여 training set의 전체 밀도를 인위적으로 증가시킨다.**\n",
    "- ImageNet에서 64 * 64 해상도로 여러 개의 Self-Attention GANs (SAGAN) train하기\n",
    "- Each model is trained on *a different 50% subset of ImageNet*, as chosen by instance selection using different embedding and scoring functions.\n",
    "- instance 선택으로 class 별로 이루어진다.\n",
    "- use the default settings for SAGAN\n",
    "- use a batch size of 128\n",
    "- apply the self-attention module at 32 × 32 resolution\n",
    "- All models are trained for 200k iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17028487-10fd-4231-9a5c-c136505de850",
   "metadata": {},
   "source": [
    "![](https://d3i71xaburhd42.cloudfront.net/d534182c1a64143e74e9f00fd7394b9223fe62a0/5-Table1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6116bc6-830d-4376-890f-24d7806fa27d",
   "metadata": {},
   "source": [
    "Table 1. Comparison of embedding and scoring functions on 64 × 64 ImageNet image generation task. \n",
    "- Models trained with instance selection significantly outperform models trained without instance selection, despite training on a fraction of the available data.\n",
    "    - 인스턴스 선택으로 훈련된 모델은 사용가능한 데이터의 일부의 훈련에도 인스턴스 선택없이 훈련된 모델보다 상당히 뛰어남\n",
    "- RR is the retention ratio (percentage of dataset trained on). Best results in bold.\n",
    "\n",
    "**All runs utilizing instance selection significantly outperform the baseline model trained on the full dataset, despite only having access to half as much training data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28dae1-b015-411b-b0d7-4ddd70f676a8",
   "metadata": {},
   "source": [
    "- We observe a large increase in image fidelity, as indicated by the improvements in Inception Score, Precision, and Density, and a slight drop in overall diversity, as measured by Recall.\n",
    "- Coverage, which measures realism-constrained diversity, benefits greatly from the more realistic samples and thus sees an increase, despite the reduction in overall diversity. \n",
    "- Since the increase in image quality is much greater than the decrease in diversity, FID also improves. \n",
    "- To verify that the gains are not simply caused by the reduction in dataset size we train a model on a 50% subset that was uniform-randomly sampled from the full dataset. \n",
    "- Here, we observe little change in performance compared to the baseline, indicating that performance improvements are **indeed due to careful selection of training data**, rather than the reduction of dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7491142c-d7c8-4714-9d87-9f8f42ebae0e",
   "metadata": {},
   "source": [
    "table 1의 결과로 미루어보아 all three candidate scoring functions: Gaussian likelihood, PPCA likelihood, and KNN distance, significantly outperform the full dataset baseline를 알 수 있었음\n",
    "- 그 중 가우시안이 가장 좋아보여서 점수 함수로 사용할 거(Gaussian likelihood slightly outperforms the alternatives, so we use it as the scoring function in the remainder of our experiments. )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28cd07-2b31-4bb1-bddc-4f8d5275e08f",
   "metadata": {},
   "source": [
    "여러가지 임베딩 비교해볼 예정\n",
    "- Inceptionv3 [28] trained on ImageNet, ResNet50 [9] trained on Places365 [40], ImageNet, and with SwAV unsupervised pretraining [4], and ResNeXt-101 32x8d [34] trained with weak supervision on Instagram 1B [15]. \n",
    "- compare a randomly initialized Inceptionv3 with no pretraining as a random embedding. \n",
    "\n",
    "For all architectures features are extracted after the global average pooling layer. \n",
    "- 모든 아키텍터 특징들은 global average pooling layer 후 extract될 거\n",
    "\n",
    "We find that all feature embeddings improve performance over the full dataset baseline except for the randomly initialized network. These results suggest that an embedding function that is well aligned with the target domain is required in order for instance selection to be effective. \n",
    "- instance selection이 효과적이려면 target domain과 잘 정렬된 embedding function이 필요하다는 것을 알았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee32e8be-ec51-4f75-bb0e-d0f5c2dd927e",
   "metadata": {},
   "source": [
    "### Retention Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48ceac-7c06-472f-9159-4250d9d12ec0",
   "metadata": {},
   "source": [
    "**Instance selection을 수행할떄 가장 중요한 고려 사항은 보존 비율이라 불리는 hyperparameter인 원래 dataset의 비율을 결정하는 것!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0681d4-b87e-4a68-a000-47a6603df221",
   "metadata": {},
   "source": [
    "![](https://d3i71xaburhd42.cloudfront.net/d534182c1a64143e74e9f00fd7394b9223fe62a0/6-Figure3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e700c75-c176-48ef-af6c-ea39512e864f",
   "metadata": {},
   "source": [
    "Figure 3.  SAGAN trained on 64 × 64 ImageNet, with instance selection used to reduced the dataset by varying amounts.\n",
    "- retention ratio 보존 비율=100은 전체 dataset에 대해 train된 model을 나타냄, 즉 no instance selection\n",
    "- The application of instance selection boosts overall performance significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39632b9-cd9f-41d5-9c2d-f17301aaf2e6",
   "metadata": {},
   "source": [
    "![](https://d3i71xaburhd42.cloudfront.net/d534182c1a64143e74e9f00fd7394b9223fe62a0/7-Figure4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f8d4e-092b-480d-8abd-199ec335004f",
   "metadata": {},
   "source": [
    "Figure 4. Samples of bird classes from SAGAN trained on 64×64 ImageNet.\n",
    "- Each row is conditioned on a different class.\n",
    "- Red borders indicate misclassification by a row-specific pretrained Inceptionv3 classifier. \n",
    "- **Instance selection (b) significantly improves sample fidelity and class consistency compared to the baseline (a).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e65a9-23c8-46c6-b706-50c1cf627629",
   "metadata": {},
   "source": [
    "Our best performing SAGAN model in terms of FID was trained on only 40% of the ImageNet dataset, yet outperforms FQ-BigGAN [39], the current state-of-the-art model for the task of 64 × 64 ImageNet generation.\n",
    "\n",
    "Despite using 2× less parameters and a 4× smaller batch size, our SAGAN achieves a better FID (9.07 vs. 9.76). \n",
    "\n",
    "Figure 4에서 볼 수 있듯이 instance selection model의 sample이 full dataset에서 train된 기준 model의 sample보다 더 잘 인식한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54287c90-4fe5-4d20-a1e4-9dcacfa5fdb6",
   "metadata": {},
   "source": [
    "### 128 × 128 ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64801623-ea22-4c5e-9069-e2dc10062445",
   "metadata": {},
   "source": [
    "이 장의 목적: To examine the impact of instance selection on the training time of large-scale models, we train two BigGAN models on 128 × 128 ImageNet3 . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e8b18-bd44-4958-92e5-c9a449b68bab",
   "metadata": {},
   "source": [
    "- uses the default hyperparameters from BigGAN with the exception that we reduce the channel multiplier from 96 to 64\n",
    "- use a single discriminator update instead of two for faster training\n",
    "- baseline BigGAN으로 우수한 성능을 이루는데 large batch가 중요하긴 하지만 instance selection과 결합하면 성능이 저하된다는 것이 발견됨.(Although large batch sizes are critical for achieving good performance with the baseline BigGAN [2], we found them to degrade performance when combined with instance selection. )\n",
    "- Therefore, we reduce the batch size from BigGAN’s default of 2048 to 256 for the instance selection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce063d-822e-4632-91e2-87b035f6b051",
   "metadata": {},
   "source": [
    "Despite using a much smaller batch size, our model trained with instance selection outperforms the baseline in all metrics except for Recall (Table 2), as expected due to the diversity/fidelity trade-off.\n",
    "- 훨씬 작은 배치 사이즈를 사용했지만 instance model로 훈련된 모델이 다양성/충실성 trade-off 때문에 예상대로 리콜을 제외한 모든 매트릭스에서 기준치를 능가한다.\n",
    "\n",
    "The instance selection model trains significantly faster than the baseline, requiring less than four days while the baseline requires more than two weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feee3ab-ef32-438a-835b-205eb790e60c",
   "metadata": {},
   "source": [
    "Figure 5: Samples from BigGAN trained on 256 × 256 ImageNet, with the truncation trick. Samples are selected to demonstrate the highest quality outputs for each model. The baseline model (a) struggles to produce convincing facial details, which the instance selection model (b) successfully achieves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa18bdb-ee76-4e80-857c-c4924b1ba033",
   "metadata": {},
   "source": [
    "### 256 ×256 ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9fadcb-33c7-4888-9c38-8f83d83ddd85",
   "metadata": {},
   "source": [
    "절대 시도 못할 조건..\n",
    "- To further demonstrate instance selection we train a BigGAN on ImageNet at 256 × 256 resolution using 4 V100s with 32GB of RAM each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ded0e-bf00-49a3-b892-3862aa7621bc",
   "metadata": {},
   "source": [
    "## Instance Selection in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64012ea-1a50-499d-99a1-d50719c7c5e9",
   "metadata": {},
   "source": [
    "As the experiments have shown, instance selection stands as a useful tool for trading away sample diversity in exchange for improvements in image fidelity, faster training, and lower model capacity requirements. We believe that this trade-off is a worthwhile hyperparameter to tune in consideration of the available compute budget, just as it is common practice to adjust model capacity or batch size to fit within the memory constraints of the available hardware.\n",
    "- instance selection은 이미지 충실도 개선, faster training, 낮은 capacity로 sample 가양성을 교확하는 유용한 tool.\n",
    "\n",
    "The control over the diversity/fidelity trade-off afforded by instance selection also yields a tool that can be used to better understand the behaviour and limitations of existing evaluation metrics. For instance, in some cases when applying instance selection, we observed that certain diversity-sensitive metrics (such as FID and Coverage) improved, even though the diversity of the training set had been significantly reduced. We leave it for future work to determine whether this is a limitation of these metrics, or a behaviour that should be expected.\n",
    "\n",
    "Finally, instance selection can be used to automatically curate new datasets for the task of image generation. Existing datasets that are designed for image synthesis often use manual filtering and hand-crafted cropping and alignment tools to increase the dataset manifold density [11]. As an alternative to these time-intensive procedures, instance selection provides a generic solution that can quickly be applied to any uncurated set of images.\n",
    "- instance selection은 uncurated set에 빠르게 적용할 수 있는 일반적인 solution을 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4132df-b6de-4aaa-8ad9-54cc5288b192",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a393a-a270-469d-b1be-81139aa1f97d",
   "metadata": {},
   "source": [
    "Our motivation is to remove sparse regions of the data manifold before training, acknowledging that they will ultimately be poorly represented by the GAN, and therefore, that attempting to capture them is an inefficient use of model capacity. \n",
    "\n",
    "\n",
    "There are multiple benefits of taking the instance selection approach: 충실도, 시간 단축\n",
    "1. We improve sample fidelity across a variety of metrics compared to training on uncurated data; \n",
    "2. We demonstrate that reallocating model capacity to denser regions of the data manifold leads to efficiency gains, meaning that we can achieve SOTA quality with smaller-capacity models trained in far less time. \n",
    "\n",
    "\n",
    "- To our knowledge, instance selection has not yet been formally analyzed in the generative setting. \n",
    "\n",
    "- **We have only considered the setting where curation is performed up-front, prior to training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a8717d-d1e7-4c76-ad11-ade2bb48e206",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e057a-fd14-4515-89b6-79e7d8d3fa9d",
   "metadata": {},
   "source": [
    "Training models with instance selection results in improvements to image quality, as well as significant reduction in training time and model capacity requirements. On 128x128 ImageNet, instance selection reduces training time by 4x compared to the baseline, and achieves better image quality than a model trained with twice the model capacity. On 256x256 ImageNet, a model trained with instance selection produces higher fidelity images than a model with twice the capacity, while also using approximately one order of magnitude less multiply-accumulate operations (MACS) throughout the duration of training.\n",
    "\n",
    "인스턴스 선택을하여 모델을 훈련시키면이미지 품질이 향상되고 훈련 시간과 모델 용량 요구사항(GPU )도 감소함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
